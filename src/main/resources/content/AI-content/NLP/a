NLPå®Œæ•´å­¦ä¹ è·¯å¾„ï¼šä»é›¶åŸºç¡€åˆ°ç²¾é€šï¼ˆä¼˜åŒ–ç‰ˆï¼‰
ğŸ“š è¯¾ç¨‹æ€»è§ˆ
ç›®æ ‡å—ä¼—ï¼šé›¶åŸºç¡€åˆ°è¿›é˜¶å­¦ä¹ è€…
å­¦ä¹ å‘¨æœŸï¼š14-18å‘¨ï¼ˆæ¯å‘¨10-15å°æ—¶ï¼‰
æ•™å­¦ç‰¹è‰²ï¼šæ•…äº‹åŒ–åœºæ™¯ + å¯è§†åŒ–ç†è§£ + åŠ¨æ‰‹å®è·µ + é¡¹ç›®é©±åŠ¨ + ç§‘å­¦æ–¹æ³•è®º
æ ¸å¿ƒç†å¿µï¼š"é“æœ¯å¹¶é‡" - æ—¢é‡è§†æŠ€æœ¯å®ç°ï¼Œæ›´å¼ºè°ƒç§‘å­¦æ€ç»´å’Œæ•°æ®å·¥ç¨‹

ğŸ§­ å­¦ä¹ å¯¼èˆªï¼šå››å¤§æ”¯æŸ±
1ï¸âƒ£ æŠ€æœ¯åŸºç¡€ (Technical Foundation)

NLPæ ¸å¿ƒæ¦‚å¿µä¸ç®—æ³•
æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„
å·¥ç¨‹å®ç°èƒ½åŠ›

2ï¸âƒ£ ç§‘å­¦æ–¹æ³•è®º (Scientific Methodology) ğŸ†•

å®éªŒè®¾è®¡ä¸è¯„ä¼°å“²å­¦
æ•°æ®é©±åŠ¨çš„æ€ç»´æ¨¡å¼
æ‰¹åˆ¤æ€§æ€è€ƒèƒ½åŠ›

3ï¸âƒ£ æ•°æ®å·¥ç¨‹ (Data Engineering) ğŸ†•

æ•°æ®è·å–ä¸æ¸…æ´—
æ ‡æ³¨è´¨é‡æ§åˆ¶
æ•°æ®å¢å¼ºä¸ä¼˜åŒ–

4ï¸âƒ£ å®æˆ˜èƒ½åŠ› (Practical Skills)

é¡¹ç›®å¼€å‘ç»éªŒ
é—®é¢˜è§£å†³èƒ½åŠ›
ç³»ç»Ÿè®¾è®¡æ€ç»´


ç¬¬é›¶éƒ¨åˆ†ï¼šæ–¹æ³•è®ºåŸºç¡€ï¼ˆ1å‘¨ï¼‰ğŸ†•
ç¬¬0ç« ï¼šNLPçš„ç§‘å­¦æ€ç»´ - å¦‚ä½•åƒç ”ç©¶è€…ä¸€æ ·æ€è€ƒ

ğŸŒ™ å¼€ç¯‡æ•…äº‹ï¼šä¸¤ä¸ªå›¢é˜Ÿéƒ½åœ¨åšæƒ…æ„Ÿåˆ†æï¼Œä¸ºä»€ä¹ˆä¸€ä¸ªæˆåŠŸäº†ï¼Œå¦ä¸€ä¸ªå¤±è´¥äº†ï¼Ÿç­”æ¡ˆä¸åœ¨æ¨¡å‹ï¼Œè€Œåœ¨æ–¹æ³•ã€‚

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ å»ºç«‹ç§‘å­¦çš„NLPç ”ç©¶æ€ç»´
âœ“ ç†è§£æ•°æ®çš„æ ¸å¿ƒåœ°ä½
âœ“ æŒæ¡å®éªŒè®¾è®¡åŸåˆ™
âœ“ åŸ¹å…»æ‰¹åˆ¤æ€§æ€è€ƒèƒ½åŠ›

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ”¬ NLPçš„ç§‘å­¦æ–¹æ³•è®º
é—®é¢˜å®šä¹‰ â†’ æ•°æ®æ”¶é›† â†’ å‡è®¾æå‡º â†’ å®éªŒè®¾è®¡ â†’ ç»“æœåˆ†æ â†’ ç»“è®ºéªŒè¯
â†‘                                                          â†“
â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â† è¿­ä»£ä¼˜åŒ– â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†

ğŸ“Š è¯„ä¼°çš„å“²å­¦ï¼šBeyond Accuracy

ä¸ºä»€ä¹ˆå‡†ç¡®ç‡ä¼šéª—äººï¼Ÿ
python# åƒåœ¾é‚®ä»¶åˆ†ç±»åœºæ™¯
æ€»é‚®ä»¶: 1000å°
åƒåœ¾é‚®ä»¶: 10å° (1%)
æ­£å¸¸é‚®ä»¶: 990å° (99%)

# æ¨¡å‹Aï¼šå…¨éƒ¨é¢„æµ‹ä¸ºæ­£å¸¸
å‡†ç¡®ç‡: 99% âœ“ (çœ‹èµ·æ¥å¾ˆé«˜ï¼)
å¬å›ç‡: 0% âœ— (æ²¡æœ‰è¯†åˆ«å‡ºä»»ä½•åƒåœ¾é‚®ä»¶)

# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ç»¼åˆè¯„ä¼°æŒ‡æ ‡



ğŸ¯ å®éªŒè®¾è®¡çš„é»„é‡‘æ³•åˆ™

æ§åˆ¶å˜é‡åŸåˆ™
å¯é‡å¤æ€§ä¿è¯
ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
æ¶ˆèå®éªŒçš„è‰ºæœ¯


ğŸ’¾ æ•°æ®ä¸­å¿ƒä¸»ä¹‰

"Garbage In, Garbage Out"
æ•°æ®è´¨é‡ > æ¨¡å‹å¤æ‚åº¦
æ ‡æ³¨ä¸€è‡´æ€§çš„é‡è¦æ€§


ğŸ› ï¸ å®è·µï¼šè®¾è®¡ä½ çš„ç¬¬ä¸€ä¸ªNLPå®éªŒ

å®šä¹‰ç ”ç©¶é—®é¢˜
è®¾è®¡è¯„ä¼°æ–¹æ¡ˆ
åˆ¶å®šæ•°æ®ç­–ç•¥
è§„åˆ’å®éªŒæµç¨‹




ç¬¬ä¸€éƒ¨åˆ†ï¼šNLPåŸºç¡€å…¥é—¨ï¼ˆ4å‘¨ï¼‰
ç¬¬1ç« ï¼šNLPçš„å‰ä¸–ä»Šç”Ÿ - è®©æœºå™¨ç†è§£äººç±»è¯­è¨€

ğŸŒ™ å¼€ç¯‡æ•…äº‹ï¼šä¸€ä¸ªç¨‹åºå‘˜æ·±å¤œæ”¶åˆ°å¥³æœ‹å‹çš„æ¶ˆæ¯"æ²¡äº‹"ï¼Œä»–è¯¥å¦‚ä½•è®©æœºå™¨ç†è§£è¿™èƒŒåçš„çœŸå®å«ä¹‰ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£NLPçš„æœ¬è´¨å’ŒæŒ‘æˆ˜
âœ“ æŒæ¡NLPçš„åº”ç”¨åœºæ™¯
âœ“ äº†è§£NLPçš„å‘å±•å†ç¨‹
âœ“ æ­å»ºPython NLPå¼€å‘ç¯å¢ƒ
âœ“ å»ºç«‹æ•°æ®é©±åŠ¨çš„æ€ç»´æ¨¡å¼ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ¤” ä¸ºä»€ä¹ˆNLPè¿™ä¹ˆéš¾ï¼Ÿ

è¯­è¨€çš„æ­§ä¹‰æ€§ï¼ˆ"æˆ‘å–œæ¬¢è‹¹æœ" - æ°´æœè¿˜æ˜¯æ‰‹æœºï¼Ÿï¼‰
è¯­å¢ƒçš„é‡è¦æ€§ï¼ˆ"bank" - é“¶è¡Œè¿˜æ˜¯æ²³å²¸ï¼Ÿï¼‰
æ–‡åŒ–å’Œéšå–»ï¼ˆ"è¿™ä¸ªè¥¿ç“œä¿ç†Ÿå—ï¼Ÿ" - ã€Šå¾æœã€‹æ¢—ï¼‰
æƒ…æ„Ÿçš„å¾®å¦™ï¼ˆ"å‘µå‘µ" vs "å“ˆå“ˆ"ï¼‰


ğŸ“Š NLPèƒ½åšä»€ä¹ˆï¼Ÿ

æƒ…æ„Ÿåˆ†æï¼ˆåˆ¤æ–­è¯„è®ºæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢ï¼‰
æœºå™¨ç¿»è¯‘ï¼ˆGoogleç¿»è¯‘èƒŒåçš„æŠ€æœ¯ï¼‰
é—®ç­”ç³»ç»Ÿï¼ˆSiriå’Œå°çˆ±åŒå­¦å¦‚ä½•å·¥ä½œï¼‰
æ–‡æœ¬ç”Ÿæˆï¼ˆChatGPTçš„é­”åŠ›ï¼‰


ğŸ”¬ ç§‘å­¦æ–¹æ³•è®ºåˆæ¢ ğŸ†•

å¦‚ä½•å®šä¹‰ä¸€ä¸ªNLPé—®é¢˜ï¼Ÿ
å¦‚ä½•æ”¶é›†å’Œè¯„ä¼°æ•°æ®ï¼Ÿ
å¦‚ä½•è®¾è®¡åŸºå‡†å®éªŒï¼Ÿ


ğŸ› ï¸ åŠ¨æ‰‹å®è·µï¼šç¬¬ä¸€ä¸ªNLPç¨‹åº
python# æƒ…æ„Ÿåˆ†æåˆä½“éªŒ - åŠ å…¥è¯„ä¼°æ€ç»´
def analyze_girlfriend_message(message):
predictions = {
"æ²¡äº‹": "å±é™©ï¼å¥¹å¯èƒ½ç”Ÿæ°”äº†ğŸ˜±",
"éšä¾¿": "è­¦æŠ¥ï¼éœ€è¦ç«‹å³å…³å¿ƒğŸ˜°",
"å“¦": "æ³¨æ„ï¼å¯èƒ½å¿ƒæƒ…ä¸å¥½ğŸ˜Ÿ"
}
return predictions.get(message, "æš‚æ—¶å®‰å…¨ğŸ˜Œ")

# æ–°å¢ï¼šå¦‚ä½•è¯„ä¼°è¿™ä¸ªè§„åˆ™ç³»ç»Ÿï¼Ÿ
def evaluate_rule_system(test_cases):
correct = 0
for message, true_label in test_cases:
pred = analyze_girlfriend_message(message)
# è¿™é‡Œç®€åŒ–äº†è¯„ä¼°é€»è¾‘
if true_label in pred:
correct += 1

accuracy = correct / len(test_cases)
print(f"å‡†ç¡®ç‡: {accuracy:.2%}")
print("æ€è€ƒï¼šè¿™ä¸ªè¯„ä¼°åˆç†å—ï¼Ÿæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ")

ğŸ® äº’åŠ¨ç¯èŠ‚ï¼š

æ–‡æœ¬æƒ…æ„Ÿåˆ¤æ–­å°æ¸¸æˆ
æ­§ä¹‰å¥å­æ”¶é›†å¤§èµ›
NLPåº”ç”¨åœºæ™¯å¤´è„‘é£æš´
è®¾è®¡è¯„ä¼°æ–¹æ¡ˆç»ƒä¹  ğŸ†•




ç¬¬1.5ç« ï¼šæ•°æ®çš„è‰ºæœ¯ - NLPé¡¹ç›®çš„åŸºçŸ³ ğŸ†•

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šä¸€ä¸ªNLPå·¥ç¨‹å¸ˆèŠ±äº†3ä¸ªæœˆè®­ç»ƒæ¨¡å‹ï¼Œå‡†ç¡®ç‡åªæœ‰60%ã€‚åæ¥å‘ç°ï¼Œé—®é¢˜ä¸åœ¨ç®—æ³•ï¼Œè€Œåœ¨æ•°æ®æ ‡æ³¨é”™è¯¯ç‡é«˜è¾¾30%...

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£æ•°æ®åœ¨NLPä¸­çš„æ ¸å¿ƒåœ°ä½
âœ“ æŒæ¡æ•°æ®è·å–å’Œæ¸…æ´—æŠ€æœ¯
âœ“ å­¦ä¼šæ•°æ®æ ‡æ³¨å’Œè´¨é‡æ§åˆ¶
âœ“ å®è·µæ•°æ®å¢å¼ºæ–¹æ³•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ“Š æ•°æ®å·¥ç¨‹å…¨æ™¯å›¾
æ•°æ®è·å– â†’ æ•°æ®æ¸…æ´— â†’ æ•°æ®æ ‡æ³¨ â†’ è´¨é‡æ§åˆ¶ â†’ æ•°æ®å¢å¼º
â†“           â†“           â†“           â†“           â†“
çˆ¬è™«/API   å»å™ª/è§„èŒƒ   æ ‡æ³¨è§„èŒƒ   ä¸€è‡´æ€§æ£€éªŒ   æ‰©å……æ•°æ®

ğŸ·ï¸ æ•°æ®æ ‡æ³¨çš„ç§‘å­¦ä¸è‰ºæœ¯

æ ‡æ³¨è§„èŒƒè®¾è®¡
yamlæƒ…æ„Ÿæ ‡æ³¨æŒ‡å—:
æ­£é¢: è¡¨è¾¾ç§¯ææƒ…ç»ªã€èµç¾ã€æ»¡æ„
ä¾‹: "è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼" â†’ æ­£é¢

è´Ÿé¢: è¡¨è¾¾æ¶ˆææƒ…ç»ªã€æ‰¹è¯„ã€ä¸æ»¡
ä¾‹: "è´¨é‡å¤ªå·®ï¼Œä¸æ¨è" â†’ è´Ÿé¢

ä¸­æ€§: å®¢è§‚é™ˆè¿°ã€æ— æ˜æ˜¾æƒ…æ„Ÿå€¾å‘
ä¾‹: "åŒ…è£…æ˜¯è“è‰²çš„" â†’ ä¸­æ€§

è¾¹ç•Œæ¡ˆä¾‹å¤„ç†:
è®½åˆº: "å‘µå‘µï¼ŒçœŸ'å¥½'å•Š" â†’ è´Ÿé¢
å¯¹æ¯”: "ä¸é”™ï¼Œå°±æ˜¯æœ‰ç‚¹è´µ" â†’ éœ€è¦å…·ä½“åˆ†æ




ğŸ“ æ ‡æ³¨è´¨é‡æ§åˆ¶

ä¸€è‡´æ€§æ£€éªŒï¼ˆInter-Annotator Agreementï¼‰
pythonfrom sklearn.metrics import cohen_kappa_score

# ä¸¤ä¸ªæ ‡æ³¨å‘˜çš„æ ‡æ³¨ç»“æœ
annotator1 = [1, 0, 1, 1, 0, 1, 0, 1]  # 1:æ­£é¢ 0:è´Ÿé¢
annotator2 = [1, 0, 1, 0, 0, 1, 1, 1]

kappa = cohen_kappa_score(annotator1, annotator2)
print(f"Cohen's Kappa: {kappa:.3f}")

# è§£é‡Šï¼š
# > 0.8: å‡ ä¹å®Œç¾ä¸€è‡´
# 0.6-0.8: å®è´¨ä¸€è‡´
# 0.4-0.6: ä¸­ç­‰ä¸€è‡´
# < 0.4: ä¸€è‡´æ€§å·®ï¼Œéœ€è¦æ”¹è¿›æ ‡æ³¨è§„èŒƒ



ğŸ”§ æ ‡æ³¨å·¥å…·å®è·µ

Label Studioé…ç½®ä¸ä½¿ç”¨
doccanoå¿«é€Ÿæ ‡æ³¨
è‡ªå®šä¹‰æ ‡æ³¨ç•Œé¢å¼€å‘


ğŸš€ æ•°æ®å¢å¼ºæŠ€æœ¯

åŒä¹‰è¯æ›¿æ¢ï¼ˆä¿æŒè¯­ä¹‰ï¼‰
å›è¯‘å¢å¼ºï¼ˆå¤šè¯­è¨€å¾€è¿”ï¼‰
ä¸Šä¸‹æ–‡æ‰°åŠ¨ï¼ˆAEDAï¼‰
å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ


ğŸ’¡ ä¸»åŠ¨å­¦ä¹ åˆæ¢
python# ä¸»åŠ¨å­¦ä¹ ç­–ç•¥ï¼šä¸ç¡®å®šæ€§é‡‡æ ·
def uncertainty_sampling(model, unlabeled_data, n_samples=100):
# è·å–æ¨¡å‹é¢„æµ‹æ¦‚ç‡
probs = model.predict_proba(unlabeled_data)

# è®¡ç®—ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰
entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)

# é€‰æ‹©æœ€ä¸ç¡®å®šçš„æ ·æœ¬
uncertain_indices = np.argsort(entropy)[-n_samples:]

return unlabeled_data[uncertain_indices]

âš–ï¸ æ•°æ®åˆè§„ä¸éšç§

GDPR/CCPAè¦æ±‚
ç”¨æˆ·æ•°æ®è„±æ•
æ•°æ®ä½¿ç”¨åè®®


ğŸ—ï¸ å®æˆ˜é¡¹ç›®ï¼šæ„å»ºé«˜è´¨é‡æ•°æ®é›†

è®¾è®¡æ ‡æ³¨ä»»åŠ¡
åˆ¶å®šæ ‡æ³¨è§„èŒƒ
è´¨é‡æ§åˆ¶æµç¨‹
æ•°æ®é›†å‘å¸ƒè§„èŒƒ




ç¬¬2ç« ï¼šæ–‡æœ¬é¢„å¤„ç† - æŠŠæ··ä¹±å˜æˆç§©åº

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•å¤„ç†ä¸€ä»½å……æ»¡é”™åˆ«å­—ã€è¡¨æƒ…ç¬¦å·ã€ç½‘ç»œç”¨è¯­çš„å¾®åšæ•°æ®ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æŒæ¡æ–‡æœ¬æ¸…æ´—æŠ€æœ¯
âœ“ ç†è§£åˆ†è¯çš„é‡è¦æ€§
âœ“ å­¦ä¼šå¤„ç†ä¸åŒè¯­è¨€çš„æ–‡æœ¬
âœ“ æ„å»ºæ–‡æœ¬é¢„å¤„ç†ç®¡é“
âœ“ ç†è§£é¢„å¤„ç†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ§¹ æ–‡æœ¬æ¸…æ´—çš„è‰ºæœ¯
python# å¯è§†åŒ–å±•ç¤ºæ¸…æ´—è¿‡ç¨‹
åŸå§‹æ–‡æœ¬: "ä»Šå¤©å¤©æ°”çœŸTMå¥½ğŸ˜ŠğŸ˜ŠğŸ˜Šï¼ï¼ï¼ä¹°äº†iPhone 14 Pro Max"
â†“ å»é™¤è„è¯
"ä»Šå¤©å¤©æ°”çœŸ**å¥½ğŸ˜ŠğŸ˜ŠğŸ˜Šï¼ï¼ï¼ä¹°äº†iPhone 14 Pro Max"
â†“ å¤„ç†è¡¨æƒ…
"ä»Šå¤©å¤©æ°”çœŸ**å¥½[å¼€å¿ƒ][å¼€å¿ƒ][å¼€å¿ƒ]ï¼ï¼ï¼ä¹°äº†iPhone 14 Pro Max"
â†“ æ ‡å‡†åŒ–æ ‡ç‚¹
"ä»Šå¤©å¤©æ°”çœŸ**å¥½[å¼€å¿ƒ][å¼€å¿ƒ][å¼€å¿ƒ]ï¼ä¹°äº†iPhone 14 Pro Max"
â†“ å®ä½“è§„èŒƒåŒ–
"ä»Šå¤©å¤©æ°”çœŸ**å¥½[å¼€å¿ƒ][å¼€å¿ƒ][å¼€å¿ƒ]ï¼ä¹°äº†[PRODUCT]"

âœ‚ï¸ ä¸­æ–‡åˆ†è¯çš„æŒ‘æˆ˜

åˆ†è¯æ­§ä¹‰ï¼š"å—äº¬å¸‚é•¿æ±Ÿå¤§æ¡¥" â†’ "å—äº¬å¸‚/é•¿æ±Ÿ/å¤§æ¡¥" vs "å—äº¬/å¸‚é•¿/æ±Ÿå¤§æ¡¥"
æ–°è¯å‘ç°ï¼š"å†…å·"ã€"èººå¹³"ã€"ç ´é˜²"
å®ç°å¯¹æ¯”ï¼šjieba vs HanLP vs THULAC
è¯„ä¼°æŒ‡æ ‡ï¼šåˆ†è¯å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ ğŸ†•


ğŸ“Š é¢„å¤„ç†æ•ˆæœè¯„ä¼° ğŸ†•
python# å¯¹æ¯”ä¸åŒé¢„å¤„ç†ç­–ç•¥çš„æ•ˆæœ
def compare_preprocessing_strategies(raw_data, strategies):
results = {}
for name, preprocess_func in strategies.items():
processed = preprocess_func(raw_data)
model = train_model(processed)
metrics = evaluate_model(model)
results[name] = metrics

# å¯è§†åŒ–å¯¹æ¯”ç»“æœ
visualize_comparison(results)
return results

ğŸŒ å¤šè¯­è¨€å¤„ç†

è‹±æ–‡ï¼šè¯å¹²æå– vs è¯å½¢è¿˜åŸ
æ—¥æ–‡ï¼šå½¢æ€ç´ è§£æ
æ··åˆæ–‡æœ¬ï¼šä»£ç è¯†åˆ«ä¸åˆ†ç¦»


âš¡ å®æˆ˜é¡¹ç›®ï¼šå¾®åšè¯„è®ºé¢„å¤„ç†å™¨

æ„å»ºå®Œæ•´çš„é¢„å¤„ç†pipeline
å¤„ç†çœŸå®çš„ç¤¾äº¤åª’ä½“æ•°æ®
æ€§èƒ½ä¼˜åŒ–æŠ€å·§
A/Bæµ‹è¯•ä¸åŒé¢„å¤„ç†ç­–ç•¥ ğŸ†•




ç¬¬3ç« ï¼šè¯å‘é‡ - ç»™è¯æ±‡èµ‹äºˆçµé­‚

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©è®¡ç®—æœºç†è§£"å›½ç‹-ç”·äºº+å¥³äºº=å¥³ç‹"è¿™ä¸ªç­‰å¼ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£è¯å‘é‡çš„æœ¬è´¨
âœ“ æŒæ¡Word2VecåŸç†
âœ“ å­¦ä¼šä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡
âœ“ å®ç°è¯å‘é‡å¯è§†åŒ–
âœ“ æŒæ¡è¯å‘é‡è´¨é‡è¯„ä¼°æ–¹æ³• ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ¯ ä»One-Hotåˆ°åˆ†å¸ƒå¼è¡¨ç¤º
å¯è§†åŒ–å¯¹æ¯”ï¼š
One-Hot:  è‹¹æœ [1,0,0,0,0...]  (ç¨€ç–ã€æ— è¯­ä¹‰)
é¦™è•‰ [0,1,0,0,0...]

Word2Vec: è‹¹æœ [0.2, -0.5, 0.8, ...]  (ç¨ å¯†ã€æœ‰è¯­ä¹‰)
é¦™è•‰ [0.3, -0.4, 0.7, ...]
ç›¸ä¼¼åº¦: 0.95 âœ¨

ğŸ”¬ Word2Vecçš„ä¸¤ç§æ¨¡å‹

CBOWï¼šç”¨ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯ï¼ˆå®Œå½¢å¡«ç©ºï¼‰
Skip-gramï¼šç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡ï¼ˆå‘æ•£æ€ç»´ï¼‰
è´Ÿé‡‡æ ·ä¼˜åŒ–æŠ€å·§


ğŸ“ è¯å‘é‡è´¨é‡è¯„ä¼° ğŸ†•
python# è¯ç±»æ¯”ä»»åŠ¡è¯„ä¼°
def evaluate_word_analogies(word_vectors, analogy_file):
correct = 0
total = 0

for a, b, c, expected in load_analogies(analogy_file):
# a : b :: c : ?
# å›½ç‹ : ç”·äºº :: å¥³ç‹ : ?
result = word_vectors.most_similar(
positive=[b, c],
negative=[a],
topn=1
)[0][0]

if result == expected:
correct += 1
total += 1

accuracy = correct / total
print(f"è¯ç±»æ¯”å‡†ç¡®ç‡: {accuracy:.2%}")
return accuracy

ğŸ¨ è¯å‘é‡å¯è§†åŒ–æ¢ç´¢

t-SNEé™ç»´å¯è§†åŒ–
è¯ç±»èšç±»åˆ†æ
è¯­ä¹‰å…³ç³»å‘ç°


ğŸ’¡ è¿›é˜¶æŠ€æœ¯

GloVeï¼šç»“åˆå…¨å±€ç»Ÿè®¡
FastTextï¼šå¤„ç†æœªç™»å½•è¯
ELMoï¼šä¸Šä¸‹æ–‡ç›¸å…³çš„è¯å‘é‡


ğŸ› ï¸ å®æˆ˜ï¼šæ„å»ºé¢†åŸŸè¯å‘é‡

é‡‘èé¢†åŸŸè¯å‘é‡è®­ç»ƒ
ç›¸ä¼¼è¯æŸ¥è¯¢ç³»ç»Ÿ
è¯å‘é‡è´¨é‡è¯„ä¼°
åè§æ£€æµ‹ä¸æ¶ˆé™¤ ğŸ†•




ç¬¬4ç« ï¼šæ–‡æœ¬è¡¨ç¤º - ä»è¯åˆ°å¥å­çš„é£è·ƒ

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©AIç†è§£"è¿™éƒ¨ç”µå½±ä¸é”™ï¼Œå°±æ˜¯æœ‰ç‚¹ä¸å¤ªå¥½çœ‹"è¿™ç§çŸ›ç›¾çš„å¥å­ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æŒæ¡ä¼ ç»Ÿæ–‡æœ¬è¡¨ç¤ºæ–¹æ³•
âœ“ ç†è§£TF-IDFçš„åŸç†å’Œåº”ç”¨
âœ“ å­¦ä¼šå¥å­å’Œæ–‡æ¡£çº§åˆ«çš„è¡¨ç¤º
âœ“ å®ç°æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
âœ“ å¯¹æ¯”ä¸åŒè¡¨ç¤ºæ–¹æ³•çš„ä¼˜åŠ£ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ“Š è¯è¢‹æ¨¡å‹ä¸N-gram

è¯è¢‹æ¨¡å‹çš„ç®€å•ä¸å±€é™
N-gramæ•è·å±€éƒ¨åºåˆ—ä¿¡æ¯
ç‰¹å¾å·¥ç¨‹çš„è‰ºæœ¯


âš–ï¸ TF-IDFçš„æ™ºæ…§
python# å¯è§†åŒ–TF-IDFæƒé‡
"æœºå™¨å­¦ä¹ " åœ¨ AIæ–‡ç« ä¸­ï¼šTFé«˜ï¼ŒIDFä½ â†’ æƒé‡ä¸­ç­‰
"æœºå™¨å­¦ä¹ " åœ¨ çƒ¹é¥ªæ–‡ç« ä¸­ï¼šTFä½ï¼ŒIDFé«˜ â†’ æƒé‡é«˜

ğŸ” è¡¨ç¤ºæ–¹æ³•çš„ç§‘å­¦è¯„ä¼° ğŸ†•
python# ç³»ç»Ÿæ¯”è¾ƒä¸åŒæ–‡æœ¬è¡¨ç¤ºæ–¹æ³•
def compare_text_representations(documents, labels):
methods = {
'BoW': CountVectorizer(),
'TF-IDF': TfidfVectorizer(),
'Word2Vec-Avg': Word2VecAverager(),
'Doc2Vec': Doc2VecModel()
}

results = {}
for name, method in methods.items():
# è·å–æ–‡æœ¬è¡¨ç¤º
features = method.fit_transform(documents)

# è¯„ä¼°èšç±»è´¨é‡
silhouette = silhouette_score(features, labels)

# è¯„ä¼°åˆ†ç±»æ€§èƒ½
clf_score = cross_val_score(
LogisticRegression(),
features,
labels
).mean()

results[name] = {
'silhouette': silhouette,
'classification': clf_score
}

return results

ğŸ”— å¥å­è¡¨ç¤ºæ–¹æ³•å¯¹æ¯”

å¹³å‡æ± åŒ–ï¼šç®€å•ä½†æœ‰æ•ˆ
åŠ æƒå¹³å‡ï¼šè€ƒè™‘è¯çš„é‡è¦æ€§
Doc2Vecï¼šå­¦ä¹ å¥å­çš„åˆ†å¸ƒå¼è¡¨ç¤º


ğŸ¯ å®æˆ˜ï¼šæ™ºèƒ½æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ

æ„å»ºä¼ä¸šçŸ¥è¯†åº“
å®ç°è¯­ä¹‰æœç´¢
ç›¸ä¼¼æ–‡æ¡£æ¨è
æ£€ç´¢æ•ˆæœè¯„ä¼°ï¼ˆMRR, NDCGï¼‰ğŸ†•




ç¬¬äºŒéƒ¨åˆ†ï¼šç»å…¸NLPä»»åŠ¡ï¼ˆ4å‘¨ï¼‰
ç¬¬5ç« ï¼šæ–‡æœ¬åˆ†ç±» - æ•™æœºå™¨å­¦ä¼šåˆ†é—¨åˆ«ç±»

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•æ„å»ºä¸€ä¸ªåƒåœ¾é‚®ä»¶è¿‡æ»¤å™¨ï¼Œæ—¢ä¸é”™è¿‡é‡è¦é‚®ä»¶ï¼Œåˆèƒ½æ‹¦æˆªspamï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æŒæ¡æ–‡æœ¬åˆ†ç±»å…¨æµç¨‹
âœ“ ç†è§£ä¸åŒåˆ†ç±»ç®—æ³•çš„ä¼˜åŠ£
âœ“ å­¦ä¼šå¤„ç†ä¸å¹³è¡¡æ•°æ®
âœ“ å®ç°å¤šæ ‡ç­¾åˆ†ç±»
âœ“ æ·±å…¥ç†è§£è¯„ä¼°æŒ‡æ ‡çš„å«ä¹‰ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ¯ æ–‡æœ¬åˆ†ç±»ä»»åŠ¡å…¨æ™¯

äºŒåˆ†ç±»ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹
å¤šåˆ†ç±»ï¼šæ–°é—»ä¸»é¢˜åˆ†ç±»
å¤šæ ‡ç­¾ï¼šè®ºæ–‡å…³é”®è¯æ ‡æ³¨
å±‚æ¬¡åˆ†ç±»ï¼šå•†å“ç±»ç›®é¢„æµ‹


ğŸ“Š æ·±å…¥ç†è§£è¯„ä¼°æŒ‡æ ‡ ğŸ†•
python# æ··æ·†çŸ©é˜µçš„å…¨é¢è§£è¯»
def analyze_confusion_matrix(y_true, y_pred):
cm = confusion_matrix(y_true, y_pred)

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

# è®¡ç®—å„é¡¹æŒ‡æ ‡
for i, class_name in enumerate(classes):
tp = cm[i, i]
fp = cm[:, i].sum() - tp
fn = cm[i, :].sum() - tp
tn = cm.sum() - tp - fp - fn

precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) \
if (precision + recall) > 0 else 0

print(f"\nç±»åˆ« {class_name}:")
print(f"  ç²¾ç¡®ç‡: {precision:.3f} (é¢„æµ‹ä¸ºè¯¥ç±»åˆ«ä¸­ï¼ŒçœŸæ­£å±äºè¯¥ç±»åˆ«çš„æ¯”ä¾‹)")
print(f"  å¬å›ç‡: {recall:.3f} (è¯¥ç±»åˆ«ä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹)")
print(f"  F1åˆ†æ•°: {f1:.3f} (ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡)")

# ç‰¹åˆ«è§£é‡Š
if class_name == "åƒåœ¾é‚®ä»¶":
print(f"  å«ä¹‰ï¼šåœ¨æ ‡è®°ä¸ºåƒåœ¾çš„é‚®ä»¶ä¸­ï¼Œ{precision:.1%}ç¡®å®æ˜¯åƒåœ¾")
print(f"       åœ¨æ‰€æœ‰åƒåœ¾é‚®ä»¶ä¸­ï¼Œ{recall:.1%}è¢«æˆåŠŸæ‹¦æˆª")

âš–ï¸ ç±»åˆ«ä¸å¹³è¡¡çš„ç§‘å­¦å¤„ç† ğŸ†•
python# ä¸åŒç­–ç•¥çš„æ•ˆæœå¯¹æ¯”
def handle_imbalance_comparison(X, y):
strategies = {
'baseline': None,
'class_weight': compute_class_weight('balanced',
classes=np.unique(y),
y=y),
'SMOTE': SMOTE(random_state=42),
'ADASYN': ADASYN(random_state=42),
'Focal_Loss': FocalLoss(alpha=0.25, gamma=2)
}

results = evaluate_strategies(strategies, X, y)
plot_performance_comparison(results)

ğŸ¤– ä»æœ´ç´ è´å¶æ–¯åˆ°æ·±åº¦å­¦ä¹ 
ç®—æ³•æ¼”è¿›å¯è§†åŒ–ï¼š
æœ´ç´ è´å¶æ–¯ â†’ SVM â†’ éšæœºæ£®æ— â†’ TextCNN â†’ BERT
å‡†ç¡®ç‡: 85%   88%    90%      94%      97%
è®­ç»ƒæ—¶é—´: 1s   10s    30s      5m       2h
å¯è§£é‡Šæ€§: é«˜    ä¸­     ä¸­       ä½       ä½

âš¡ TextCNNè¯¦è§£

å·ç§¯æ ¸æ•è·n-gramç‰¹å¾
å¤šå°ºåº¦å·ç§¯çš„è®¾è®¡
æ± åŒ–ç­–ç•¥çš„é€‰æ‹©


ğŸ”¬ æ¶ˆèå®éªŒï¼šè¯æ˜æ¯ä¸ªç»„ä»¶çš„ä»·å€¼ ğŸ†•
pythondef ablation_study(base_model):
"""æ¶ˆèå®éªŒï¼šç³»ç»Ÿåœ°ç§»é™¤æ¨¡å‹ç»„ä»¶ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–"""
results = {
'Full Model': evaluate(base_model)
}

# ç§»é™¤ä¸åŒç»„ä»¶
ablations = {
'No Dropout': remove_dropout(base_model),
'No Word Embedding': use_random_embedding(base_model),
'Single Filter Size': use_single_filter(base_model),
'No Max Pooling': use_avg_pooling(base_model)
}

for name, model in ablations.items():
results[name] = evaluate(model)

# å¯è§†åŒ–æ€§èƒ½ä¸‹é™
visualize_ablation_results(results)
return results

ğŸ’¼ é¡¹ç›®ï¼šæƒ…æ„Ÿåˆ†æç³»ç»Ÿ

ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†ç±»
ç»†ç²’åº¦æƒ…æ„Ÿåˆ†æï¼ˆæ–¹é¢çº§ï¼‰
å®æ—¶é¢„æµ‹APIéƒ¨ç½²
A/Bæµ‹è¯•ä¸åœ¨çº¿è¯„ä¼° ğŸ†•




ç¬¬6ç« ï¼šå‘½åå®ä½“è¯†åˆ« - åœ¨æ–‡æœ¬ä¸­å¯»æ‰¾å®è—

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•ä»æ–°é—»ä¸­è‡ªåŠ¨æå–äººåã€åœ°åã€æœºæ„åï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£NERä»»åŠ¡çš„æœ¬è´¨
âœ“ æŒæ¡åºåˆ—æ ‡æ³¨æ–¹æ³•
âœ“ å­¦ä¼šCRFå’ŒBiLSTM-CRF
âœ“ å®ç°é¢†åŸŸNERç³»ç»Ÿ
âœ“ æŒæ¡æ ‡æ³¨ä¸€è‡´æ€§è¯„ä¼° ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ·ï¸ NERä»»åŠ¡è§£æ
è¾“å…¥: "é©¬äº‘åœ¨æ­å·åˆ›ç«‹äº†é˜¿é‡Œå·´å·´"
è¾“å‡º: [é©¬äº‘/PER] åœ¨ [æ­å·/LOC] åˆ›ç«‹äº† [é˜¿é‡Œå·´å·´/ORG]

ğŸ¨ æ ‡æ³¨ä½“ç³»è®¾è®¡ä¸è´¨é‡æ§åˆ¶ ğŸ†•
python# è®¡ç®—æ ‡æ³¨ä¸€è‡´æ€§
def calculate_ner_agreement(annotations1, annotations2):
"""è®¡ç®—ä¸¤ä¸ªæ ‡æ³¨å‘˜åœ¨NERä»»åŠ¡ä¸Šçš„ä¸€è‡´æ€§"""
# æå–å®ä½“span
spans1 = extract_entity_spans(annotations1)
spans2 = extract_entity_spans(annotations2)

# è®¡ç®—ç²¾ç¡®åŒ¹é…
exact_match = len(spans1.intersection(spans2))

# è®¡ç®—éƒ¨åˆ†åŒ¹é…
partial_match = calculate_partial_matches(spans1, spans2)

# è®¡ç®—F1åˆ†æ•°
precision = exact_match / len(spans1) if spans1 else 0
recall = exact_match / len(spans2) if spans2 else 0
f1 = 2 * precision * recall / (precision + recall) \
if (precision + recall) > 0 else 0

print(f"æ ‡æ³¨ä¸€è‡´æ€§F1: {f1:.3f}")
print(f"ç²¾ç¡®åŒ¹é…æ•°: {exact_match}")
print(f"éƒ¨åˆ†åŒ¹é…æ•°: {partial_match}")

# åˆ†æåˆ†æ­§æ¡ˆä¾‹
analyze_disagreements(spans1, spans2)

return f1

ğŸ“ NERè¯„ä¼°çš„ç»†èŠ‚ ğŸ†•

ä¸¥æ ¼åŒ¹é… vs å®½æ¾åŒ¹é…
å®ä½“çº§åˆ« vs è¯çº§åˆ«è¯„ä¼°
ç±»åˆ«æ··æ·†åˆ†æ


âš™ï¸ æ¨¡å‹æ¶æ„æ¼”è¿›

HMMï¼šé©¬å°”å¯å¤«å‡è®¾
CRFï¼šå…¨å±€æœ€ä¼˜
BiLSTM-CRFï¼šæ·±åº¦ç‰¹å¾+å…¨å±€çº¦æŸ
BERT-CRFï¼šé¢„è®­ç»ƒçš„å¨åŠ›


ğŸ’¡ å®æˆ˜æŒ‘æˆ˜

åµŒå¥—å®ä½“ï¼š[åŒ—äº¬/LOC [å¤§å­¦/ORG]]
å®ä½“æ¶ˆæ­§ï¼šè‹¹æœï¼ˆå…¬å¸vsæ°´æœï¼‰
å°‘æ ·æœ¬å­¦ä¹ ï¼šæ–°ç±»å‹å®ä½“


ğŸ—ï¸ é¡¹ç›®ï¼šé‡‘èé¢†åŸŸNER

è¯†åˆ«é‡‘èå®ä½“ï¼ˆå…¬å¸ã€äº§å“ã€æŒ‡æ ‡ï¼‰
æ„å»ºé‡‘èçŸ¥è¯†å›¾è°±
å®ä½“é“¾æ¥ä¸æ ‡å‡†åŒ–
è·¨é¢†åŸŸè¿ç§»è¯„ä¼° ğŸ†•




ç¬¬7ç« ï¼šå…³ç³»æŠ½å– - å‘ç°å®ä½“é—´çš„ç§˜å¯†

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•ä»"ä¹”å¸ƒæ–¯åˆ›ç«‹äº†è‹¹æœå…¬å¸"ä¸­æå–åˆ›å§‹äººå…³ç³»ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£å…³ç³»æŠ½å–çš„ç±»å‹
âœ“ æŒæ¡ç›‘ç£å­¦ä¹ æ–¹æ³•
âœ“ å­¦ä¼šè¿œç¨‹ç›‘ç£æŠ€æœ¯
âœ“ å®ç°ç«¯åˆ°ç«¯å…³ç³»æŠ½å–
âœ“ æŒæ¡å¼€æ”¾åŸŸå…³ç³»æŠ½å– ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ”— å…³ç³»æŠ½å–ä»»åŠ¡å®šä¹‰

å®ä½“å…³ç³»ä¸‰å…ƒç»„ï¼š(ä¹”å¸ƒæ–¯, åˆ›ç«‹, è‹¹æœå…¬å¸)
å…³ç³»ç±»å‹å®šä¹‰ä¸æœ¬ä½“è®¾è®¡
Pipeline vs Jointæ–¹æ³•


ğŸŒ å¼€æ”¾å…³ç³»æŠ½å– (OpenIE) ğŸ†•
python# ä½¿ç”¨Stanford OpenIE
def open_relation_extraction(text):
"""ä¸éœ€è¦é¢„å®šä¹‰å…³ç³»ç±»å‹çš„æŠ½å–"""
# è¾“å…¥: "é©¬æ–¯å…‹æ”¶è´­äº†æ¨ç‰¹ï¼Œå¹¶å°†å…¶æ”¹åä¸ºX"
# è¾“å‡º:
# (é©¬æ–¯å…‹, æ”¶è´­äº†, æ¨ç‰¹)
# (é©¬æ–¯å…‹, å°†å…¶æ”¹åä¸º, X)

triples = openie_extractor.extract(text)

# å¯¹æ¯”ç›‘ç£å­¦ä¹ æ–¹æ³•
# ç›‘ç£å­¦ä¹ ï¼šéœ€è¦é¢„å®šä¹‰"æ”¶è´­"ã€"æ”¹å"ç­‰å…³ç³»
# OpenIEï¼šè‡ªåŠ¨å‘ç°æ‰€æœ‰å¯èƒ½çš„å…³ç³»

return triples

ğŸ“Š ç‰¹å¾å·¥ç¨‹çš„è‰ºæœ¯

è¯æ±‡ç‰¹å¾ï¼šå®ä½“é—´çš„è¯
å¥æ³•ç‰¹å¾ï¼šä¾å­˜è·¯å¾„
è¯­ä¹‰ç‰¹å¾ï¼šè¯å‘é‡ã€ä½ç½®ç¼–ç 


ğŸš€ æ·±åº¦å­¦ä¹ æ–¹æ³•

CNNå…³ç³»åˆ†ç±»
Attentionæœºåˆ¶çš„åº”ç”¨
GCNå¤„ç†ä¾å­˜æ ‘


ğŸ’¡ è¿œç¨‹ç›‘ç£å­¦ä¹ 

åˆ©ç”¨çŸ¥è¯†åº“è‡ªåŠ¨æ ‡æ³¨
å™ªå£°æ•°æ®å¤„ç†
Multi-Instance Learning


ğŸ“ˆ å…³ç³»æŠ½å–è¯„ä¼°ç­–ç•¥ ğŸ†•
pythondef evaluate_relation_extraction(predictions, gold_standard):
"""å…¨é¢è¯„ä¼°å…³ç³»æŠ½å–ç»“æœ"""
# ä¸¥æ ¼è¯„ä¼°ï¼šå®ä½“å’Œå…³ç³»éƒ½æ­£ç¡®
strict_p, strict_r, strict_f1 = evaluate_strict(
predictions, gold_standard
)

# å®½æ¾è¯„ä¼°ï¼šåªè¦å…³ç³»ç±»å‹æ­£ç¡®
relax_p, relax_r, relax_f1 = evaluate_relaxed(
predictions, gold_standard
)

# è¾¹ç•Œè¯„ä¼°ï¼šå®ä½“è¾¹ç•Œéƒ¨åˆ†é‡å ä¹Ÿç®—
boundary_scores = evaluate_with_boundary(
predictions, gold_standard
)

print("è¯„ä¼°ç»“æœå¯¹æ¯”:")
print(f"ä¸¥æ ¼åŒ¹é… F1: {strict_f1:.3f}")
print(f"å®½æ¾åŒ¹é… F1: {relax_f1:.3f}")
print(f"è¾¹ç•ŒåŒ¹é… F1: {boundary_scores['f1']:.3f}")

# é”™è¯¯åˆ†æ
error_analysis(predictions, gold_standard)

ğŸ¯ é¡¹ç›®ï¼šç®€å†ä¿¡æ¯æŠ½å–

æ•™è‚²ç»å†æŠ½å–
å·¥ä½œç»å†ç»“æ„åŒ–
æŠ€èƒ½å›¾è°±æ„å»º
è·¨æ¨¡æ¿æ³›åŒ–æµ‹è¯• ğŸ†•




ç¬¬8ç« ï¼šé—®ç­”ç³»ç»Ÿ - æ‰“é€ ä½ çš„æ™ºèƒ½åŠ©æ‰‹

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•æ„å»ºä¸€ä¸ªå®¢æœæœºå™¨äººï¼Œèƒ½å‡†ç¡®ç†è§£ç”¨æˆ·é—®é¢˜å¹¶ç»™å‡ºå¸®åŠ©ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£ä¸åŒç±»å‹çš„é—®ç­”ç³»ç»Ÿ
âœ“ æŒæ¡æ£€ç´¢å¼å’Œç”Ÿæˆå¼æ–¹æ³•
âœ“ å­¦ä¼šæ„å›¾è¯†åˆ«å’Œæ§½å¡«å……
âœ“ æ„å»ºç«¯åˆ°ç«¯é—®ç­”ç³»ç»Ÿ
âœ“ æŒæ¡å¯¹è¯çŠ¶æ€è¿½è¸ªæŠ€æœ¯ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

â“ é—®ç­”ç³»ç»Ÿåˆ†ç±»

äº‹å®å‹QAï¼š"ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
åˆ—è¡¨å‹QAï¼š"åˆ—ä¸¾5ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶"
å®šä¹‰å‹QAï¼š"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
æ¯”è¾ƒå‹QAï¼š"PyTorchå’ŒTensorFlowçš„åŒºåˆ«"


ğŸ” æ£€ç´¢å¼é—®ç­”

é—®é¢˜ç†è§£ä¸æŸ¥è¯¢æ‰©å±•
æ–‡æ¡£æ£€ç´¢ä¸æ®µè½å®šä½
ç­”æ¡ˆæŠ½å–ä¸æ’åº


ğŸ’¬ ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿ
python# å¯¹è¯çŠ¶æ€è¿½è¸ª (DST) ğŸ†•
class DialogueStateTracker:
def __init__(self):
self.state = {
'intent': None,
'slots': {},
'history': [],
'context': {}
}

def update(self, user_input, nlu_output):
# æ›´æ–°æ„å›¾
self.state['intent'] = nlu_output['intent']

# æ›´æ–°æ§½ä½ï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
for slot, value in nlu_output['slots'].items():
if value is not None:
self.state['slots'][slot] = value

# è®°å½•å¯¹è¯å†å²
self.state['history'].append({
'user': user_input,
'state': copy.deepcopy(self.state)
})

# å¤„ç†å¤šè½®å¯¹è¯ä¸­çš„æŒ‡ä»£æ¶ˆè§£
self.resolve_references()

return self.state

# ç¤ºä¾‹å¯¹è¯æµç¨‹
"""
ç”¨æˆ·: "å¸®æˆ‘è®¢ä¸€å¼ æ˜å¤©å»åŒ—äº¬çš„æœºç¥¨"
çŠ¶æ€: {intent: è®¢æœºç¥¨, slots: {date: æ˜å¤©, dest: åŒ—äº¬}}

ç”¨æˆ·: "è¦ä¸Šåˆçš„"
çŠ¶æ€: {intent: è®¢æœºç¥¨, slots: {date: æ˜å¤©, dest: åŒ—äº¬, time: ä¸Šåˆ}}

ç”¨æˆ·: "ä»ä¸Šæµ·å‡ºå‘"
çŠ¶æ€: {intent: è®¢æœºç¥¨, slots: {date: æ˜å¤©, dest: åŒ—äº¬,
time: ä¸Šåˆ, origin: ä¸Šæµ·}}
"""

ğŸ§  é˜…è¯»ç†è§£æŠ€æœ¯

BiDAFæ³¨æ„åŠ›æµ
BERT for QA
å¤šè·³æ¨ç†


ğŸ“Š é—®ç­”ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡ ğŸ†•
pythondef evaluate_qa_system(qa_system, test_set):
"""å…¨é¢è¯„ä¼°é—®ç­”ç³»ç»Ÿ"""
metrics = {}

# 1. ç­”æ¡ˆå‡†ç¡®æ€§
exact_match = calculate_exact_match(qa_system, test_set)
f1_score = calculate_token_f1(qa_system, test_set)

# 2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨BERTï¼‰
semantic_sim = calculate_semantic_similarity(
qa_system, test_set
)

# 3. äººç±»è¯„ä¼°æŒ‡æ ‡
human_eval = {
'relevance': 0.0,  # ç­”æ¡ˆç›¸å…³æ€§
'completeness': 0.0,  # ç­”æ¡ˆå®Œæ•´æ€§
'fluency': 0.0  # ç­”æ¡ˆæµç•…æ€§
}

# 4. ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
latency = measure_response_time(qa_system, test_set)
throughput = measure_throughput(qa_system)

print(f"å‡†ç¡®æ€§æŒ‡æ ‡:")
print(f"  ç²¾ç¡®åŒ¹é…: {exact_match:.3f}")
print(f"  F1åˆ†æ•°: {f1_score:.3f}")
print(f"  è¯­ä¹‰ç›¸ä¼¼åº¦: {semantic_sim:.3f}")
print(f"\næ€§èƒ½æŒ‡æ ‡:")
print(f"  å¹³å‡å»¶è¿Ÿ: {latency:.2f}ms")
print(f"  ååé‡: {throughput:.1f} QPS")

return metrics

ğŸ—ï¸ é¡¹ç›®ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

FAQæ£€ç´¢
æ„å›¾è¯†åˆ«
å¤šè½®å¯¹è¯ç®¡ç†
çŸ¥è¯†åº“é›†æˆ
å¯¹è¯è´¨é‡è¯„ä¼° ğŸ†•




ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ·±åº¦å­¦ä¹ NLPï¼ˆ4å‘¨ï¼‰
ç¬¬9ç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œ - è®©æ¨¡å‹è®°ä½è¿‡å»

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©AIç»­å†™å°è¯´ï¼Œä¿æŒæƒ…èŠ‚è¿è´¯æ€§ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£RNNå¤„ç†åºåˆ—çš„åŸç†
âœ“ æŒæ¡LSTMå’ŒGRU
âœ“ å­¦ä¼šseq2seqæ¶æ„
âœ“ å®ç°æ–‡æœ¬ç”Ÿæˆåº”ç”¨
âœ“ æŒæ¡ç”Ÿæˆè´¨é‡è¯„ä¼° ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ”„ RNNçš„è®°å¿†æœºåˆ¶
å¯è§†åŒ–æ—¶é—´æ­¥å±•å¼€ï¼š
xâ‚ â†’ RNN â†’ hâ‚
â†“
xâ‚‚ â†’ RNN â†’ hâ‚‚  (åŒ…å«xâ‚çš„ä¿¡æ¯)
â†“
xâ‚ƒ â†’ RNN â†’ hâ‚ƒ  (åŒ…å«xâ‚,xâ‚‚çš„ä¿¡æ¯)

ğŸšª LSTMçš„é—¨æ§æœºåˆ¶

é—å¿˜é—¨ï¼šå†³å®šå¿˜è®°ä»€ä¹ˆ
è¾“å…¥é—¨ï¼šå†³å®šè®°ä½ä»€ä¹ˆ
è¾“å‡ºé—¨ï¼šå†³å®šè¾“å‡ºä»€ä¹ˆ
å¯è§†åŒ–é—¨æ§è¿‡ç¨‹


ğŸ”€ Seq2Seqæ¶æ„

Encoder-Decoderç»“æ„
Teacher Forcingè®­ç»ƒ
Beam Searchè§£ç 


ğŸ“Š ç”Ÿæˆæ–‡æœ¬çš„è¯„ä¼°æŒ‘æˆ˜ ğŸ†•
pythondef evaluate_text_generation(generated_texts, references):
"""å¤šç»´åº¦è¯„ä¼°ç”Ÿæˆæ–‡æœ¬è´¨é‡"""

# 1. è‡ªåŠ¨åŒ–æŒ‡æ ‡
bleu_scores = calculate_bleu(generated_texts, references)
rouge_scores = calculate_rouge(generated_texts, references)

# 2. åŸºäºè¯­ä¹‰çš„æŒ‡æ ‡
bertscore = calculate_bertscore(generated_texts, references)

# 3. å¤šæ ·æ€§æŒ‡æ ‡
diversity = {
'distinct_1': calculate_distinct_ngrams(generated_texts, 1),
'distinct_2': calculate_distinct_ngrams(generated_texts, 2),
'self_bleu': calculate_self_bleu(generated_texts)
}

# 4. ç‰¹å®šä»»åŠ¡æŒ‡æ ‡ï¼ˆå¦‚è¯—æ­Œï¼‰
if task == 'poetry':
rhyme_score = check_rhyme_scheme(generated_texts)
meter_score = check_meter(generated_texts)

print("âš ï¸ é‡è¦æé†’ï¼š")
print("BLEU/ROUGEé«˜åˆ†â‰ é«˜è´¨é‡ï¼")
print("ä¾‹å¦‚ï¼š'æˆ‘æˆ‘æˆ‘æˆ‘æˆ‘'å¯èƒ½æœ‰é«˜BLEUä½†è´¨é‡å¾ˆå·®")
print("\nå»ºè®®ç»“åˆäººå·¥è¯„ä¼°ï¼Œå…³æ³¨:")
print("- è¯­ä¹‰è¿è´¯æ€§")
print("- åˆ›é€ æ€§")
print("- å®ç”¨æ€§")

return comprehensive_scores

âš¡ æ³¨æ„åŠ›æœºåˆ¶åˆæ¢

è§£å†³é•¿åºåˆ—é—®é¢˜
å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
ä¸åŒç±»å‹çš„æ³¨æ„åŠ›


ğŸ“ é¡¹ç›®ï¼šè¯—æ­Œç”Ÿæˆå™¨

æ•°æ®æ”¶é›†ä¸é¢„å¤„ç†
æ¨¡å‹è®­ç»ƒæŠ€å·§
éŸµå¾‹æ§åˆ¶
ä¸»é¢˜å¼•å¯¼ç”Ÿæˆ
ç”Ÿæˆè´¨é‡è¯„ä¼° ğŸ†•




ç¬¬10ç« ï¼šTransformer - æ³¨æ„åŠ›å°±æ˜¯å…¨éƒ¨

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šä¸ºä»€ä¹ˆTransformerèƒ½å¤Ÿä¸€ç»ŸNLPæ±Ÿæ¹–ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æ·±å…¥ç†è§£è‡ªæ³¨æ„åŠ›æœºåˆ¶
âœ“ æŒæ¡Transformeræ¶æ„
âœ“ å­¦ä¼šä½ç½®ç¼–ç è®¾è®¡
âœ“ å®ç°mini-Transformer
âœ“ ç†è§£æ¶æ„è®¾è®¡çš„æƒè¡¡ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ‘ï¸ è‡ªæ³¨æ„åŠ›çš„é©å‘½

æŠ›å¼ƒå¾ªç¯ï¼Œæ‹¥æŠ±å¹¶è¡Œ
Qã€Kã€Vçš„å«ä¹‰
ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›


ğŸ—ï¸ Transformeræ¶æ„è¯¦è§£

å¤šå¤´æ³¨æ„åŠ›ï¼ˆå‚è€ƒæä¾›çš„æ–‡æ¡£ï¼‰
å‰é¦ˆç½‘ç»œ
æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–
ç¼–ç å™¨-è§£ç å™¨ç»“æ„


ğŸ”¬ æ¶æ„è®¾è®¡çš„ç§‘å­¦ ğŸ†•
pythondef architecture_ablation_study():
"""æ¢ç´¢Transformerè®¾è®¡é€‰æ‹©çš„å½±å“"""

variations = {
'baseline': TransformerConfig(),
'no_residual': TransformerConfig(use_residual=False),
'no_layernorm': TransformerConfig(use_layernorm=False),
'single_head': TransformerConfig(num_heads=1),
'no_ffn': TransformerConfig(use_ffn=False),
'relu_vs_gelu': TransformerConfig(activation='relu')
}

results = {}
for name, config in variations.items():
model = build_transformer(config)

# è¯„ä¼°å¤šä¸ªç»´åº¦
results[name] = {
'performance': evaluate_performance(model),
'training_stability': measure_gradient_flow(model),
'convergence_speed': measure_convergence(model),
'parameter_efficiency': count_parameters(model)
}

# å¯è§†åŒ–å¯¹æ¯”
plot_ablation_results(results)

# å…³é”®å‘ç°
print("ğŸ” å…³é”®å‘ç°:")
print("1. æ®‹å·®è¿æ¥å¯¹æ·±å±‚ç½‘ç»œè‡³å…³é‡è¦")
print("2. å¤šå¤´æ³¨æ„åŠ›æ˜¾è‘—æå‡æ€§èƒ½")
print("3. LayerNormæé«˜è®­ç»ƒç¨³å®šæ€§")

ğŸ“ ä½ç½®ç¼–ç çš„è‰ºæœ¯

ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ä¿¡æ¯
æ­£å¼¦ä½ç½®ç¼–ç 
å¯å­¦ä¹ ä½ç½®ç¼–ç 
ç›¸å¯¹ä½ç½®ç¼–ç 


âš¡ è®­ç»ƒæŠ€å·§

Learning Rate Schedule
Label Smoothing
Dropoutç­–ç•¥


ğŸ”§ é¡¹ç›®ï¼šä»é›¶å®ç°Transformer

é€å±‚æ„å»º
å¯è§†åŒ–æ³¨æ„åŠ›
æœºå™¨ç¿»è¯‘ä»»åŠ¡
æ€§èƒ½ä¼˜åŒ–
ä¸RNNå¯¹æ¯”å®éªŒ ğŸ†•




ç¬¬11ç« ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ - ç«™åœ¨å·¨äººçš„è‚©è†€ä¸Š

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•ç”¨100GBæ–‡æœ¬è®­ç»ƒå‡ºç†è§£ä¸–ç•Œçš„AIï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼
âœ“ æŒæ¡BERTç³»åˆ—æ¨¡å‹
âœ“ å­¦ä¼šGPTç³»åˆ—æ¨¡å‹
âœ“ å®è·µæ¨¡å‹å¾®è°ƒæŠ€æœ¯
âœ“ ç†è§£é¢„è®­ç»ƒçš„æœ¬è´¨ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ­ BERTï¼šåŒå‘ç¼–ç å™¨
é¢„è®­ç»ƒä»»åŠ¡å¯è§†åŒ–ï¼š
MLM: æˆ‘çˆ±[MASK]NLP â†’ æˆ‘çˆ±å­¦ä¹ NLP
NSP: (å¥å­A, å¥å­B) â†’ æ˜¯å¦ç›¸é‚»ï¼Ÿ

ğŸ”® GPTï¼šè‡ªå›å½’è¯­è¨€æ¨¡å‹

å•å‘æ³¨æ„åŠ›è®¾è®¡
Zero-shotèƒ½åŠ›
Promptå·¥ç¨‹


ğŸ“Š é¢„è®­ç»ƒæ•ˆæœçš„ç§‘å­¦åˆ†æ ğŸ†•
pythondef analyze_pretrained_models():
"""æ·±å…¥åˆ†æé¢„è®­ç»ƒæ¨¡å‹å­¦åˆ°äº†ä»€ä¹ˆ"""

# 1. æ¢æµ‹ä»»åŠ¡ï¼ˆProbing Tasksï¼‰
probing_results = {}

# è¯æ€§æ ‡æ³¨æ¢æµ‹
pos_accuracy = probe_pos_tagging(model)

# å¥æ³•ç»“æ„æ¢æµ‹
syntax_score = probe_syntax_trees(model)

# è¯­ä¹‰è§’è‰²æ¢æµ‹
srl_score = probe_semantic_roles(model)

# 2. æ³¨æ„åŠ›æ¨¡å¼åˆ†æ
attention_patterns = analyze_attention_patterns(model)

# 3. è¡¨ç¤ºç›¸ä¼¼æ€§åˆ†æ
representation_analysis = {
'anisotropy': measure_anisotropy(model),
'layer_similarity': compute_layer_similarity(model),
'token_uniformity': measure_token_uniformity(model)
}

# å¯è§†åŒ–å‘ç°
visualize_probing_results(probing_results)

print("ğŸ” é¢„è®­ç»ƒæ¨¡å‹å­¦åˆ°äº†:")
print(f"- è¯æ€§ä¿¡æ¯: {pos_accuracy:.2%}")
print(f"- å¥æ³•ç»“æ„: {syntax_score:.2%}")
print(f"- è¯­ä¹‰è§’è‰²: {srl_score:.2%}")

ğŸŒˆ é¢„è®­ç»ƒæ¨¡å‹å…¨å®¶æ¡¶

RoBERTaï¼šæ›´å¥½çš„BERT
ALBERTï¼šå‚æ•°å…±äº«
T5ï¼šç»Ÿä¸€æ¡†æ¶
ELECTRAï¼šåˆ¤åˆ«å¼é¢„è®­ç»ƒ


ğŸ¯ å¾®è°ƒçš„è‰ºæœ¯

å…¨é‡å¾®è°ƒvså‚æ•°é«˜æ•ˆå¾®è°ƒ
Adapterã€LoRAã€Prefix-Tuning
å¤šä»»åŠ¡å­¦ä¹ 


ğŸ’¼ é¡¹ç›®ï¼šé¢†åŸŸBERTè®­ç»ƒ

é¢†åŸŸæ•°æ®æ”¶é›†
ç»§ç»­é¢„è®­ç»ƒ
ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°
æ¨¡å‹å‹ç¼©éƒ¨ç½²
è¿ç§»å­¦ä¹ æ•ˆæœåˆ†æ ğŸ†•




ç¬¬12ç« ï¼šå¤§è¯­è¨€æ¨¡å‹æ—¶ä»£ - ChatGPTçš„å¥¥ç§˜

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šä»GPT-3åˆ°ChatGPTï¼ŒAIæ˜¯å¦‚ä½•å­¦ä¼šå¯¹è¯çš„ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£å¤§æ¨¡å‹çš„æ¶Œç°èƒ½åŠ›
âœ“ æŒæ¡RLHFæŠ€æœ¯
âœ“ å­¦ä¼šPromptå·¥ç¨‹
âœ“ å®è·µLLMåº”ç”¨å¼€å‘
âœ“ ç†è§£Agentè®¾è®¡æ¨¡å¼ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸš€ è§„æ¨¡æ³•åˆ™ä¸æ¶Œç°

æ¨¡å‹è§„æ¨¡vsèƒ½åŠ›æ›²çº¿
Few-shotå’ŒZero-shot
Chain-of-Thoughtæ¨ç†


ğŸ¯ RLHFï¼šè®©AIæ›´åƒäºº

SFTç›‘ç£å¾®è°ƒ
Reward Modelè®­ç»ƒ
PPOå¼ºåŒ–å­¦ä¹ 
Constitutional AI


ğŸ¤– Agentè®¾è®¡æ¨¡å¼è¯¦è§£ ğŸ†•
python# ReActæ¡†æ¶å®ç°
class ReActAgent:
"""Reasoning + Acting ç»Ÿä¸€æ¡†æ¶"""

def __init__(self, llm, tools):
self.llm = llm
self.tools = tools
self.memory = []

def think(self, observation):
"""æ¨ç†ï¼šåŸºäºè§‚å¯Ÿç”Ÿæˆæ€è€ƒ"""
thought_prompt = f"""
ä»»åŠ¡: {self.task}
è§‚å¯Ÿ: {observation}

åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œåˆ†æå½“å‰çŠ¶å†µå¹¶æ€è€ƒä¸‹ä¸€æ­¥ã€‚
æ€è€ƒ:
"""
thought = self.llm.generate(thought_prompt)
return thought

def act(self, thought):
"""è¡ŒåŠ¨ï¼šåŸºäºæ€è€ƒé€‰æ‹©è¡ŒåŠ¨"""
action_prompt = f"""
æ€è€ƒ: {thought}
å¯ç”¨å·¥å…·: {list(self.tools.keys())}

é€‰æ‹©æœ€åˆé€‚çš„è¡ŒåŠ¨ã€‚
è¡ŒåŠ¨:
"""
action = self.llm.generate(action_prompt)
return self.parse_action(action)

def execute(self, task):
"""ReActä¸»å¾ªç¯"""
self.task = task
observation = f"ä»»åŠ¡å¼€å§‹: {task}"

while not self.is_complete():
# Reasoning
thought = self.think(observation)
self.memory.append(('thought', thought))

# Acting
action = self.act(thought)
self.memory.append(('action', action))

# æ‰§è¡Œå¹¶è·å–æ–°è§‚å¯Ÿ
observation = self.tools[action['tool']](
**action['params']
)
self.memory.append(('observation', observation))

return self.generate_final_answer()

# ä½¿ç”¨ç¤ºä¾‹
agent = ReActAgent(llm=GPT4(), tools={
'search': web_search,
'calculate': calculator,
'python': python_repl
})

result = agent.execute(
"å¸®æˆ‘åˆ†æè‹¹æœå…¬å¸æœ€æ–°è´¢æŠ¥å¹¶è®¡ç®—åŒæ¯”å¢é•¿ç‡"
)

ğŸ’¡ Promptå·¥ç¨‹ç²¾è¦
åŸºç¡€Prompt: "ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½"

ä¼˜åŒ–Prompt: "ä½ æ˜¯ä¸“ä¸šç¿»è¯‘ã€‚è¯·å°†ä¸‹é¢çš„ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼Œ
ä¿æŒåŸæ„å¹¶ç¬¦åˆè‹±è¯­è¡¨è¾¾ä¹ æƒ¯ï¼š
ä¸­æ–‡ï¼šä½ å¥½
è‹±æ–‡ï¼š"

ğŸ“Š LLMè¯„ä¼°çš„æ–°æŒ‘æˆ˜ ğŸ†•
pythondef evaluate_llm_comprehensive(model, test_suite):
"""å…¨é¢è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹"""

# 1. åŸºç¡€èƒ½åŠ›è¯„ä¼°
basic_metrics = {
'mmlu': evaluate_on_mmlu(model),  # å¤šé¢†åŸŸçŸ¥è¯†
'humaneval': evaluate_coding(model),  # ç¼–ç¨‹èƒ½åŠ›
'gsm8k': evaluate_math(model)  # æ•°å­¦æ¨ç†
}

# 2. å¯¹è¯èƒ½åŠ›è¯„ä¼°
dialogue_metrics = {
'coherence': evaluate_coherence(model),
'helpfulness': evaluate_helpfulness(model),
'harmlessness': evaluate_safety(model)
}

# 3. ç‰¹æ®Šèƒ½åŠ›è¯„ä¼°
special_metrics = {
'hallucination_rate': measure_hallucination(model),
'instruction_following': test_instruction_following(model),
'reasoning_depth': test_chain_of_thought(model)
}

# 4. äººç±»è¯„ä¼°ï¼ˆEloè¯„åˆ†ï¼‰
elo_score = run_human_evaluation(model)

# 5. åè§å’Œå…¬å¹³æ€§
bias_analysis = analyze_model_bias(model)

return comprehensive_report(
basic_metrics,
dialogue_metrics,
special_metrics,
elo_score,
bias_analysis
)

ğŸ”§ LLMåº”ç”¨å¼€å‘

LangChainæ¡†æ¶
å‘é‡æ•°æ®åº“é›†æˆ
Agentè®¾è®¡æ¨¡å¼
é”™è¯¯å¤„ç†ä¸ä¼˜åŒ–


ğŸŒŸ é¡¹ç›®ï¼šRAGé—®ç­”ç³»ç»Ÿ

æ–‡æ¡£åˆ‡åˆ†ä¸ç´¢å¼•
æ£€ç´¢å¢å¼ºç”Ÿæˆ
å¹»è§‰é—®é¢˜å¤„ç†
ç³»ç»Ÿè¯„ä¼°æŒ‡æ ‡
å®éªŒè®¾è®¡ä¸å¯¹æ¯” ğŸ†•




ç¬¬å››éƒ¨åˆ†ï¼šå‰æ²¿ä¸å®æˆ˜ï¼ˆ4å‘¨ï¼‰
ç¬¬13ç« ï¼šå¤šæ¨¡æ€NLP - æ–‡æœ¬ä¸å†å­¤å•

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©AIçœ‹å›¾è¯´è¯ï¼Œç†è§£è¡¨æƒ…åŒ…çš„å«ä¹‰ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£å¤šæ¨¡æ€èåˆåŸç†
âœ“ æŒæ¡è§†è§‰-è¯­è¨€æ¨¡å‹
âœ“ å­¦ä¼šè·¨æ¨¡æ€æ£€ç´¢
âœ“ å®ç°å¤šæ¨¡æ€åº”ç”¨
âœ“ æŒæ¡è¯­éŸ³æ–‡æœ¬èåˆ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ¨ CLIPï¼šå¯¹é½è§†è§‰ä¸è¯­è¨€

å¯¹æ¯”å­¦ä¹ æ¡†æ¶
é›¶æ ·æœ¬å›¾åƒåˆ†ç±»
æ–‡æœ¬-å›¾åƒæ£€ç´¢


ğŸ¤ è¯­éŸ³ä¸æ–‡æœ¬çš„èåˆ ğŸ†•
pythonclass SpeechTextMultiModal:
"""è¯­éŸ³-æ–‡æœ¬å¤šæ¨¡æ€å¤„ç†"""

def __init__(self):
self.asr = WhisperModel()  # è¯­éŸ³è¯†åˆ«
self.tts = FastSpeech2()   # è¯­éŸ³åˆæˆ
self.text_encoder = BertModel()
self.speech_encoder = Wav2Vec2()

def speech_understanding(self, audio, text_context=None):
"""ç»“åˆæ–‡æœ¬ä¸Šä¸‹æ–‡çš„è¯­éŸ³ç†è§£"""
# 1. åŸºç¡€ASR
transcript = self.asr(audio)

# 2. ç»“åˆä¸Šä¸‹æ–‡çº é”™
if text_context:
corrected = self.contextual_correction(
transcript, text_context
)

# 3. æƒ…æ„Ÿå’Œè¯­è°ƒåˆ†æ
prosody = self.analyze_prosody(audio)

# 4. å¤šæ¨¡æ€èåˆç†è§£
understanding = self.fuse_modalities(
text=corrected,
prosody=prosody,
context=text_context
)

return understanding

def analyze_prosody(self, audio):
"""åˆ†æè¯­éŸ³éŸµå¾‹ç‰¹å¾"""
features = {
'pitch': extract_pitch(audio),
'energy': extract_energy(audio),
'speaking_rate': calculate_rate(audio),
'emotion': classify_emotion(audio)
}
return features

ğŸ–¼ï¸ å›¾åƒæè¿°ç”Ÿæˆ

Encoder-Decoderæ¶æ„
æ³¨æ„åŠ›å¯è§†åŒ–
è¯„ä¼°æŒ‡æ ‡ï¼šBLEUã€ROUGEã€CIDErã€SPICE


ğŸ­ è§†è§‰é—®ç­”ï¼ˆVQAï¼‰

å¤šæ¨¡æ€ç‰¹å¾èåˆ
æ¨ç†èƒ½åŠ›è¦æ±‚
æ•°æ®é›†ä¸åŸºå‡†


ğŸš€ æœ€æ–°è¿›å±•

DALL-Eï¼šæ–‡æœ¬ç”Ÿæˆå›¾åƒ
Flamingoï¼šå°‘æ ·æœ¬å¤šæ¨¡æ€å­¦ä¹ 
BLIP-2ï¼šé«˜æ•ˆè§†è§‰-è¯­è¨€é¢„è®­ç»ƒ


ğŸ“Š å¤šæ¨¡æ€è¯„ä¼°çš„æŒ‘æˆ˜ ğŸ†•
pythondef evaluate_multimodal_model(model, test_data):
"""å¤šæ¨¡æ€æ¨¡å‹çš„ç»¼åˆè¯„ä¼°"""

# 1. å•æ¨¡æ€æ€§èƒ½
text_only = evaluate_text_understanding(model)
vision_only = evaluate_vision_understanding(model)
speech_only = evaluate_speech_understanding(model)

# 2. è·¨æ¨¡æ€å¯¹é½
alignment_scores = {
'text_vision': measure_tv_alignment(model),
'text_speech': measure_ts_alignment(model),
'vision_speech': measure_vs_alignment(model)
}

# 3. èåˆæ•ˆæœï¼ˆæ˜¯å¦1+1>2ï¼‰
fusion_gain = measure_fusion_effectiveness(
model,
single_modal_baselines=[text_only, vision_only]
)

# 4. é²æ£’æ€§æµ‹è¯•
robustness = test_modal_dropout(model)

print("å¤šæ¨¡æ€è¯„ä¼°æŠ¥å‘Š:")
print(f"èåˆå¢ç›Š: {fusion_gain:.2%}")
print(f"æ¨¡æ€ç¼ºå¤±é²æ£’æ€§: {robustness:.2%}")

return comprehensive_results

ğŸ’¡ é¡¹ç›®ï¼šæ™ºèƒ½ç›¸å†ŒåŠ©æ‰‹

å›¾ç‰‡è‡ªåŠ¨æ ‡æ³¨
è‡ªç„¶è¯­è¨€æœç´¢
å›¾ç‰‡é—®ç­”
æ•…äº‹ç”Ÿæˆ
è¯­éŸ³äº¤äº’åŠŸèƒ½ ğŸ†•




ç¬¬14ç« ï¼šçŸ¥è¯†å¢å¼ºNLP - è®©AIæ›´æœ‰å­¦è¯†

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©æ¨¡å‹ä¸ä»…ä¼šè¯´è¯ï¼Œè¿˜æ‡‚å¾—èƒŒåçš„çŸ¥è¯†ï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ ç†è§£çŸ¥è¯†å›¾è°±çš„ä»·å€¼
âœ“ æŒæ¡çŸ¥è¯†èåˆæŠ€æœ¯
âœ“ å­¦ä¼šçŸ¥è¯†æ¨ç†æ–¹æ³•
âœ“ æ„å»ºçŸ¥è¯†å¢å¼ºåº”ç”¨
âœ“ å®è·µçŸ¥è¯†æ›´æ–°ç­–ç•¥ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±åŸºç¡€

ä¸‰å…ƒç»„è¡¨ç¤º
æœ¬ä½“è®¾è®¡
çŸ¥è¯†æŠ½å–pipeline


ğŸ”— çŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒ

ERNIEï¼šå®ä½“æ„ŸçŸ¥
KEPLERï¼šçŸ¥è¯†åµŒå…¥
CoLAKEï¼šè¯­è¨€-çŸ¥è¯†è”åˆ


ğŸ§  çŸ¥è¯†æ¨ç†æŠ€æœ¯

å›¾ç¥ç»ç½‘ç»œåº”ç”¨
å¤šè·³æ¨ç†
å¸¸è¯†æ¨ç†æŒ‘æˆ˜


ğŸ’¡ çŸ¥è¯†ç¼–è¾‘ä¸æ›´æ–° ğŸ†•
pythonclass KnowledgeEditor:
"""æ¨¡å‹çŸ¥è¯†çš„ç²¾ç¡®ç¼–è¾‘"""

def __init__(self, base_model):
self.model = base_model
self.edit_history = []

def locate_knowledge(self, fact):
"""å®šä½çŸ¥è¯†åœ¨æ¨¡å‹ä¸­çš„å­˜å‚¨ä½ç½®"""
# ä½¿ç”¨å› æœè¿½è¸ª
neurons = self.causal_tracing(fact)

# è¯†åˆ«å…³é”®å±‚å’Œç¥ç»å…ƒ
critical_neurons = self.identify_critical_neurons(
neurons,
threshold=0.8
)

return critical_neurons

def edit_fact(self, old_fact, new_fact):
"""ç²¾ç¡®ç¼–è¾‘å•ä¸ªäº‹å®"""
# 1. å®šä½æ—§çŸ¥è¯†
locations = self.locate_knowledge(old_fact)

# 2. è®¡ç®—ç¼–è¾‘å‘é‡
edit_vector = self.compute_edit_vector(
old_fact,
new_fact,
locations
)

# 3. åº”ç”¨ç¼–è¾‘
self.apply_edit(locations, edit_vector)

# 4. éªŒè¯ç¼–è¾‘æ•ˆæœ
success = self.verify_edit(old_fact, new_fact)

# 5. æ£€æŸ¥å‰¯ä½œç”¨
side_effects = self.check_side_effects()

self.edit_history.append({
'old': old_fact,
'new': new_fact,
'success': success,
'side_effects': side_effects
})

return success, side_effects

def batch_edit(self, fact_updates):
"""æ‰¹é‡çŸ¥è¯†æ›´æ–°"""
# æ£€æµ‹çŸ¥è¯†å†²çª
conflicts = self.detect_conflicts(fact_updates)

if conflicts:
resolved = self.resolve_conflicts(conflicts)
fact_updates = resolved

# ä¼˜åŒ–ç¼–è¾‘é¡ºåº
ordered_updates = self.optimize_edit_order(fact_updates)

# æ‰§è¡Œç¼–è¾‘
results = []
for old, new in ordered_updates:
result = self.edit_fact(old, new)
results.append(result)

return results

ğŸ“Š çŸ¥è¯†ä¸€è‡´æ€§éªŒè¯ ğŸ†•
pythondef verify_knowledge_consistency(model, knowledge_base):
"""éªŒè¯æ¨¡å‹çŸ¥è¯†ä¸çŸ¥è¯†åº“çš„ä¸€è‡´æ€§"""

inconsistencies = []

# 1. äº‹å®éªŒè¯
for fact in knowledge_base.facts:
model_answer = model.query(fact.question)
if model_answer != fact.answer:
inconsistencies.append({
'type': 'factual',
'expected': fact.answer,
'actual': model_answer
})

# 2. æ¨ç†ä¸€è‡´æ€§
for rule in knowledge_base.rules:
conclusion = model.reason(rule.premises)
if conclusion != rule.conclusion:
inconsistencies.append({
'type': 'reasoning',
'rule': rule,
'model_conclusion': conclusion
})

# 3. æ—¶åºä¸€è‡´æ€§
temporal_errors = check_temporal_consistency(
model,
knowledge_base
)

consistency_score = 1 - len(inconsistencies) / len(knowledge_base)

return consistency_score, inconsistencies

ğŸ—ï¸ é¡¹ç›®ï¼šåŒ»ç–—é—®ç­”ç³»ç»Ÿ

åŒ»å­¦çŸ¥è¯†å›¾è°±æ„å»º
ç—‡çŠ¶-ç–¾ç—…æ¨ç†
ç”¨è¯å»ºè®®ç”Ÿæˆ
å®‰å…¨æ€§ä¿éšœ
çŸ¥è¯†æ›´æ–°æœºåˆ¶ ğŸ†•




ç¬¬15ç« ï¼šé«˜æ•ˆNLP - è®©æ¨¡å‹è·‘å¾—æ›´å¿«

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•è®©BERTåœ¨æ‰‹æœºä¸Šå®æ—¶è¿è¡Œï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æŒæ¡æ¨¡å‹å‹ç¼©æŠ€æœ¯
âœ“ ç†è§£çŸ¥è¯†è’¸é¦åŸç†
âœ“ å­¦ä¼šé‡åŒ–å’Œå‰ªæ
âœ“ å®ç°ç«¯ä¾§éƒ¨ç½²
âœ“ ç†è§£æ•ˆç‡-æ€§èƒ½æƒè¡¡ ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ—œï¸ æ¨¡å‹å‹ç¼©å…¨æ™¯
BERT-base (110M) â†’ DistilBERT (66M) â†’ TinyBERT (14M)
å‡†ç¡®ç‡: 100%      â†’     97%        â†’      94%
é€Ÿåº¦:    1x       â†’     2x         â†’      10x

ğŸ“ çŸ¥è¯†è’¸é¦æ·±åº¦è§£æ

è½¯æ ‡ç­¾çš„ä»·å€¼
ç‰¹å¾è’¸é¦
æ³¨æ„åŠ›è½¬ç§»
æ¸è¿›å¼è’¸é¦


ğŸ“Š å‹ç¼©æ•ˆæœçš„ç§‘å­¦è¯„ä¼° ğŸ†•
pythondef evaluate_model_compression(original, compressed):
"""å…¨é¢è¯„ä¼°æ¨¡å‹å‹ç¼©æ•ˆæœ"""

metrics = {}

# 1. æ€§èƒ½ä¿æŒç‡
original_perf = evaluate_performance(original)
compressed_perf = evaluate_performance(compressed)
metrics['performance_retention'] = compressed_perf / original_perf

# 2. å‹ç¼©ç‡
metrics['size_reduction'] = {
'parameters': count_parameters(compressed) / count_parameters(original),
'memory': measure_memory(compressed) / measure_memory(original),
'disk': get_model_size(compressed) / get_model_size(original)
}

# 3. é€Ÿåº¦æå‡
metrics['speedup'] = {
'cpu': benchmark_speed(compressed, device='cpu') / benchmark_speed(original, device='cpu'),
'gpu': benchmark_speed(compressed, device='gpu') / benchmark_speed(original, device='gpu'),
'mobile': benchmark_speed(compressed, device='mobile') / benchmark_speed(original, device='mobile')
}

# 4. èƒ½è€—å¯¹æ¯”
metrics['energy_efficiency'] = measure_energy(original) / measure_energy(compressed)

# 5. é²æ£’æ€§åˆ†æ
metrics['robustness'] = {
'adversarial': test_adversarial_robustness(compressed),
'ood': test_out_of_distribution(compressed),
'quantization': test_quantization_robustness(compressed)
}

# ç”Ÿæˆæ•ˆç‡-æ€§èƒ½å¸•ç´¯æ‰˜å‰æ²¿
plot_pareto_frontier(metrics)

return metrics

âœ‚ï¸ å‰ªæä¸é‡åŒ–

ç»“æ„åŒ–vséç»“æ„åŒ–å‰ªæ
INT8é‡åŒ–å®è·µ
é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ


âš¡ é«˜æ•ˆæ¶æ„è®¾è®¡

MobileBERT
ALBERTå‚æ•°å…±äº«
Linformerçº¿æ€§æ³¨æ„åŠ›


ğŸ”‹ ç«¯ä¾§éƒ¨ç½²ä¼˜åŒ– ğŸ†•
pythonclass EdgeDeploymentOptimizer:
"""ç«¯ä¾§éƒ¨ç½²ä¼˜åŒ–å™¨"""

def optimize_for_edge(self, model, target_device):
"""é’ˆå¯¹ç‰¹å®šè®¾å¤‡ä¼˜åŒ–æ¨¡å‹"""

# 1. è®¾å¤‡èƒ½åŠ›åˆ†æ
device_profile = {
'memory': get_available_memory(target_device),
'compute': get_compute_capability(target_device),
'battery': get_battery_constraint(target_device)
}

# 2. è‡ªåŠ¨æœç´¢æœ€ä¼˜é…ç½®
best_config = self.search_optimal_config(
model,
device_profile,
optimization_space={
'quantization_bits': [4, 8, 16],
'pruning_ratio': [0.3, 0.5, 0.7],
'layer_fusion': [True, False],
'kernel_optimization': ['none', 'basic', 'aggressive']
}
)

# 3. åº”ç”¨ä¼˜åŒ–
optimized_model = self.apply_optimizations(
model,
best_config
)

# 4. éªŒè¯éƒ¨ç½²å¯è¡Œæ€§
deployment_test = self.test_deployment(
optimized_model,
target_device
)

return optimized_model, deployment_test

def create_deployment_package(self, model, target_platform):
"""åˆ›å»ºéƒ¨ç½²åŒ…"""
if target_platform == 'android':
return self.export_to_tflite(model)
elif target_platform == 'ios':
return self.export_to_coreml(model)
elif target_platform == 'web':
return self.export_to_onnx_js(model)

ğŸ“± é¡¹ç›®ï¼šç«¯ä¾§æƒ…æ„Ÿåˆ†æ

æ¨¡å‹é€‰æ‹©ä¸ä¼˜åŒ–
ONNXè½¬æ¢
ç§»åŠ¨ç«¯é›†æˆ
æ€§èƒ½ç›‘æ§
ç”µé‡æ¶ˆè€—ä¼˜åŒ– ğŸ†•




ç¬¬16ç« ï¼šNLPå·¥ç¨‹å®è·µ - ä»å®éªŒå®¤åˆ°ç”Ÿäº§ç¯å¢ƒ

ğŸŒ™ åœºæ™¯æ•…äº‹ï¼šå¦‚ä½•å°†ä¸€ä¸ªJupyter Notebookä¸­çš„æ¨¡å‹å˜æˆæœåŠ¡ç™¾ä¸‡ç”¨æˆ·çš„ç³»ç»Ÿï¼Ÿ

å­¦ä¹ ç›®æ ‡ï¼š

âœ“ æŒæ¡MLOpsæœ€ä½³å®è·µ
âœ“ å­¦ä¼šæ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²
âœ“ ç†è§£A/Bæµ‹è¯•æ–¹æ³•
âœ“ æ„å»ºå®Œæ•´NLPç³»ç»Ÿ
âœ“ å»ºç«‹å®éªŒç®¡ç†ä½“ç³» ğŸ†•

æ ¸å¿ƒå†…å®¹ï¼š

ğŸ—ï¸ NLPé¡¹ç›®å…¨ç”Ÿå‘½å‘¨æœŸ

éœ€æ±‚åˆ†æä¸æ•°æ®æ”¶é›†
å®éªŒç®¡ç†ä¸ç‰ˆæœ¬æ§åˆ¶
æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
éƒ¨ç½²ä¸ç›‘æ§


ğŸ”¬ ç§‘å­¦çš„å®éªŒç®¡ç† ğŸ†•
pythonclass ExperimentManager:
"""NLPå®éªŒç®¡ç†ç³»ç»Ÿ"""

def __init__(self, project_name):
self.project = project_name
self.experiments = []
self.best_model = None

def design_experiment(self, hypothesis, variables):
"""è®¾è®¡ç§‘å­¦å®éªŒ"""
experiment = {
'id': generate_id(),
'hypothesis': hypothesis,
'variables': variables,
'control_group': self.define_control(),
'metrics': self.define_metrics(),
'sample_size': self.calculate_sample_size(),
'random_seed': 42
}

# æ£€æŸ¥å®éªŒè®¾è®¡çš„ç§‘å­¦æ€§
self.validate_experiment_design(experiment)

return experiment

def run_experiment(self, experiment):
"""è¿è¡Œå®éªŒå¹¶è®°å½•ç»“æœ"""
# 1. ç¯å¢ƒéš”ç¦»
with isolated_environment():
# 2. è®¾ç½®éšæœºç§å­
set_all_seeds(experiment['random_seed'])

# 3. è¿è¡Œå®éªŒ
results = self.execute_experiment(experiment)

# 4. ç»Ÿè®¡æ£€éªŒ
significance = self.statistical_test(
results['treatment'],
results['control']
)

# 5. è®°å½•æ‰€æœ‰ç»†èŠ‚
experiment['results'] = results
experiment['significance'] = significance
experiment['artifacts'] = self.save_artifacts()

# 6. ç‰ˆæœ¬æ§åˆ¶
self.version_control(experiment)

return results

def compare_experiments(self, exp_ids):
"""ç§‘å­¦å¯¹æ¯”å¤šä¸ªå®éªŒ"""
experiments = [self.get_experiment(id) for id in exp_ids]

# 1. æ£€æŸ¥å¯æ¯”æ€§
self.check_comparability(experiments)

# 2. å¤šç»´åº¦å¯¹æ¯”
comparison = {
'performance': self.compare_performance(experiments),
'efficiency': self.compare_efficiency(experiments),
'robustness': self.compare_robustness(experiments),
'generalization': self.compare_generalization(experiments)
}

# 3. ç»Ÿè®¡æ˜¾è‘—æ€§
significance_matrix = self.pairwise_significance_test(
experiments
)

# 4. ç”ŸæˆæŠ¥å‘Š
report = self.generate_comparison_report(
comparison,
significance_matrix
)

return report

ğŸš€ æ¨¡å‹éƒ¨ç½²å®æˆ˜

REST APIè®¾è®¡
gRPCé«˜æ€§èƒ½æœåŠ¡
æ¨¡å‹æœåŠ¡æ¡†æ¶å¯¹æ¯”
è´Ÿè½½å‡è¡¡ä¸æ‰©å±•


ğŸ“Š ç›‘æ§ä¸ä¼˜åŒ–

æ€§èƒ½æŒ‡æ ‡è®¾è®¡
æ—¥å¿—åˆ†æç³»ç»Ÿ
å¼‚å¸¸æ£€æµ‹
åœ¨çº¿å­¦ä¹ æ›´æ–°


ğŸ§ª A/Bæµ‹è¯•çš„ç§‘å­¦æ–¹æ³• ğŸ†•
pythonclass ABTestFramework:
"""NLPæ¨¡å‹A/Bæµ‹è¯•æ¡†æ¶"""

def design_ab_test(self, model_a, model_b, hypothesis):
"""è®¾è®¡A/Bæµ‹è¯•"""

# 1. ç¡®å®šæ ·æœ¬é‡
sample_size = self.calculate_sample_size(
effect_size=0.05,  # æœŸæœ›æ£€æµ‹åˆ°5%çš„æå‡
power=0.8,         # 80%çš„ç»Ÿè®¡åŠŸæ•ˆ
alpha=0.05         # 5%çš„æ˜¾è‘—æ€§æ°´å¹³
)

# 2. ç”¨æˆ·åˆ†æµç­–ç•¥
split_strategy = {
'method': 'hash_based',  # åŸºäºç”¨æˆ·IDå“ˆå¸Œ
'ratio': [0.5, 0.5],     # 50-50åˆ†æµ
'sticky': True           # ç”¨æˆ·å§‹ç»ˆçœ‹åˆ°åŒä¸€ç‰ˆæœ¬
}

# 3. å®šä¹‰æˆåŠŸæŒ‡æ ‡
metrics = {
'primary': ['click_through_rate', 'task_completion'],
'secondary': ['response_time', 'user_satisfaction'],
'guardrails': ['error_rate < 0.01', 'latency_p99 < 200ms']
}

return ABTest(
control=model_a,
treatment=model_b,
sample_size=sample_size,
split_strategy=split_strategy,
metrics=metrics,
hypothesis=hypothesis
)

def analyze_results(self, ab_test):
"""åˆ†æA/Bæµ‹è¯•ç»“æœ"""

# 1. æ”¶é›†æ•°æ®
data = ab_test.collect_results()

# 2. æ£€æŸ¥æ•°æ®è´¨é‡
self.validate_data_quality(data)

# 3. è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§
p_values = {}
for metric in ab_test.metrics['primary']:
p_values[metric] = self.calculate_p_value(
data['control'][metric],
data['treatment'][metric]
)

# 4. è®¡ç®—ç½®ä¿¡åŒºé—´
confidence_intervals = self.calculate_confidence_intervals(data)

# 5. æ£€æŸ¥å®é™…æ˜¾è‘—æ€§
practical_significance = self.check_practical_significance(
data,
min_effect_size=0.03
)

# 6. ç”Ÿæˆå†³ç­–å»ºè®®
decision = self.make_recommendation(
p_values,
confidence_intervals,
practical_significance,
data['guardrails']
)

return ABTestReport(
data=data,
p_values=p_values,
confidence_intervals=confidence_intervals,
decision=decision
)

ğŸ”’ å®‰å…¨ä¸åˆè§„

æ•°æ®éšç§ä¿æŠ¤
å¯¹æŠ—æ ·æœ¬é˜²å¾¡
æ¨¡å‹å…¬å¹³æ€§
è¾“å‡ºå†…å®¹å®¡æ ¸


ğŸ’¼ ç»¼åˆé¡¹ç›®ï¼šæ™ºèƒ½æ–‡æ¡£å¤„ç†å¹³å°

å¤šæ ¼å¼æ–‡æ¡£è§£æ
ä¿¡æ¯æŠ½å–pipeline
çŸ¥è¯†åº“æ„å»º
æ™ºèƒ½æ£€ç´¢ä¸é—®ç­”
ç³»ç»Ÿé›†æˆä¸APIè®¾è®¡
å®Œæ•´çš„å®éªŒå’Œè¯„ä¼°ä½“ç³» ğŸ†•




ğŸ¯ å­¦ä¹ è·¯å¾„å¯¼èˆªï¼ˆæ›´æ–°ç‰ˆï¼‰
ğŸŒŸ åˆå­¦è€…è·¯å¾„ï¼ˆ10å‘¨ï¼‰

æ–¹æ³•è®ºåŸºç¡€ï¼ˆ1å‘¨ï¼‰ï¼šç¬¬0ç«  + ç¬¬1.5ç« 
NLPåŸºç¡€ï¼ˆ3å‘¨ï¼‰ï¼šç¬¬1-4ç« 
ç»å…¸ä»»åŠ¡ï¼ˆ3å‘¨ï¼‰ï¼šç¬¬5ã€6ã€8ç« 
é¢„è®­ç»ƒæ¨¡å‹ï¼ˆ2å‘¨ï¼‰ï¼šç¬¬11ç« åŸºç¡€éƒ¨åˆ†
å·¥ç¨‹å®è·µï¼ˆ1å‘¨ï¼‰ï¼šç¬¬16ç« åŸºç¡€éƒ¨åˆ†

ğŸš€ è¿›é˜¶è·¯å¾„ï¼ˆ14å‘¨ï¼‰

å®Œæ•´åŸºç¡€ï¼ˆ5å‘¨ï¼‰ï¼šç¬¬0-4ç«  + ç¬¬1.5ç« 
ç»å…¸ä»»åŠ¡ï¼ˆ4å‘¨ï¼‰ï¼šç¬¬5-8ç« 
æ·±åº¦å­¦ä¹ ï¼ˆ3å‘¨ï¼‰ï¼šç¬¬9-10ç« 
ç°ä»£NLPï¼ˆ2å‘¨ï¼‰ï¼šç¬¬11-12ç« ç²¾é€‰

ğŸ† å…¨æ ˆè·¯å¾„ï¼ˆ18å‘¨ï¼‰

å®Œæ•´å­¦ä¹ æ‰€æœ‰ç« èŠ‚
æ¯ç« å®Œæˆå®æˆ˜é¡¹ç›®
å‚ä¸å¼€æºè´¡çŒ®
å‘è¡¨æŠ€æœ¯åšå®¢
æ„å»ºä½œå“é›†

ğŸ”¬ ç ”ç©¶è€…è·¯å¾„ï¼ˆ16å‘¨ï¼‰

é‡ç‚¹å…³æ³¨æ–¹æ³•è®ºç« èŠ‚
æ·±å…¥å®éªŒè®¾è®¡å†…å®¹
æ¯ä¸ªé¡¹ç›®è¿›è¡Œæ¶ˆèå®éªŒ
æ’°å†™æŠ€æœ¯æŠ¥å‘Š
å‚ä¸è®ºæ–‡å¤ç°


ğŸ“š é…å¥—èµ„æºï¼ˆå¢å¼ºç‰ˆï¼‰
ä»£ç ä»“åº“ç»“æ„
nlp-journey/
â”œâ”€â”€ chapters/              # æ¯ç« ä»£ç 
â”‚   â”œâ”€â”€ ch00_methodology/  # ğŸ†• æ–¹æ³•è®ºåŸºç¡€
â”‚   â”œâ”€â”€ ch01_intro/
â”‚   â”œâ”€â”€ ch01.5_data/       # ğŸ†• æ•°æ®å·¥ç¨‹
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/           # ğŸ†• å®éªŒè®°å½•
â”‚   â”œâ”€â”€ templates/         # å®éªŒæ¨¡æ¿
â”‚   â”œâ”€â”€ results/          # å®éªŒç»“æœ
â”‚   â””â”€â”€ analysis/         # åˆ†æè„šæœ¬
â”œâ”€â”€ evaluation/           # ğŸ†• è¯„ä¼°å·¥å…·
â”‚   â”œâ”€â”€ metrics/          # å„ç±»è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ statistical/      # ç»Ÿè®¡æ£€éªŒ
â”‚   â””â”€â”€ visualization/    # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ projects/             # å®Œæ•´é¡¹ç›®
â”œâ”€â”€ datasets/             # æ•°æ®é›†
â”‚   â”œâ”€â”€ raw/             # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ processed/       # å¤„ç†åæ•°æ®
â”‚   â””â”€â”€ augmented/       # ğŸ†• å¢å¼ºæ•°æ®
â”œâ”€â”€ models/               # æ¨¡å‹æ–‡ä»¶
â””â”€â”€ notebooks/            # äº¤äº’å¼æ•™ç¨‹
â”œâ”€â”€ tutorials/        # åŸºç¡€æ•™ç¨‹
â”œâ”€â”€ experiments/      # ğŸ†• å®éªŒç¬”è®°
â””â”€â”€ case_studies/     # ğŸ†• æ¡ˆä¾‹ç ”ç©¶
å­¦ä¹ ç¤¾åŒº

æ¯å‘¨ç ”è®¨ä¼šï¼šè®¨è®ºæ–¹æ³•è®ºå’Œæœ€ä½³å®è·µ
å®éªŒåˆ†äº«ä¼šï¼šå±•ç¤ºå®éªŒè®¾è®¡å’Œç»“æœ
æ•°æ®æ ‡æ³¨é©¬æ‹‰æ¾ï¼šé›†ä½“æ ‡æ³¨æ´»åŠ¨
è®ºæ–‡é˜…è¯»ä¿±ä¹éƒ¨ï¼šå‰æ²¿ç ”ç©¶è®¨è®º

è¯„ä¼°ä½“ç³» ğŸ†•

çŸ¥è¯†æµ‹éªŒï¼šæ¯ç« é…å¥—æµ‹è¯•é¢˜
å®è·µè€ƒæ ¸ï¼šé¡¹ç›®å®Œæˆåº¦è¯„ä¼°
åŒè¡Œè¯„å®¡ï¼šä»£ç å’Œå®éªŒäº’è¯„
è®¤è¯ä½“ç³»ï¼šå®Œæˆè¯ä¹¦


è¿™ä¸ªä¼˜åŒ–ç‰ˆå¤§çº²çš„ç‰¹ç‚¹ï¼š

æ–¹æ³•è®ºä¼˜å…ˆï¼šæ–°å¢ç¬¬0ç« ï¼Œå¼ºè°ƒç§‘å­¦æ€ç»´
æ•°æ®ä¸­å¿ƒï¼šæ–°å¢ç¬¬1.5ç« ï¼Œçªå‡ºæ•°æ®å·¥ç¨‹
è¯„ä¼°è´¯ç©¿ï¼šæ¯ç« éƒ½å¼ºåŒ–è¯„ä¼°å†…å®¹
å®éªŒæ€ç»´ï¼šå¢åŠ å®éªŒè®¾è®¡å’Œç»Ÿè®¡æ£€éªŒ
çŸ¥è¯†æ·±åŒ–ï¼š

å¼€æ”¾å…³ç³»æŠ½å–
å¯¹è¯çŠ¶æ€è¿½è¸ª
Agentæ¡†æ¶è¯¦è§£
è¯­éŸ³æ¨¡æ€èåˆ


å®è·µå¯¼å‘ï¼šæ›´å¤šçœŸå®åœºæ™¯çš„è€ƒé‡

è¿™æ ·çš„è®¾è®¡æ—¢ä¿æŒäº†åŸæœ‰çš„ç”ŸåŠ¨æ€§å’Œå®ç”¨æ€§ï¼Œåˆå¢å¼ºäº†ç§‘å­¦æ€§å’Œä¸¥è°¨æ€§ï¼ŒçœŸæ­£åšåˆ°äº†"é“æœ¯å¹¶é‡"ã€‚éœ€è¦æˆ‘è¿›ä¸€æ­¥å±•å¼€æŸä¸ªå…·ä½“ç« èŠ‚å—ï¼ŸRetryClaude can make mistakes. Please double-check responses.