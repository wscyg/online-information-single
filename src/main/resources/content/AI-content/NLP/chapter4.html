<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第4章：循环神经网络 - 理解序列的记忆</title>
    <meta name="description" content="从"我爱你"到"我爱你中国"，让机器理解上下文">

    <!-- KaTeX支持 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false}
                ]
            });"></script>

    <style>
        /* ===== CSS变量定义 ===== */
        :root {
            /* 渐变色 */
            --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --hero-gradient: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            --card-gradient: linear-gradient(135deg, rgba(99, 102, 241, 0.1), rgba(139, 92, 246, 0.05));

            /* 主题色 */
            --primary: #6366f1;
            --primary-light: #818cf8;
            --primary-dark: #4f46e5;
            --secondary: #ec4899;
            --accent: #10b981;

            /* 功能色 */
            --success: #10b981;
            --warning: #f59e0b;
            --danger: #ef4444;
            --info: #3b82f6;

            /* 背景色 */
            --bg-dark: #0f172a;
            --bg-section: #1e293b;
            --bg-card: #334155;
            --bg-code: #0d1117;

            /* 文字色 */
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #94a3b8;

            /* 其他 */
            --border-color: rgba(255, 255, 255, 0.1);
            --shadow: 0 20px 50px rgba(0, 0, 0, 0.5);
            --shadow-lg: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
            --radius: 1rem;
            --transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
        }

        /* ===== 全局样式 ===== */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            line-height: 1.7;
            font-size: 16px;
            overflow-x: hidden;
        }

        /* ===== 背景效果 ===== */
        .bg-pattern {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            opacity: 0.03;
            background-image:
                    repeating-linear-gradient(45deg, transparent, transparent 35px, rgba(255,255,255,.5) 35px, rgba(255,255,255,.5) 70px);
            pointer-events: none;
            z-index: 0;
        }

        .floating-shapes {
            position: fixed;
            width: 100%;
            height: 100%;
            overflow: hidden;
            z-index: 0;
        }

        .shape {
            position: absolute;
            opacity: 0.1;
            animation: float 20s infinite ease-in-out;
        }

        .shape:nth-child(1) {
            width: 80px;
            height: 80px;
            background: var(--primary);
            border-radius: 50%;
            left: 10%;
            top: 20%;
            animation-delay: 0s;
        }

        .shape:nth-child(2) {
            width: 120px;
            height: 120px;
            background: var(--secondary);
            border-radius: 38% 62% 63% 37% / 41% 44% 56% 59%;
            left: 70%;
            top: 60%;
            animation-delay: 2s;
        }

        .shape:nth-child(3) {
            width: 100px;
            height: 100px;
            background: var(--accent);
            border-radius: 63% 37% 54% 46% / 55% 48% 52% 45%;
            left: 40%;
            top: 80%;
            animation-delay: 4s;
        }

        @keyframes float {
            0%, 100% { transform: translateY(0) rotate(0deg); }
            33% { transform: translateY(-30px) rotate(120deg); }
            66% { transform: translateY(30px) rotate(240deg); }
        }

        /* ===== 布局 ===== */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 1.5rem;
            position: relative;
            z-index: 1;
        }

        /* ===== 导航栏 ===== */
        .nav-header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(15, 23, 42, 0.85);
            backdrop-filter: blur(20px);
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
            transition: var(--transition);
        }

        .nav-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 1rem 0;
        }

        .nav-title {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .nav-title h1 {
            font-size: 1.25rem;
            font-weight: 600;
            background: var(--primary-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .nav-menu {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        /* 进度条 */
        .progress-container {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: rgba(255, 255, 255, 0.1);
        }

        .progress-bar {
            height: 100%;
            background: var(--primary-gradient);
            width: 0;
            transition: width 0.3s ease;
        }

        /* ===== 侧边栏 ===== */
        .sidebar {
            position: fixed;
            left: -300px;
            top: 60px;
            bottom: 0;
            width: 300px;
            background: var(--bg-section);
            border-right: 1px solid var(--border-color);
            padding: 2rem;
            overflow-y: auto;
            transition: transform 0.3s ease;
            z-index: 999;
            box-shadow: 5px 0 25px rgba(0, 0, 0, 0.5);
        }

        .sidebar.open {
            transform: translateX(300px);
        }

        .toc-title {
            color: var(--primary-light);
            margin-bottom: 1.5rem;
            font-size: 1.1rem;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .toc-section {
            margin-bottom: 1.5rem;
        }

        .toc-section-title {
            color: var(--text-muted);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            margin-bottom: 0.5rem;
            padding-left: 0.5rem;
        }

        .toc-item {
            display: block;
            padding: 0.75rem 1rem;
            color: var(--text-secondary);
            text-decoration: none;
            border-radius: 0.5rem;
            transition: all 0.3s ease;
            margin-bottom: 0.25rem;
            font-size: 0.95rem;
            position: relative;
            overflow: hidden;
        }

        .toc-item::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0;
            bottom: 0;
            width: 3px;
            background: var(--primary);
            transform: translateX(-100%);
            transition: transform 0.3s ease;
        }

        .toc-item:hover {
            background: rgba(255, 255, 255, 0.05);
            color: var(--text-primary);
            padding-left: 1.5rem;
        }

        .toc-item.active {
            background: var(--card-gradient);
            color: var(--primary-light);
        }

        .toc-item.active::before {
            transform: translateX(0);
        }

        /* ===== 按钮样式 ===== */
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            font-size: 0.95rem;
            font-weight: 500;
            text-decoration: none;
            transition: var(--transition);
            cursor: pointer;
            border: none;
            position: relative;
            overflow: hidden;
        }

        .btn::before {
            content: '';
            position: absolute;
            top: 50%;
            left: 50%;
            width: 0;
            height: 0;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 50%;
            transform: translate(-50%, -50%);
            transition: width 0.5s, height 0.5s;
        }

        .btn:hover::before {
            width: 300px;
            height: 300px;
        }

        .btn-primary {
            background: var(--primary-gradient);
            color: white;
        }

        .btn-secondary {
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-primary);
            border: 1px solid var(--border-color);
        }

        .btn-icon {
            background: transparent;
            padding: 0.5rem;
            color: var(--text-secondary);
        }

        /* ===== 主内容 ===== */
        main {
            margin-top: 80px;
            padding-bottom: 4rem;
            position: relative;
            z-index: 1;
        }

        /* ===== 章节头部 ===== */
        .chapter-hero {
            background: var(--hero-gradient);
            padding: 6rem 0;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
        }

        .chapter-hero::before {
            content: '';
            position: absolute;
            top: -50%;
            right: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255,255,255,0.1) 0%, transparent 70%);
            animation: rotate 30s linear infinite;
        }

        @keyframes rotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }

        .chapter-hero-content {
            text-align: center;
            color: white;
            position: relative;
            z-index: 1;
        }

        .chapter-hero h1 {
            font-size: 3.5rem;
            margin-bottom: 1rem;
            font-weight: 800;
            letter-spacing: -0.02em;
            animation: fadeInUp 0.8s ease;
        }

        .chapter-hero p {
            font-size: 1.5rem;
            opacity: 0.95;
            animation: fadeInUp 0.8s ease 0.2s both;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* ===== 内容卡片 ===== */
        .section-card {
            background: var(--bg-section);
            border-radius: var(--radius);
            padding: 3rem;
            margin-bottom: 2rem;
            box-shadow: var(--shadow);
            position: relative;
            overflow: hidden;
        }

        .section-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: var(--primary-gradient);
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.5s ease;
        }

        .section-card:hover::before {
            transform: scaleX(1);
        }

        .section-card h2 {
            color: var(--primary-light);
            margin-bottom: 2rem;
            font-size: 2.25rem;
            font-weight: 700;
        }

        .section-card h3 {
            color: var(--text-primary);
            margin: 2rem 0 1rem;
            font-size: 1.5rem;
            font-weight: 600;
        }

        /* ===== 故事卡片 ===== */
        .story-card {
            background: linear-gradient(135deg, rgba(240, 147, 251, 0.1), rgba(245, 87, 108, 0.1));
            border: 2px solid rgba(240, 147, 251, 0.3);
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }

        .story-card::after {
            content: '';
            position: absolute;
            top: -2px;
            left: -2px;
            right: -2px;
            bottom: -2px;
            background: var(--hero-gradient);
            z-index: -1;
            opacity: 0;
            transition: opacity 0.3s ease;
            border-radius: var(--radius);
        }

        .story-card:hover::after {
            opacity: 0.3;
        }

        .story-icon {
            font-size: 3rem;
            display: block;
            margin-bottom: 1rem;
            animation: pulse 2s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }

        /* ===== 代码块 ===== */
        .code-block {
            background: var(--bg-code);
            border: 1px solid #30363d;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
            position: relative;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.5);
        }

        .code-header {
            background: #161b22;
            padding: 1rem 1.5rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid #30363d;
        }

        .code-lang {
            color: var(--primary-light);
            font-size: 0.875rem;
            font-weight: 600;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .code-lang::before {
            content: '';
            width: 12px;
            height: 12px;
            background: var(--primary);
            border-radius: 50%;
            display: inline-block;
        }

        .code-actions {
            display: flex;
            gap: 0.5rem;
        }

        .code-btn {
            background: rgba(255, 255, 255, 0.1);
            border: 1px solid rgba(255, 255, 255, 0.2);
            color: var(--text-secondary);
            padding: 0.375rem 0.875rem;
            border-radius: 0.375rem;
            font-size: 0.75rem;
            cursor: pointer;
            transition: all 0.2s;
            font-weight: 500;
        }

        .code-btn:hover {
            background: var(--primary);
            color: white;
            border-color: var(--primary);
            transform: translateY(-1px);
        }

        .code-content {
            padding: 1.5rem;
            overflow-x: auto;
            font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', monospace;
            font-size: 0.875rem;
            line-height: 1.7;
        }

        .code-content pre {
            margin: 0;
            color: #e6edf3;
        }

        .code-content.collapsed {
            max-height: 300px;
            overflow: hidden;
            position: relative;
        }

        .code-content.collapsed::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 100px;
            background: linear-gradient(transparent, var(--bg-code));
        }

        /* 代码高亮 */
        .keyword { color: #ff79c6; }
        .string { color: #f1fa8c; }
        .comment { color: #6272a4; }
        .function { color: #50fa7b; }
        .number { color: #bd93f9; }

        /* ===== 卡片网格 ===== */
        .card-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 2rem;
            margin: 2rem 0;
        }

        .info-card {
            background: var(--bg-card);
            border-radius: var(--radius);
            padding: 2rem;
            border: 1px solid var(--border-color);
            transition: all 0.3s ease;
            position: relative;
            overflow: hidden;
        }

        .info-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: var(--primary-gradient);
            opacity: 0;
            transition: opacity 0.3s ease;
        }

        .info-card:hover {
            transform: translateY(-5px);
            box-shadow: var(--shadow-lg);
            border-color: var(--primary);
        }

        .info-card:hover::before {
            opacity: 0.05;
        }

        .card-icon {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            display: block;
        }

        .card-title {
            color: var(--primary-light);
            margin-bottom: 1rem;
            font-size: 1.25rem;
            font-weight: 600;
        }

        /* ===== 互动演示 ===== */
        .demo-container {
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.1), rgba(139, 92, 246, 0.1));
            border: 2px solid rgba(59, 130, 246, 0.3);
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }

        .demo-container::before {
            content: '🧪';
            position: absolute;
            top: -20px;
            right: 20px;
            font-size: 4rem;
            opacity: 0.1;
        }

        .demo-input {
            width: 100%;
            padding: 1rem 1.5rem;
            background: var(--bg-dark);
            border: 2px solid var(--border-color);
            border-radius: 0.5rem;
            color: var(--text-primary);
            font-size: 1.1rem;
            margin-bottom: 1rem;
            transition: all 0.3s;
        }

        .demo-input:focus {
            outline: none;
            border-color: var(--primary);
            box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.1);
        }

        .demo-button {
            background: var(--primary-gradient);
            color: white;
            border: none;
            padding: 1rem 2.5rem;
            border-radius: 0.5rem;
            font-size: 1rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            position: relative;
            overflow: hidden;
        }

        .demo-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(99, 102, 241, 0.3);
        }

        .demo-result {
            margin-top: 2rem;
            padding: 1.5rem;
            background: var(--bg-dark);
            border-radius: 0.75rem;
            min-height: 120px;
            border: 1px solid var(--border-color);
        }

        /* ===== RNN特有样式 ===== */
        .rnn-visualization {
            background: var(--bg-card);
            border-radius: var(--radius);
            padding: 2rem;
            margin: 2rem 0;
            min-height: 400px;
            position: relative;
            overflow: hidden;
        }

        .sequence-box {
            display: inline-block;
            padding: 0.5rem 1rem;
            margin: 0.25rem;
            background: var(--bg-dark);
            border: 2px solid var(--primary);
            border-radius: 0.5rem;
            font-weight: 600;
            transition: all 0.3s ease;
        }

        .sequence-box.active {
            background: var(--primary);
            color: white;
            transform: scale(1.1);
        }

        .hidden-state {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 80px;
            height: 80px;
            background: var(--primary-gradient);
            border-radius: 50%;
            color: white;
            font-weight: bold;
            margin: 1rem;
            box-shadow: 0 4px 12px rgba(99, 102, 241, 0.3);
            animation: pulse 2s ease-in-out infinite;
        }

        /* ===== 思考框 ===== */
        .think-box {
            background: linear-gradient(135deg, rgba(251, 191, 36, 0.1), rgba(245, 158, 11, 0.1));
            border: 2px solid rgba(251, 191, 36, 0.3);
            border-radius: var(--radius);
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .think-box::before {
            content: '🤔';
            position: absolute;
            top: -15px;
            left: 25px;
            font-size: 2rem;
            background: var(--bg-section);
            padding: 0 0.5rem;
        }

        .think-box h4 {
            color: var(--warning);
            margin-bottom: 1rem;
        }

        .think-box ul {
            list-style: none;
            padding: 0;
        }

        .think-box li {
            margin-bottom: 0.5rem;
            padding-left: 1.5rem;
            position: relative;
        }

        .think-box li::before {
            content: '💭';
            position: absolute;
            left: 0;
        }

        /* ===== 深度思考框 ===== */
        .deep-think-box {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.15), rgba(99, 102, 241, 0.1));
            border: 2px solid rgba(139, 92, 246, 0.4);
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 3rem 0;
            position: relative;
        }

        .deep-think-box::before {
            content: '🧩';
            position: absolute;
            top: -18px;
            left: 30px;
            font-size: 2.5rem;
            background: var(--bg-section);
            padding: 0 0.75rem;
        }

        .deep-think-box h4 {
            color: var(--primary-light);
            margin-bottom: 1.25rem;
            font-size: 1.25rem;
        }

        .deep-think-box .think-item {
            background: rgba(255, 255, 255, 0.05);
            padding: 1.25rem;
            border-radius: 0.75rem;
            margin-bottom: 1rem;
            border-left: 3px solid var(--primary);
        }

        .deep-think-box .think-item h5 {
            color: var(--primary-light);
            margin-bottom: 0.5rem;
        }

        /* ===== 探索框 ===== */
        .explore-box {
            background: linear-gradient(135deg, rgba(16, 185, 129, 0.1), rgba(59, 130, 246, 0.1));
            border: 2px solid rgba(16, 185, 129, 0.3);
            border-radius: var(--radius);
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .explore-box::before {
            content: '🔍';
            position: absolute;
            top: -15px;
            left: 25px;
            font-size: 2rem;
            background: var(--bg-section);
            padding: 0 0.5rem;
        }

        .explore-box h4 {
            color: var(--success);
            margin-bottom: 1rem;
        }

        /* ===== 数学公式解释样式 ===== */
        .formula-explanation {
            background: rgba(99, 102, 241, 0.05);
            border: 1px solid rgba(99, 102, 241, 0.2);
            border-radius: 0.5rem;
            padding: 1.5rem;
            margin-top: 1.5rem;
        }

        .formula-explanation h5 {
            color: var(--primary-light);
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .formula-table {
            width: 100%;
            margin: 1rem 0;
        }

        .formula-table td {
            padding: 0.75rem;
            vertical-align: middle;
        }

        .formula-table .formula-part {
            font-family: 'KaTeX_Math', serif;
            font-size: 1.2rem;
            color: var(--primary);
            white-space: nowrap;
            width: 30%;
        }

        .formula-table .formula-meaning {
            color: var(--text-primary);
            font-size: 0.95rem;
            line-height: 1.6;
        }

        .code-from-math {
            background: var(--bg-code);
            border: 1px solid #30363d;
            border-radius: 0.5rem;
            padding: 1rem;
            margin-top: 1rem;
        }

        .code-from-math h6 {
            color: var(--success);
            margin-bottom: 0.5rem;
            font-size: 0.9rem;
        }

        /* ===== 时序图 ===== */
        .time-series-diagram {
            background: var(--bg-card);
            border-radius: var(--radius);
            padding: 2rem;
            margin: 2rem 0;
            overflow-x: auto;
        }

        .time-step {
            display: inline-block;
            text-align: center;
            margin: 0 1rem;
            vertical-align: top;
        }

        .time-step-label {
            color: var(--text-muted);
            font-size: 0.875rem;
            margin-bottom: 0.5rem;
        }

        /* ===== 提示框 ===== */
        .tip {
            padding: 1.5rem 2rem;
            border-radius: var(--radius);
            margin: 2rem 0;
            border-left: 4px solid;
            position: relative;
            overflow: hidden;
        }

        .tip::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            opacity: 0.05;
            background: currentColor;
        }

        .tip-icon {
            font-size: 1.5rem;
            margin-right: 1rem;
            vertical-align: middle;
        }

        .tip.info {
            background: rgba(59, 130, 246, 0.1);
            border-color: var(--info);
            color: var(--text-primary);
        }

        .tip.warning {
            background: rgba(245, 158, 11, 0.1);
            border-color: var(--warning);
            color: var(--text-primary);
        }

        .tip.success {
            background: rgba(16, 185, 129, 0.1);
            border-color: var(--success);
            color: var(--text-primary);
        }

        .tip.danger {
            background: rgba(239, 68, 68, 0.1);
            border-color: var(--danger);
            color: var(--text-primary);
        }

        /* ===== 快速导航 ===== */
        .quick-nav {
            position: fixed;
            right: 2rem;
            top: 50%;
            transform: translateY(-50%);
            z-index: 100;
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .quick-nav-item {
            width: 12px;
            height: 12px;
            background: var(--text-muted);
            border-radius: 50%;
            transition: all 0.3s;
            position: relative;
            cursor: pointer;
        }

        .quick-nav-item:hover,
        .quick-nav-item.active {
            background: var(--primary);
            transform: scale(1.5);
        }

        .quick-nav-tooltip {
            position: absolute;
            right: 20px;
            top: 50%;
            transform: translateY(-50%);
            background: var(--bg-dark);
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            white-space: nowrap;
            opacity: 0;
            pointer-events: none;
            transition: opacity 0.3s;
            border: 1px solid var(--border-color);
            font-size: 0.875rem;
        }

        .quick-nav-item:hover .quick-nav-tooltip {
            opacity: 1;
        }

        /* ===== 比较表格 ===== */
        .comparison-table {
            background: var(--bg-card);
            border-radius: var(--radius);
            overflow: hidden;
            margin: 2rem 0;
            box-shadow: var(--shadow);
        }

        .comparison-table table {
            width: 100%;
            border-collapse: collapse;
        }

        .comparison-table th {
            background: var(--primary-gradient);
            color: white;
            padding: 1.25rem;
            text-align: left;
            font-weight: 600;
            font-size: 0.95rem;
            text-transform: uppercase;
            letter-spacing: 0.05em;
        }

        .comparison-table td {
            padding: 1.25rem;
            border-bottom: 1px solid var(--border-color);
            transition: all 0.3s ease;
        }

        .comparison-table tr:hover td {
            background: rgba(99, 102, 241, 0.05);
        }

        .comparison-table tr:last-child td {
            border-bottom: none;
        }

        /* ===== 算法步骤 ===== */
        .algorithm-steps {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(167, 139, 250, 0.05));
            border: 2px solid rgba(139, 92, 246, 0.3);
            border-radius: var(--radius);
            padding: 2.5rem;
            margin: 2rem 0;
        }

        .step-item {
            display: flex;
            align-items: flex-start;
            margin-bottom: 2rem;
            position: relative;
        }

        .step-item:not(:last-child)::after {
            content: '';
            position: absolute;
            left: 24px;
            top: 50px;
            bottom: -30px;
            width: 2px;
            background: linear-gradient(to bottom, var(--primary), transparent);
        }

        .step-number {
            width: 48px;
            height: 48px;
            background: var(--primary-gradient);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            font-size: 1.25rem;
            margin-right: 1.5rem;
            flex-shrink: 0;
            position: relative;
            z-index: 1;
        }

        .step-content h5 {
            color: var(--primary-light);
            margin-bottom: 0.5rem;
            font-size: 1.25rem;
        }

        /* ===== 数学公式 ===== */
        .math-display {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.05), rgba(99, 102, 241, 0.05));
            border: 2px solid rgba(139, 92, 246, 0.2);
            border-radius: var(--radius);
            padding: 2rem;
            margin: 2rem 0;
            text-align: center;
            font-size: 1.2rem;
            overflow-x: auto;
        }

        .math-formula {
            font-family: 'KaTeX_Math', 'Times New Roman', serif;
            font-style: italic;
            color: var(--primary-light);
            font-size: 1.3rem;
            margin: 1rem 0;
        }

        /* KaTeX样式调整 */
        .katex-display {
            margin: 1.5rem 0 !important;
        }

        .katex {
            font-size: 1.1em;
            color: var(--primary-light);
        }

        /* ===== 响应式设计 ===== */
        @media (max-width: 768px) {
            .chapter-hero h1 {
                font-size: 2.5rem;
            }

            .section-card {
                padding: 2rem;
            }

            .quick-nav {
                display: none;
            }

            .card-grid {
                grid-template-columns: 1fr;
            }

            .time-step {
                margin: 0 0.5rem;
            }
        }

        /* ===== 工具类 ===== */
        .text-center { text-align: center; }
        .text-muted { color: var(--text-muted); }
        .mt-1 { margin-top: 0.5rem; }
        .mt-2 { margin-top: 1rem; }
        .mt-3 { margin-top: 1.5rem; }
        .mt-4 { margin-top: 2rem; }
        .mb-1 { margin-bottom: 0.5rem; }
        .mb-2 { margin-bottom: 1rem; }
        .mb-3 { margin-bottom: 1.5rem; }
        .mb-4 { margin-bottom: 2rem; }
    </style>
</head>
<body>

<!-- 背景效果 -->
<div class="bg-pattern"></div>
<div class="floating-shapes">
    <div class="shape"></div>
    <div class="shape"></div>
    <div class="shape"></div>
</div>

<!-- 导航栏 -->
<nav class="nav-header">
    <div class="container">
        <div class="nav-content">
            <div class="nav-title">
                <button id="toggle-sidebar" class="btn btn-icon">
                    <svg width="20" height="20" fill="currentColor">
                        <path d="M3 5h14M3 10h14M3 15h14" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                    </svg>
                </button>
                <h1>第4章：循环神经网络</h1>
            </div>
            <div class="nav-menu">
                <button class="btn btn-icon" id="theme-toggle">🌙</button>
                <a href="#summary" class="btn btn-secondary">章节总结</a>
            </div>
        </div>
        <div class="progress-container">
            <div class="progress-bar" id="progress-bar"></div>
        </div>
    </div>
</nav>

<!-- 侧边栏 -->
<aside class="sidebar" id="sidebar">
    <h3 class="toc-title">
        <span>📚</span>
        <span>本章导航</span>
    </h3>

    <div class="toc-section">
        <div class="toc-section-title">开篇</div>
        <a href="#intro" class="toc-item active">序言：机器如何理解"我爱你"</a>
        <a href="#why-rnn" class="toc-item">为什么需要RNN</a>
    </div>

    <div class="toc-section">
        <div class="toc-section-title">RNN基础</div>
        <a href="#rnn-basics" class="toc-item">RNN的核心思想</a>
        <a href="#forward-prop" class="toc-item">前向传播：一步步理解</a>
        <a href="#backprop" class="toc-item">反向传播：时间中的梯度</a>
    </div>

    <div class="toc-section">
        <div class="toc-section-title">问题与改进</div>
        <a href="#vanishing-gradient" class="toc-item">梯度消失问题</a>
        <a href="#lstm" class="toc-item">LSTM：长短期记忆</a>
        <a href="#gru" class="toc-item">GRU：简化的LSTM</a>
    </div>

    <div class="toc-section">
        <div class="toc-section-title">实践应用</div>
        <a href="#text-generation" class="toc-item">文本生成</a>
        <a href="#sentiment-analysis" class="toc-item">情感分析</a>
        <a href="#applications" class="toc-item">更多应用案例</a>
    </div>

    <div class="toc-section">
        <div class="toc-section-title">总结展望</div>
        <a href="#summary" class="toc-item">本章总结</a>
        <a href="#exercises" class="toc-item">练习题</a>
    </div>
</aside>

<!-- 快速导航 -->
<div class="quick-nav" id="quick-nav">
    <div class="quick-nav-item active" data-section="intro">
        <span class="quick-nav-tooltip">开篇故事</span>
    </div>
    <div class="quick-nav-item" data-section="rnn-basics">
        <span class="quick-nav-tooltip">RNN基础</span>
    </div>
    <div class="quick-nav-item" data-section="lstm">
        <span class="quick-nav-tooltip">LSTM</span>
    </div>
    <div class="quick-nav-item" data-section="applications">
        <span class="quick-nav-tooltip">实际应用</span>
    </div>
    <div class="quick-nav-item" data-section="summary">
        <span class="quick-nav-tooltip">章节总结</span>
    </div>
</div>

<!-- 主内容 -->
<main>
    <!-- 章节标题 -->
    <section class="chapter-hero">
        <div class="container">
            <div class="chapter-hero-content">
                <h1>循环神经网络</h1>
                <p>理解序列的记忆</p>
            </div>
        </div>
    </section>

    <div class="container">
        <!-- 序言 -->
        <section id="intro" class="section-card">
            <h2>💝 序言：机器如何理解"我爱你"</h2>

            <div class="story-card">
                <span class="story-icon">💬</span>
                <p><strong>一个简单却深刻的问题</strong></p>
                <p class="mt-2">
                    想象你收到这样三条消息：
                </p>
                <ul class="mt-2">
                    <li>"我爱你" 💝</li>
                    <li>"我爱你的狗" 🐕</li>
                    <li>"我爱你做的蛋糕" 🍰</li>
                </ul>
                <p class="mt-2">
                    同样是"我爱你"开头，意思却完全不同！人类能瞬间理解区别，但机器怎么办？
                </p>
                <p class="mt-3 text-center" style="font-size: 1.25rem; color: var(--warning);">
                    💡 答案是：机器需要"记忆"——这就是RNN的魔力！
                </p>
            </div>

            <div class="deep-think-box">
                <h4>深入思考：序列理解的本质</h4>
                <div class="think-item">
                    <h5>🎯 语言的时序性</h5>
                    <p>为什么我们不能把一句话的所有词同时说出来？思考一下：</p>
                    <ul>
                        <li>如果把"我爱你"三个字同时发音，会是什么效果？</li>
                        <li>为什么人类语言天然就是序列化的？</li>
                        <li>这种序列化给机器理解带来了什么挑战？</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🧠 记忆与理解</h5>
                    <p>当你读到句子的结尾时，你还记得开头吗？</p>
                    <ul>
                        <li>人类是如何在阅读长句时保持对前文的记忆的？</li>
                        <li>为什么有些句子太长我们就理解困难？</li>
                        <li>这给我们设计神经网络什么启示？</li>
                    </ul>
                </div>
            </div>

            <div class="demo-container">
                <h3 class="text-center mb-3" style="color: var(--primary-light);">🎯 体验序列的重要性</h3>

                <div class="card-grid">
                    <div class="info-card">
                        <span class="card-icon">🎵</span>
                        <h4 class="card-title">音乐旋律</h4>
                        <p>Do Re Mi Fa So La Ti Do</p>
                        <p class="mt-2 text-muted">顺序改变，旋律全变</p>
                    </div>

                    <div class="info-card">
                        <span class="card-icon">📖</span>
                        <h4 class="card-title">故事情节</h4>
                        <p>起因→发展→高潮→结局</p>
                        <p class="mt-2 text-muted">顺序错乱，故事崩塌</p>
                    </div>

                    <div class="info-card">
                        <span class="card-icon">💰</span>
                        <h4 class="card-title">股票价格</h4>
                        <p>昨天100→今天120→明天？</p>
                        <p class="mt-2 text-muted">历史决定预测</p>
                    </div>
                </div>

                <div class="tip info mt-4">
                    <span class="tip-icon">🧠</span>
                    <strong>序列数据无处不在</strong>
                    <p class="mt-2">
                        文字、语音、视频、时间序列...这些数据的共同特点是：<strong>顺序很重要</strong>！
                        前面的内容会影响后面的理解，这就是为什么我们需要一种能"记住"历史的神经网络。
                    </p>
                </div>
            </div>

            <div class="think-box">
                <h4>🤔 思考一下</h4>
                <ul>
                    <li>为什么传统的前馈神经网络处理不了"我爱你"这样的序列？</li>
                    <li>人类是如何理解上下文的？机器能模仿这个过程吗？</li>
                    <li>如果让你设计一个能记住历史的网络，你会怎么做？</li>
                </ul>
            </div>

            <div class="explore-box">
                <h4>探索活动：设计你的记忆系统</h4>
                <p><strong>场景：</strong>你要设计一个系统来理解句子"我在中国北京学习，所以我会说中文"</p>
                <p><strong>挑战：</strong></p>
                <ul>
                    <li>系统需要记住"中国北京"这个地点信息</li>
                    <li>当看到"中文"时，要能关联到之前的地点</li>
                    <li>中间可能有很多无关的词</li>
                </ul>
                <p class="mt-3"><strong>💡 思考：</strong>你会如何设计这个记忆机制？需要存储什么？如何决定记住或忘记？</p>
            </div>
        </section>

        <!-- 为什么需要RNN -->
        <section id="why-rnn" class="section-card">
            <h2>🎯 为什么需要RNN？</h2>

            <div class="story-card">
                <span class="story-icon">🤖</span>
                <p><strong>传统神经网络的困境</strong></p>
                <p class="mt-2">
                    让我们用一个简单的例子来理解问题：
                </p>
                <div class="code-block mt-3">
                    <div class="code-header">
                        <span class="code-lang">传统方法的局限</span>
                    </div>
                    <div class="code-content">
                        <pre>输入："今天天气真好"
传统神经网络看到的：
[今天] → 独立处理 → 输出1
[天气] → 独立处理 → 输出2
[真]   → 独立处理 → 输出3
[好]   → 独立处理 → 输出4

问题：
- 无法理解"真好"是修饰"天气"的
- 不知道"今天"提供了时间信息
- 每个词都是孤立的，没有上下文</pre>
                    </div>
                </div>
            </div>

            <div class="deep-think-box">
                <h4>深度剖析：为什么独立处理不行？</h4>
                <div class="think-item">
                    <h5>🔗 词语间的依赖关系</h5>
                    <p>考虑这个句子："虽然雨很大，但是心情很好"</p>
                    <ul>
                        <li>"虽然"和"但是"形成转折关系，如果独立看，无法理解转折</li>
                        <li>"雨很大"本应导致负面情绪，但"但是"改变了整个语义</li>
                        <li>如果机器只看到"心情很好"，可能误判整个句子的情感</li>
                    </ul>
                    <p class="mt-2"><strong>思考：</strong>这告诉我们什么？为什么上下文如此重要？</p>
                </div>
                <div class="think-item">
                    <h5>💾 信息的累积效应</h5>
                    <p>想象你在读一本推理小说：</p>
                    <ul>
                        <li>第1章：介绍了主角是个侦探</li>
                        <li>第5章：提到了一个可疑的管家</li>
                        <li>第10章：当再次提到"他"时，你需要记得这是指侦探还是管家</li>
                    </ul>
                    <p class="mt-2"><strong>启发：</strong>序列处理需要的不仅是当前信息，还有历史信息的积累</p>
                </div>
            </div>

            <h3>序列处理的三大挑战</h3>

            <div class="algorithm-steps">
                <div class="step-item">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h5>输入长度不固定</h5>
                        <p>句子可长可短，传统网络需要固定输入维度</p>
                        <div class="demo-container mt-3">
                            <p>例子：</p>
                            <div class="sequence-box">我</div>
                            <div class="sequence-box">爱</div>
                            <div class="sequence-box">你</div>
                            <p class="mt-2">vs</p>
                            <div class="sequence-box">我</div>
                            <div class="sequence-box">深</div>
                            <div class="sequence-box">深</div>
                            <div class="sequence-box">地</div>
                            <div class="sequence-box">爱</div>
                            <div class="sequence-box">着</div>
                            <div class="sequence-box">你</div>
                        </div>
                        <div class="think-box mt-3">
                            <h4>🤔 深入思考</h4>
                            <ul>
                                <li>传统解决方案：padding（填充）有什么问题？</li>
                                <li>为什么不能简单地把所有句子都裁剪成相同长度？</li>
                                <li>变长输入对网络架构有什么要求？</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h5>位置信息丢失</h5>
                        <p>词的顺序改变，意思完全不同</p>
                        <div class="comparison-table mt-3">
                            <table>
                                <tr>
                                    <td>"狗咬人" 😱</td>
                                    <td>普通新闻</td>
                                </tr>
                                <tr>
                                    <td>"人咬狗" 😲</td>
                                    <td>大新闻！</td>
                                </tr>
                            </table>
                        </div>
                        <div class="explore-box mt-3">
                            <h4>实验：词序的重要性</h4>
                            <p>试着重新排列这句话的词序："我昨天在公园看到了一只可爱的小狗"</p>
                            <ul>
                                <li>有多少种排列方式？</li>
                                <li>有多少种排列仍然有意义？</li>
                                <li>哪些词的位置相对固定？哪些可以移动？</li>
                            </ul>
                            <p class="mt-2"><strong>发现：</strong>语言有语法规则，但也有灵活性。RNN需要学习这两者！</p>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h5>长距离依赖</h5>
                        <p>相关的信息可能相隔很远</p>
                        <div class="demo-container mt-3">
                            <p><strong>例子：</strong></p>
                            <p>
                                "我在<span style="color: var(--primary);">法国</span>长大，
                                那里有美丽的埃菲尔铁塔，浪漫的塞纳河，
                                还有香醇的红酒，所以我会说流利的<span style="color: var(--primary);">法语</span>。"
                            </p>
                            <p class="mt-2 text-muted">
                                "法国"和"法语"相隔很远，但有强相关性
                            </p>
                        </div>
                        <div class="deep-think-box mt-3">
                            <h4>长距离依赖的挑战</h4>
                            <div class="think-item">
                                <h5>📏 距离与遗忘</h5>
                                <p>实验表明，人类也有类似问题：</p>
                                <ul>
                                    <li>短句：记忆准确率 > 95%</li>
                                    <li>中句（20-30词）：记忆准确率约 80%</li>
                                    <li>长句（50+词）：记忆准确率 < 60%</li>
                                </ul>
                                <p class="mt-2"><strong>问题：</strong>如果人类都有这个问题，机器如何克服？</p>
                            </div>
                            <div class="think-item">
                                <h5>🎯 选择性记忆</h5>
                                <p>并非所有信息都值得长期记忆：</p>
                                <ul>
                                    <li>"的"、"了"、"在"等虚词 → 短期记忆即可</li>
                                    <li>"法国"、"中文"等实词 → 需要长期保持</li>
                                    <li>如何让网络学会这种选择性？</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="tip success mt-4">
                <span class="tip-icon">💡</span>
                <strong>RNN的解决方案</strong>
                <p class="mt-2">
                    RNN通过引入"隐藏状态"（hidden state）来保存历史信息，就像人类的短期记忆。
                    每处理一个新词，都会更新这个记忆，从而理解上下文。
                </p>
            </div>

            <div class="think-box">
                <h4>🤔 预备思考：进入RNN之前</h4>
                <ul>
                    <li>如果你是隐藏状态，你会选择记住什么？忘记什么？</li>
                    <li>隐藏状态的大小应该如何确定？太大或太小会怎样？</li>
                    <li>如何在"记住一切"和"灵活遗忘"之间找到平衡？</li>
                </ul>
            </div>
        </section>

        <!-- RNN基础 -->
        <section id="rnn-basics" class="section-card">
            <h2>🌟 RNN的核心思想</h2>

            <div class="story-card">
                <span class="story-icon">🔄</span>
                <p><strong>一个简单的比喻：传话游戏</strong></p>
                <p class="mt-2">
                    还记得小时候玩的传话游戏吗？第一个人说一句话，传给第二个人，第二个人再传给第三个人...
                </p>
                <p class="mt-2">
                    RNN就像这个游戏：每个时刻不仅处理当前的输入，还要把之前的"记忆"传递下去。
                </p>
            </div>

            <div class="deep-think-box">
                <h4>理解循环：RNN的哲学</h4>
                <div class="think-item">
                    <h5>🔄 循环的本质</h5>
                    <p>为什么叫"循环"神经网络？思考这些循环系统：</p>
                    <ul>
                        <li><strong>水循环：</strong>水→蒸发→云→降雨→水</li>
                        <li><strong>生态循环：</strong>植物→动物→分解者→植物</li>
                        <li><strong>RNN循环：</strong>状态→处理→新状态→处理...</li>
                    </ul>
                    <p class="mt-2"><strong>共同点：</strong>输出会影响下一次的输入，形成反馈回路</p>
                </div>
                <div class="think-item">
                    <h5>🧠 记忆的两面性</h5>
                    <p>隐藏状态既是祝福也是诅咒：</p>
                    <ul>
                        <li><strong>祝福：</strong>能记住历史信息，理解上下文</li>
                        <li><strong>诅咒：</strong>可能记住无用信息，或忘记重要信息</li>
                        <li><strong>平衡：</strong>如何设计一个"智能"的记忆系统？</li>
                    </ul>
                </div>
            </div>

            <div class="rnn-visualization">
                <h3 class="text-center mb-3" style="color: var(--primary-light);">RNN的两种视角</h3>

                <div class="card-grid">
                    <div class="info-card">
                        <h4 class="text-center mb-3">压缩视角</h4>
                        <svg width="300" height="200" viewBox="0 0 300 200">
                            <!-- 输入 -->
                            <rect x="20" y="140" width="60" height="40" fill="#3b82f6" rx="5"/>
                            <text x="50" y="165" text-anchor="middle" fill="white" font-size="14">x</text>

                            <!-- RNN单元 -->
                            <circle cx="150" cy="100" r="40" fill="#8b5cf6"/>
                            <text x="150" y="105" text-anchor="middle" fill="white" font-size="14">RNN</text>

                            <!-- 自循环 -->
                            <path d="M 150 60 Q 200 30, 190 100" stroke="#f59e0b" stroke-width="3" fill="none" marker-end="url(#arrowhead)"/>

                            <!-- 输出 -->
                            <rect x="220" y="80" width="60" height="40" fill="#10b981" rx="5"/>
                            <text x="250" y="105" text-anchor="middle" fill="white" font-size="14">y</text>

                            <!-- 箭头 -->
                            <path d="M 80 160 L 110 120" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead)"/>
                            <path d="M 190 100 L 220 100" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead)"/>

                            <defs>
                                <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#64748b"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="text-center mt-2 text-muted">循环连接保存记忆</p>
                    </div>

                    <div class="info-card">
                        <h4 class="text-center mb-3">展开视角</h4>
                        <svg width="300" height="200" viewBox="0 0 300 200">
                            <!-- 时刻t-1 -->
                            <circle cx="50" cy="100" r="25" fill="#8b5cf6" opacity="0.5"/>
                            <text x="50" y="105" text-anchor="middle" fill="white" font-size="12">h<tspan dy="3" font-size="10">t-1</tspan></text>

                            <!-- 时刻t -->
                            <circle cx="150" cy="100" r="25" fill="#8b5cf6"/>
                            <text x="150" y="105" text-anchor="middle" fill="white" font-size="12">h<tspan dy="3" font-size="10">t</tspan></text>

                            <!-- 时刻t+1 -->
                            <circle cx="250" cy="100" r="25" fill="#8b5cf6" opacity="0.5"/>
                            <text x="250" y="105" text-anchor="middle" fill="white" font-size="12">h<tspan dy="3" font-size="10">t+1</tspan></text>

                            <!-- 连接线 -->
                            <path d="M 75 100 L 125 100" stroke="#f59e0b" stroke-width="2" marker-end="url(#arrowhead2)"/>
                            <path d="M 175 100 L 225 100" stroke="#f59e0b" stroke-width="2" marker-end="url(#arrowhead2)"/>

                            <!-- 输入 -->
                            <rect x="130" y="150" width="40" height="25" fill="#3b82f6" rx="3"/>
                            <text x="150" y="167" text-anchor="middle" fill="white" font-size="12">x<tspan dy="3" font-size="10">t</tspan></text>

                            <!-- 输出 -->
                            <rect x="130" y="25" width="40" height="25" fill="#10b981" rx="3"/>
                            <text x="150" y="42" text-anchor="middle" fill="white" font-size="12">y<tspan dy="3" font-size="10">t</tspan></text>

                            <!-- 箭头 -->
                            <path d="M 150 150 L 150 125" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead2)"/>
                            <path d="M 150 75 L 150 50" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead2)"/>

                            <defs>
                                <marker id="arrowhead2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#64748b"/>
                                </marker>
                            </defs>
                        </svg>
                        <p class="text-center mt-2 text-muted">时间展开后的序列</p>
                    </div>
                </div>

                <div class="explore-box mt-4">
                    <h4>动手理解：展开的意义</h4>
                    <p><strong>练习：</strong>用纸和笔画出处理"我爱你"时的RNN展开图</p>
                    <ol>
                        <li>画三个圆圈代表三个时间步</li>
                        <li>标注每个时间步的输入（我/爱/你）</li>
                        <li>用箭头连接隐藏状态的传递</li>
                        <li>思考：每个隐藏状态包含什么信息？</li>
                    </ol>
                </div>
            </div>

            <div class="math-display">
                <h4 class="mb-3" style="color: var(--primary-light);">RNN的数学表达</h4>

                <p class="math-formula">
                    $$h_t = \tanh(W_{hh} \cdot h_{t-1} + W_{xh} \cdot x_t + b_h)$$
                    $$y_t = W_{hy} \cdot h_t + b_y$$
                </p>

                <div class="formula-explanation">
                    <h5>📖 公式解读：RNN如何工作？</h5>
                    <table class="formula-table">
                        <tr>
                            <td class="formula-part">$h_t$</td>
                            <td class="formula-meaning">当前时刻的隐藏状态（记忆）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$h_{t-1}$</td>
                            <td class="formula-meaning">上一时刻的隐藏状态（之前的记忆）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$x_t$</td>
                            <td class="formula-meaning">当前时刻的输入（如当前的词）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$W_{hh}$</td>
                            <td class="formula-meaning">隐藏状态到隐藏状态的权重（记忆如何传递）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$W_{xh}$</td>
                            <td class="formula-meaning">输入到隐藏状态的权重（新信息如何影响记忆）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$\tanh$</td>
                            <td class="formula-meaning">激活函数，将值压缩到[-1,1]之间</td>
                        </tr>
                    </table>

                    <div class="deep-think-box mt-3">
                        <h4>深入理解每个组件</h4>
                        <div class="think-item">
                            <h5>🎚️ 权重矩阵的含义</h5>
                            <ul>
                                <li><strong>$W_{hh}$：</strong>决定了网络的"记忆力"。如果接近0，网络会快速遗忘；如果太大，可能导致梯度爆炸</li>
                                <li><strong>$W_{xh}$：</strong>决定了网络对新信息的"敏感度"。太小会忽略新输入，太大会被新信息主导</li>
                                <li><strong>平衡点：</strong>这两个权重需要协同工作，找到记忆与更新的平衡</li>
                            </ul>
                        </div>
                        <div class="think-item">
                            <h5>🌊 为什么用tanh？</h5>
                            <ul>
                                <li>输出范围[-1, 1]，可以表示正负两种状态</li>
                                <li>零中心化，有助于下一层的学习</li>
                                <li>但是：在两端会饱和，导致梯度消失</li>
                            </ul>
                            <p class="mt-2"><strong>思考：</strong>如果换成ReLU会怎样？（提示：考虑循环结构）</p>
                        </div>
                    </div>

                    <div class="code-from-math">
                        <h6>💻 代码实现：</h6>
                        <pre style="color: #e6edf3; margin: 0;">
<span class="keyword">def</span> <span class="function">rnn_step</span>(x_t, h_prev, W_hh, W_xh, W_hy, b_h, b_y):
    <span class="string">"""RNN的一个时间步"""</span>
    <span class="comment"># 计算新的隐藏状态</span>
    h_t = np.tanh(
        np.dot(W_hh, h_prev) +  <span class="comment"># 之前的记忆</span>
        np.dot(W_xh, x_t) +     <span class="comment"># 当前的输入</span>
        b_h                     <span class="comment"># 偏置</span>
    )

    <span class="comment"># 计算输出</span>
    y_t = np.dot(W_hy, h_t) + b_y

    <span class="keyword">return</span> h_t, y_t</pre>
                    </div>

                    <p class="mt-3"><strong>💡 直观理解：</strong></p>
                    <ul>
                        <li>就像炒菜时，既要加新的调料（$x_t$），也要考虑之前的味道（$h_{t-1}$）</li>
                        <li>$W_{hh}$决定了"记忆力"有多强</li>
                        <li>$W_{xh}$决定了对新信息的"敏感度"</li>
                    </ul>
                </div>
            </div>

            <div class="demo-container mt-4">
                <h3 class="mb-3">🎮 交互演示：RNN处理"我爱你"</h3>

                <div class="time-series-diagram">
                    <div class="time-step">
                        <div class="time-step-label">t=1</div>
                        <div class="sequence-box active">我</div>
                        <div class="hidden-state">h₁</div>
                        <p class="text-muted mt-2">初始记忆</p>
                    </div>

                    <div class="time-step">
                        <div class="time-step-label">t=2</div>
                        <div class="sequence-box">爱</div>
                        <div class="hidden-state">h₂</div>
                        <p class="text-muted mt-2">记住"我+爱"</p>
                    </div>

                    <div class="time-step">
                        <div class="time-step-label">t=3</div>
                        <div class="sequence-box">你</div>
                        <div class="hidden-state">h₃</div>
                        <p class="text-muted mt-2">理解完整句子</p>
                    </div>
                </div>

                <div class="think-box mt-3">
                    <h4>🤔 理解隐藏状态的演化</h4>
                    <ul>
                        <li>$h_1$只包含"我"的信息，可能编码了主语信息</li>
                        <li>$h_2$结合了"我"和"爱"，开始形成主谓结构</li>
                        <li>$h_3$包含完整句子，可以输出情感分类或下一个词预测</li>
                        <li>思考：如果句子是"我爱你的狗"，$h_3$和$h_4$会如何变化？</li>
                    </ul>
                </div>

                <div class="tip info mt-3">
                    <span class="tip-icon">💭</span>
                    <strong>关键洞察</strong>
                    <p class="mt-2">
                        每个隐藏状态$h_t$都包含了从句子开始到当前位置的所有信息。
                        这就是RNN能理解"我爱你"、"我爱你的狗"区别的原因！
                    </p>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>实验：隐藏状态的容量</h4>
                <p><strong>假设：</strong>隐藏状态维度为100</p>
                <p><strong>问题：</strong></p>
                <ul>
                    <li>100个数字能编码多少信息？</li>
                    <li>如果词汇表有10000个词，100维够用吗？</li>
                    <li>增加到1000维会有什么好处和坏处？</li>
                </ul>
                <p class="mt-3"><strong>实验思路：</strong>用不同大小的隐藏状态训练模型，观察性能变化</p>
            </div>
        </section>

        <!-- 前向传播 -->
        <section id="forward-prop" class="section-card">
            <h2>➡️ 前向传播：一步步理解</h2>

            <div class="story-card">
                <span class="story-icon">🚶</span>
                <p><strong>像走路一样，一步一个脚印</strong></p>
                <p class="mt-2">
                    RNN的前向传播就像走路：每一步都依赖于前一步的位置（隐藏状态），
                    同时根据当前的路况（输入）调整方向。
                </p>
            </div>

            <div class="deep-think-box">
                <h4>前向传播的哲学思考</h4>
                <div class="think-item">
                    <h5>⏱️ 时间的不可逆性</h5>
                    <p>前向传播模拟了时间的单向流动：</p>
                    <ul>
                        <li>只能从过去到未来，不能反向</li>
                        <li>每个时刻只能基于"已知"预测"未知"</li>
                        <li>这种限制反而给了RNN理解因果关系的能力</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🎭 信息的变换</h5>
                    <p>观察信息如何在网络中流动和变换：</p>
                    <ul>
                        <li>原始输入（词）→ 词向量（密集表示）</li>
                        <li>词向量 + 历史 → 新的理解（隐藏状态）</li>
                        <li>隐藏状态 → 输出（预测/分类）</li>
                    </ul>
                    <p class="mt-2"><strong>每一步都是一次抽象和提炼</strong></p>
                </div>
            </div>

            <div class="demo-container">
                <h3 class="mb-3">详细的前向传播过程</h3>

                <div class="code-block">
                    <div class="code-header">
                        <span class="code-lang">Python - 完整的RNN前向传播</span>
                        <div class="code-actions">
                            <button class="code-btn" onclick="toggleCode(this)">展开</button>
                            <button class="code-btn" onclick="copyCode(this)">复制</button>
                        </div>
                    </div>
                    <div class="code-content">
                        <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="keyword">class</span> <span class="function">SimpleRNN</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(self, input_size, hidden_size, output_size):
        <span class="string">"""初始化RNN参数"""</span>
        <span class="comment"># 初始化权重（使用小的随机值）</span>
        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01

        <span class="comment"># 初始化偏置</span>
        self.b_h = np.zeros((hidden_size, 1))
        self.b_y = np.zeros((output_size, 1))

        <span class="comment"># 保存维度信息</span>
        self.hidden_size = hidden_size

    <span class="keyword">def</span> <span class="function">forward</span>(self, inputs):
        <span class="string">"""
        前向传播
        inputs: 输入序列 [x1, x2, ..., xT]
        """</span>
        T = len(inputs)  <span class="comment"># 序列长度</span>

        <span class="comment"># 初始化</span>
        h = np.zeros((self.hidden_size, 1))  <span class="comment"># 初始隐藏状态</span>
        self.h_states = {-1: h}              <span class="comment"># 保存所有隐藏状态</span>
        self.y_outputs = []                  <span class="comment"># 保存所有输出</span>

        <span class="comment"># 逐时间步处理</span>
        <span class="keyword">for</span> t <span class="keyword">in</span> range(T):
            <span class="comment"># 1. 计算新的隐藏状态</span>
            h = np.tanh(
                np.dot(self.W_xh, inputs[t]) +    <span class="comment"># 输入贡献</span>
                np.dot(self.W_hh, h) +            <span class="comment"># 历史贡献</span>
                self.b_h                          <span class="comment"># 偏置</span>
            )

            <span class="comment"># 2. 计算输出</span>
            y = np.dot(self.W_hy, h) + self.b_y

            <span class="comment"># 3. 保存状态（用于反向传播）</span>
            self.h_states[t] = h
            self.y_outputs.append(y)

        <span class="keyword">return</span> self.y_outputs, self.h_states

<span class="comment"># 使用示例：处理"我爱你"</span>
<span class="comment"># 假设词向量维度为3</span>
word_vectors = {
    <span class="string">'我'</span>: np.array([[0.1], [0.2], [0.3]]),
    <span class="string">'爱'</span>: np.array([[0.4], [0.5], [0.6]]),
    <span class="string">'你'</span>: np.array([[0.7], [0.8], [0.9]])
}

<span class="comment"># 创建RNN</span>
rnn = SimpleRNN(input_size=<span class="number">3</span>, hidden_size=<span class="number">4</span>, output_size=<span class="number">2</span>)

<span class="comment"># 前向传播</span>
inputs = [word_vectors[<span class="string">'我'</span>], word_vectors[<span class="string">'爱'</span>], word_vectors[<span class="string">'你'</span>]]
outputs, hidden_states = rnn.forward(inputs)

print(<span class="string">"每个时间步的隐藏状态维度:"</span>, hidden_states[<span class="number">0</span>].shape)
print(<span class="string">"最终输出个数:"</span>, len(outputs))</pre>
                    </div>
                </div>

                <div class="think-box mt-3">
                    <h4>🤔 代码细节思考</h4>
                    <ul>
                        <li>为什么权重初始化要乘以0.01？如果初始化为1会怎样？</li>
                        <li>为什么要保存所有的隐藏状态？（提示：反向传播需要）</li>
                        <li>如果输入序列很长（如1000个词），会有什么问题？</li>
                    </ul>
                </div>

                <div class="formula-explanation mt-3">
                    <h5>📊 可视化每一步的计算</h5>

                    <div class="time-series-diagram">
                        <div class="text-center mb-3">
                            <p><strong>输入序列："我爱你"</strong></p>
                        </div>

                        <table class="formula-table" style="width: 100%;">
                            <tr style="background: var(--bg-dark);">
                                <th style="padding: 1rem;">时间步</th>
                                <th style="padding: 1rem;">输入词</th>
                                <th style="padding: 1rem;">计算过程</th>
                                <th style="padding: 1rem;">隐藏状态含义</th>
                            </tr>
                            <tr>
                                <td style="padding: 1rem;">t=0</td>
                                <td style="padding: 1rem;">我</td>
                                <td style="padding: 1rem;">
                                    $h_0 = \tanh(W_{xh} \cdot x_0 + W_{hh} \cdot 0)$
                                </td>
                                <td style="padding: 1rem;">主语信息</td>
                            </tr>
                            <tr>
                                <td style="padding: 1rem;">t=1</td>
                                <td style="padding: 1rem;">爱</td>
                                <td style="padding: 1rem;">
                                    $h_1 = \tanh(W_{xh} \cdot x_1 + W_{hh} \cdot h_0)$
                                </td>
                                <td style="padding: 1rem;">主语+动词</td>
                            </tr>
                            <tr>
                                <td style="padding: 1rem;">t=2</td>
                                <td style="padding: 1rem;">你</td>
                                <td style="padding: 1rem;">
                                    $h_2 = \tanh(W_{xh} \cdot x_2 + W_{hh} \cdot h_1)$
                                </td>
                                <td style="padding: 1rem;">完整句子语义</td>
                            </tr>
                        </table>
                    </div>

                    <div class="deep-think-box mt-3">
                        <h4>深入理解计算过程</h4>
                        <div class="think-item">
                            <h5>🔢 维度变换</h5>
                            <p>追踪数据维度的变化：</p>
                            <ul>
                                <li>输入词向量：3维 → 隐藏状态：4维</li>
                                <li>为什么要改变维度？（提示：表达能力）</li>
                                <li>隐藏层维度如何选择？太大或太小会怎样？</li>
                            </ul>
                        </div>
                        <div class="think-item">
                            <h5>🌀 非线性的作用</h5>
                            <p>tanh激活函数的重要性：</p>
                            <ul>
                                <li>如果没有tanh，只有线性变换会怎样？</li>
                                <li>多个线性变换的组合还是线性的</li>
                                <li>非线性让网络能学习复杂的模式</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>动手实验：观察隐藏状态的变化</h4>
                <p><strong>实验设置：</strong></p>
                <ol>
                    <li>用相同的RNN处理不同的句子：
                        <ul>
                            <li>"我爱你"</li>
                            <li>"我恨你"</li>
                            <li>"你爱我"</li>
                        </ul>
                    </li>
                    <li>可视化每个句子的隐藏状态序列</li>
                    <li>观察：
                        <ul>
                            <li>哪些句子的隐藏状态相似？</li>
                            <li>词序改变如何影响最终状态？</li>
                            <li>情感词（爱/恨）如何改变轨迹？</li>
                        </ul>
                    </li>
                </ol>
                <p class="mt-3"><strong>发现：</strong>隐藏状态确实编码了语义信息！</p>
            </div>

            <div class="tip warning mt-4">
                <span class="tip-icon">⚠️</span>
                <strong>注意tanh激活函数</strong>
                <p class="mt-2">
                    为什么用tanh而不是ReLU？因为tanh输出在[-1,1]之间，可以表示正负两种状态，
                    更适合建模语言中的对立关系（如积极/消极）。但这也导致了梯度消失问题...
                </p>
            </div>
        </section>

        <!-- 反向传播 -->
        <section id="backprop" class="section-card">
            <h2>⬅️ 反向传播：时间中的梯度</h2>

            <div class="story-card">
                <span class="story-icon">⏰</span>
                <p><strong>时间旅行的梯度</strong></p>
                <p class="mt-2">
                    如果说前向传播是顺着时间前进，那么反向传播就是逆着时间回溯。
                    每个时刻的错误不仅要修正当前的参数，还要告诉过去："你们也有责任！"
                </p>
            </div>

            <div class="deep-think-box">
                <h4>BPTT的深层含义</h4>
                <div class="think-item">
                    <h5>⏮️ 因果链的回溯</h5>
                    <p>BPTT本质上是在回答：</p>
                    <ul>
                        <li>"如果我在时刻t犯了错，是因为什么？"</li>
                        <li>"是当前输入的问题，还是之前记忆的问题？"</li>
                        <li>"如何分配责任，让每个时刻都学到教训？"</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🎯 梯度的长征</h5>
                    <p>梯度需要穿越时间，就像长征：</p>
                    <ul>
                        <li>路途遥远（时间步多）</li>
                        <li>逐渐衰减（梯度消失）</li>
                        <li>可能迷失方向（梯度爆炸）</li>
                    </ul>
                    <p class="mt-2"><strong>关键问题：如何让梯度安全到达目的地？</strong></p>
                </div>
            </div>

            <div class="math-display">
                <h4 class="mb-3" style="color: var(--primary-light);">BPTT：通过时间的反向传播</h4>

                <p>损失函数（以分类任务为例）：</p>
                <p class="math-formula">
                    $$L = \sum_{t=1}^{T} L_t = -\sum_{t=1}^{T} \log P(y_t^{true} | x_1, ..., x_t)$$
                </p>

                <p class="mt-3">关键梯度计算：</p>
                <p class="math-formula">
                    $$\frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W_{hh}}$$
                </p>

                <div class="formula-explanation">
                    <h5>📖 为什么BPTT这么复杂？</h5>

                    <p><strong>核心问题：</strong>参数$W_{hh}$在每个时间步都被使用！</p>

                    <div class="demo-container mt-3">
                        <svg width="600" height="250" viewBox="0 0 600 250">
                            <!-- 时间步 -->
                            <text x="50" y="30" fill="#94a3b8" font-size="14">t=1</text>
                            <text x="150" y="30" fill="#94a3b8" font-size="14">t=2</text>
                            <text x="250" y="30" fill="#94a3b8" font-size="14">t=3</text>
                            <text x="350" y="30" fill="#94a3b8" font-size="14">t=4</text>

                            <!-- 隐藏状态 -->
                            <circle cx="50" cy="100" r="30" fill="#8b5cf6"/>
                            <text x="50" y="105" text-anchor="middle" fill="white" font-size="12">h₁</text>

                            <circle cx="150" cy="100" r="30" fill="#8b5cf6"/>
                            <text x="150" y="105" text-anchor="middle" fill="white" font-size="12">h₂</text>

                            <circle cx="250" cy="100" r="30" fill="#8b5cf6"/>
                            <text x="250" y="105" text-anchor="middle" fill="white" font-size="12">h₃</text>

                            <circle cx="350" cy="100" r="30" fill="#8b5cf6"/>
                            <text x="350" y="105" text-anchor="middle" fill="white" font-size="12">h₄</text>

                            <!-- 损失 -->
                            <rect x="320" y="180" width="60" height="30" fill="#ef4444" rx="5"/>
                            <text x="350" y="200" text-anchor="middle" fill="white" font-size="12">Loss</text>

                            <!-- 前向连接 -->
                            <path d="M 80 100 L 120 100" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead3)"/>
                            <path d="M 180 100 L 220 100" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead3)"/>
                            <path d="M 280 100 L 320 100" stroke="#64748b" stroke-width="2" marker-end="url(#arrowhead3)"/>

                            <!-- 反向梯度 -->
                            <path d="M 350 180 L 350 130" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead4)"/>
                            <path d="M 320 100 L 280 100" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead4)"/>
                            <path d="M 220 100 L 180 100" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead4)"/>
                            <path d="M 120 100 L 80 100" stroke="#ef4444" stroke-width="2" stroke-dasharray="5,5" marker-end="url(#arrowhead4)"/>

                            <!-- W_hh标记 -->
                            <text x="100" y="85" fill="#f59e0b" font-size="10">W_hh</text>
                            <text x="200" y="85" fill="#f59e0b" font-size="10">W_hh</text>
                            <text x="300" y="85" fill="#f59e0b" font-size="10">W_hh</text>

                            <!-- 梯度累积说明 -->
                            <text x="450" y="100" fill="#ef4444" font-size="14">梯度逐层</text>
                            <text x="450" y="120" fill="#ef4444" font-size="14">向前传递</text>

                            <defs>
                                <marker id="arrowhead3" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#64748b"/>
                                </marker>
                                <marker id="arrowhead4" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                                    <polygon points="0 0, 10 3.5, 0 7" fill="#ef4444"/>
                                </marker>
                            </defs>
                        </svg>
                    </div>

                    <p class="mt-3"><strong>💡 关键洞察：</strong></p>
                    <ul>
                        <li>梯度需要穿越多个时间步，每次都要乘以$W_{hh}$</li>
                        <li>如果$W_{hh}$的值小于1，梯度会指数级衰减（梯度消失）</li>
                        <li>如果$W_{hh}$的值大于1，梯度会指数级增长（梯度爆炸）</li>
                    </ul>

                    <div class="deep-think-box mt-3">
                        <h4>数学直觉：为什么会消失/爆炸？</h4>
                        <div class="think-item">
                            <h5>📉 指数衰减的数学</h5>
                            <p>考虑简化情况，假设梯度传播n步：</p>
                            <p class="text-center">梯度 ∝ $(W_{hh})^n$</p>
                            <ul>
                                <li>如果$|W_{hh}| = 0.9$，则$(0.9)^{10} ≈ 0.35$</li>
                                <li>如果$|W_{hh}| = 0.9$，则$(0.9)^{50} ≈ 0.005$</li>
                                <li>50步后，梯度几乎为0！</li>
                            </ul>
                        </div>
                        <div class="think-item">
                            <h5>📈 指数增长的危险</h5>
                            <p>相反的情况：</p>
                            <ul>
                                <li>如果$|W_{hh}| = 1.1$，则$(1.1)^{50} ≈ 117$</li>
                                <li>梯度变得巨大，参数更新失控</li>
                                <li>模型权重可能变成NaN</li>
                            </ul>
                        </div>
                    </div>

                    <div class="code-from-math">
                        <h6>💻 简化的BPTT实现：</h6>
                        <pre style="color: #e6edf3; margin: 0;">
<span class="keyword">def</span> <span class="function">backward</span>(self, targets, learning_rate=0.01):
    <span class="string">"""反向传播通过时间(BPTT)"""</span>
    T = len(targets)

    <span class="comment"># 初始化梯度</span>
    dW_xh = np.zeros_like(self.W_xh)
    dW_hh = np.zeros_like(self.W_hh)
    dW_hy = np.zeros_like(self.W_hy)

    <span class="comment"># 从最后一个时间步开始反向传播</span>
    dh_next = np.zeros_like(self.h_states[0])

    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T)):
        <span class="comment"># 1. 输出层的梯度</span>
        dy = self.y_outputs[t] - targets[t]
        dW_hy += np.dot(dy, self.h_states[t].T)

        <span class="comment"># 2. 隐藏层的梯度</span>
        dh = np.dot(self.W_hy.T, dy) + dh_next
        dh_raw = (1 - self.h_states[t]**2) * dh  <span class="comment"># tanh的导数</span>

        <span class="comment"># 3. 参数梯度</span>
        dW_xh += np.dot(dh_raw, inputs[t].T)
        dW_hh += np.dot(dh_raw, self.h_states[t-1].T)

        <span class="comment"># 4. 传递给前一时间步的梯度</span>
        dh_next = np.dot(self.W_hh.T, dh_raw)

    <span class="comment"># 梯度裁剪（防止梯度爆炸）</span>
    <span class="keyword">for</span> dparam <span class="keyword">in</span> [dW_xh, dW_hh, dW_hy]:
        np.clip(dparam, -5, 5, out=dparam)

    <span class="comment"># 参数更新</span>
    self.W_xh -= learning_rate * dW_xh
    self.W_hh -= learning_rate * dW_hh
    self.W_hy -= learning_rate * dW_hy</pre>
                    </div>

                    <div class="think-box mt-3">
                        <h4>🤔 代码中的关键点</h4>
                        <ul>
                            <li><strong>反向循环：</strong>为什么要从最后一个时间步开始？</li>
                            <li><strong>梯度累积：</strong>dh_next的作用是什么？</li>
                            <li><strong>梯度裁剪：</strong>clip操作如何防止梯度爆炸？</li>
                            <li><strong>tanh导数：</strong>$(1 - h_t^2)$是怎么来的？</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>实验：观察梯度传播</h4>
                <p><strong>设置：</strong>构建一个10步的RNN，观察梯度如何传播</p>
                <ol>
                    <li>在最后一步设置误差为1.0</li>
                    <li>记录每一步收到的梯度大小</li>
                    <li>尝试不同的权重初始化：
                        <ul>
                            <li>W_hh ~ N(0, 0.01)</li>
                            <li>W_hh ~ N(0, 1.0)</li>
                            <li>W_hh = 0.9 * I（接近单位矩阵）</li>
                        </ul>
                    </li>
                    <li>绘制梯度大小vs时间步的曲线</li>
                </ol>
                <p class="mt-3"><strong>预期结果：</strong>你会看到梯度消失/爆炸的实际效果！</p>
            </div>

            <div class="think-box mt-4">
                <h4>🤔 思考一下</h4>
                <ul>
                    <li>为什么RNN的训练比普通神经网络困难得多？</li>
                    <li>梯度裁剪是如何防止梯度爆炸的？有什么副作用吗？</li>
                    <li>如果序列很长（比如100个时间步），会发生什么？</li>
                    <li>能否设计一种不需要BPTT的训练方法？</li>
                </ul>
            </div>
        </section>

        <!-- 梯度消失问题 -->
        <section id="vanishing-gradient" class="section-card">
            <h2>💀 梯度消失问题</h2>

            <div class="story-card">
                <span class="story-icon">📉</span>
                <p><strong>记忆的衰退</strong></p>
                <p class="mt-2">
                    想象你在传话游戏中，每个人都只能小声说话（相当于梯度乘以小于1的数）。
                    传到第10个人时，声音已经小到听不见了。这就是梯度消失！
                </p>
            </div>

            <div class="deep-think-box">
                <h4>梯度消失的本质</h4>
                <div class="think-item">
                    <h5>🔬 从生物学角度理解</h5>
                    <p>人脑也有类似的"梯度消失"：</p>
                    <ul>
                        <li>短期记忆：容量有限（7±2个项目）</li>
                        <li>遗忘曲线：信息随时间指数衰减</li>
                        <li>但人脑有解决方案：重要信息会被强化，转入长期记忆</li>
                    </ul>
                    <p class="mt-2"><strong>启发：</strong>RNN也需要类似的机制来保护重要信息</p>
                </div>
                <div class="think-item">
                    <h5>⚡ 信息论视角</h5>
                    <p>每经过一个时间步，信息都会损失：</p>
                    <ul>
                        <li>类似于有损压缩</li>
                        <li>重要信息和噪声一起衰减</li>
                        <li>最终信噪比太低，无法恢复原始信息</li>
                    </ul>
                </div>
            </div>

            <div class="demo-container">
                <h3 class="mb-3">梯度消失的数学原理</h3>

                <div class="math-display">
                    <p>考虑梯度如何通过时间传播：</p>
                    <p class="math-formula">
                        $$\frac{\partial h_t}{\partial h_k} = \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} = \prod_{i=k+1}^{t} W_{hh}^T \cdot \text{diag}(f'(h_{i-1}))$$
                    </p>

                    <div class="formula-explanation">
                        <h5>📖 为什么会消失？</h5>

                        <p>对于tanh激活函数：$f'(x) = 1 - \tanh^2(x) \in [0, 1]$</p>

                        <div class="demo-container mt-3">
                            <p><strong>举个例子：</strong>假设每步梯度缩小0.5倍</p>
                            <pre style="background: var(--bg-code); padding: 1rem; border-radius: 0.5rem; color: #e6edf3;">
时间步1: 梯度 = 1.0
时间步2: 梯度 = 1.0 × 0.5 = 0.5
时间步3: 梯度 = 0.5 × 0.5 = 0.25
时间步4: 梯度 = 0.25 × 0.5 = 0.125
...
时间步10: 梯度 = 0.5^10 ≈ 0.001

<span class="comment"># 10步后，梯度几乎消失！</span></pre>
                        </div>

                        <div class="explore-box mt-3">
                            <h4>动手计算：梯度衰减速度</h4>
                            <p><strong>问题：</strong>如果每步梯度缩小为原来的r倍，需要多少步梯度会小于0.01？</p>
                            <ul>
                                <li>r = 0.9时：需要约44步</li>
                                <li>r = 0.8时：需要约21步</li>
                                <li>r = 0.7时：需要约13步</li>
                            </ul>
                            <p class="mt-2"><strong>发现：</strong>即使是0.9这样接近1的值，梯度也会快速消失！</p>
                        </div>

                        <div class="tip danger mt-3">
                            <span class="tip-icon">🚨</span>
                            <strong>严重后果</strong>
                            <ul class="mt-2">
                                <li>无法学习长距离依赖（超过5-10个时间步）</li>
                                <li>早期的输入对最终输出几乎没有影响</li>
                                <li>训练变得极其缓慢甚至停滞</li>
                            </ul>
                        </div>

                        <div class="deep-think-box mt-3">
                            <h4>更深层的问题</h4>
                            <div class="think-item">
                                <h5>🎭 梯度消失的不对称性</h5>
                                <p>不是所有信息都同等重要：</p>
                                <ul>
                                    <li>关键词（如人名、地名）的梯度消失 → 灾难性</li>
                                    <li>填充词（如"的"、"了"）的梯度消失 → 可接受</li>
                                    <li>但RNN无差别地让所有梯度消失</li>
                                </ul>
                            </div>
                            <div class="think-item">
                                <h5>🌊 复合效应</h5>
                                <p>梯度消失不是孤立的：</p>
                                <ul>
                                    <li>导致参数更新缓慢</li>
                                    <li>网络倾向于学习短期模式</li>
                                    <li>形成恶性循环：越不更新，越学不到长期依赖</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="info-card mt-4">
                <span class="card-icon">💊</span>
                <h4 class="card-title">解决方案预览</h4>
                <ul>
                    <li><strong>LSTM：</strong>引入"高速公路"让梯度直接流过</li>
                    <li><strong>GRU：</strong>简化版的LSTM，更容易训练</li>
                    <li><strong>残差连接：</strong>给梯度提供"捷径"</li>
                    <li><strong>梯度裁剪：</strong>防止梯度爆炸</li>
                </ul>
            </div>

            <div class="think-box mt-4">
                <h4>🤔 创新思考</h4>
                <ul>
                    <li>如果你来设计解决方案，会怎么做？</li>
                    <li>能否让网络自动识别哪些信息需要长期保存？</li>
                    <li>除了改变网络结构，还有其他方法吗？（提示：优化器、初始化...）</li>
                </ul>
            </div>
        </section>

        <!-- LSTM -->
        <section id="lstm" class="section-card">
            <h2>🚪 LSTM：长短期记忆网络</h2>

            <div class="story-card">
                <span class="story-icon">🏰</span>
                <p><strong>记忆的城堡</strong></p>
                <p class="mt-2">
                    如果RNN是一个简单的房间，那么LSTM就是一座精心设计的城堡。
                    它有多个门（Gate）来控制信息的流动：
                </p>
                <ul class="mt-2">
                    <li>🚪 <strong>遗忘门：</strong>决定忘记什么</li>
                    <li>🚪 <strong>输入门：</strong>决定记住什么</li>
                    <li>🚪 <strong>输出门：</strong>决定输出什么</li>
                </ul>
            </div>

            <div class="deep-think-box">
                <h4>LSTM的设计哲学</h4>
                <div class="think-item">
                    <h5>🧠 模仿人类记忆</h5>
                    <p>LSTM的设计灵感来自人类记忆系统：</p>
                    <ul>
                        <li><strong>工作记忆：</strong>对应隐藏状态h，处理当前任务</li>
                        <li><strong>长期记忆：</strong>对应细胞状态C，存储重要信息</li>
                        <li><strong>注意力机制：</strong>门控制什么信息值得记住</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🎛️ 精细控制的艺术</h5>
                    <p>三个门的协同工作：</p>
                    <ul>
                        <li>不是简单的开/关，而是0-1之间的连续控制</li>
                        <li>每个门都是学习得到的，不是预设的</li>
                        <li>可以实现复杂的记忆策略</li>
                    </ul>
                </div>
            </div>

            <div class="rnn-visualization">
                <h3 class="text-center mb-3" style="color: var(--primary-light);">LSTM单元结构</h3>

                <svg width="800" height="400" viewBox="0 0 800 400" style="background: var(--bg-dark); border-radius: 1rem;">
                    <!-- 标题 -->
                    <text x="400" y="30" text-anchor="middle" fill="#f1f5f9" font-size="18" font-weight="bold">LSTM内部结构</text>

                    <!-- 细胞状态线 -->
                    <line x1="50" y1="100" x2="750" y2="100" stroke="#10b981" stroke-width="3"/>
                    <text x="400" y="85" text-anchor="middle" fill="#10b981" font-size="12">细胞状态 C<tspan dy="3" font-size="10">t</tspan></text>

                    <!-- 遗忘门 -->
                    <rect x="150" y="150" width="80" height="60" fill="#ef4444" rx="10" opacity="0.8"/>
                    <text x="190" y="185" text-anchor="middle" fill="white" font-size="14" font-weight="bold">遗忘门</text>
                    <text x="190" y="200" text-anchor="middle" fill="white" font-size="11">f<tspan dy="3" font-size="9">t</tspan></text>

                    <!-- 输入门 -->
                    <rect x="350" y="150" width="80" height="60" fill="#3b82f6" rx="10" opacity="0.8"/>
                    <text x="390" y="185" text-anchor="middle" fill="white" font-size="14" font-weight="bold">输入门</text>
                    <text x="390" y="200" text-anchor="middle" fill="white" font-size="11">i<tspan dy="3" font-size="9">t</tspan></text>

                    <!-- 输出门 -->
                    <rect x="550" y="150" width="80" height="60" fill="#f59e0b" rx="10" opacity="0.8"/>
                    <text x="590" y="185" text-anchor="middle" fill="white" font-size="14" font-weight="bold">输出门</text>
                    <text x="590" y="200" text-anchor="middle" fill="white" font-size="11">o<tspan dy="3" font-size="9">t</tspan></text>

                    <!-- 输入 -->
                    <rect x="50" y="300" width="60" height="40" fill="#8b5cf6" rx="5"/>
                    <text x="80" y="325" text-anchor="middle" fill="white" font-size="12">x<tspan dy="3" font-size="10">t</tspan></text>

                    <rect x="150" y="300" width="60" height="40" fill="#8b5cf6" rx="5"/>
                    <text x="180" y="325" text-anchor="middle" fill="white" font-size="12">h<tspan dy="3" font-size="10">t-1</tspan></text>

                    <!-- 连接线和操作符号 -->
                    <circle cx="190" cy="100" r="20" fill="#ef4444" opacity="0.3"/>
                    <text x="190" y="105" text-anchor="middle" fill="white" font-size="20" font-weight="bold">×</text>

                    <circle cx="390" cy="100" r="20" fill="#3b82f6" opacity="0.3"/>
                    <text x="390" y="105" text-anchor="middle" fill="white" font-size="20" font-weight="bold">+</text>

                    <circle cx="590" cy="250" r="20" fill="#f59e0b" opacity="0.3"/>
                    <text x="590" y="255" text-anchor="middle" fill="white" font-size="20" font-weight="bold">×</text>

                    <!-- 输出 -->
                    <rect x="680" y="230" width="60" height="40" fill="#10b981" rx="5"/>
                    <text x="710" y="255" text-anchor="middle" fill="white" font-size="12">h<tspan dy="3" font-size="10">t</tspan></text>
                </svg>
            </div>

            <div class="math-display mt-4">
                <h4 class="mb-3" style="color: var(--primary-light);">LSTM的数学公式</h4>

                <p class="math-formula">
                    $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
                    $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$
                    $\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$
                    $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
                    $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$
                    $h_t = o_t * \tanh(C_t)$
                </p>

                <div class="formula-explanation">
                    <h5>📖 LSTM工作原理解析</h5>

                    <table class="formula-table">
                        <tr>
                            <td class="formula-part">遗忘门 $f_t$</td>
                            <td class="formula-meaning">决定从上一时刻的记忆中忘记什么（0=全忘，1=全记）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">输入门 $i_t$</td>
                            <td class="formula-meaning">决定当前输入的哪些信息值得记住</td>
                        </tr>
                        <tr>
                            <td class="formula-part">候选值 $\tilde{C}_t$</td>
                            <td class="formula-meaning">当前时刻的新信息候选</td>
                        </tr>
                        <tr>
                            <td class="formula-part">细胞状态 $C_t$</td>
                            <td class="formula-meaning">更新后的长期记忆</td>
                        </tr>
                        <tr>
                            <td class="formula-part">输出门 $o_t$</td>
                            <td class="formula-meaning">决定输出记忆的哪些部分</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$\sigma$</td>
                            <td class="formula-meaning">Sigmoid函数，输出0-1之间，用作门控信号</td>
                        </tr>
                    </table>

                    <div class="deep-think-box mt-3">
                        <h4>深入理解每个组件</h4>
                        <div class="think-item">
                            <h5>🎚️ 权重矩阵的含义</h5>
                            <ul>
                                <li><strong>$W_{hh}$：</strong>决定了网络的"记忆力"。如果接近0，网络会快速遗忘；如果太大，可能导致梯度爆炸</li>
                                <li><strong>$W_{xh}$：</strong>决定了网络对新信息的"敏感度"。太小会忽略新输入，太大会被新信息主导</li>
                                <li><strong>平衡点：</strong>这两个权重需要协同工作，找到记忆与更新的平衡</li>
                            </ul>
                        </div>
                        <div class="think-item">
                            <h5>🌊 为什么用tanh？</h5>
                            <ul>
                                <li>输出范围[-1, 1]，可以表示正负两种状态</li>
                                <li>零中心化，有助于下一层的学习</li>
                                <li>但是：在两端会饱和，导致梯度消失</li>
                            </ul>
                            <p class="mt-2"><strong>思考：</strong>如果换成ReLU会怎样？（提示：考虑循环结构）</p>
                        </div>
                    </div>

                    <div class="demo-container mt-3">
                        <h6>🎯 举例理解：处理"我在法国长大...所以我会说法语"</h6>

                        <div class="time-series-diagram">
                            <div class="time-step">
                                <p><strong>读到"法国"时：</strong></p>
                                <ul style="text-align: left;">
                                    <li>输入门：打开！这是重要信息</li>
                                    <li>记忆：C = {..., 国家:法国, ...}</li>
                                </ul>
                            </div>

                            <div class="time-step">
                                <p><strong>读到中间内容时：</strong></p>
                                <ul style="text-align: left;">
                                    <li>遗忘门：保持关闭，别忘记"法国"</li>
                                    <li>记忆：C = {..., 国家:法国, ...} ✓</li>
                                </ul>
                            </div>

                            <div class="time-step">
                                <p><strong>读到"法语"时：</strong></p>
                                <ul style="text-align: left;">
                                    <li>输出门：打开！输出相关信息</li>
                                    <li>输出：基于"法国"预测"法语"</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div class="explore-box mt-3">
                        <h4>实验：门的协同工作</h4>
                        <p><strong>场景模拟：</strong>处理句子"我昨天买了苹果，今天买了香蕉"</p>
                        <ul>
                            <li><strong>时刻1（"昨天"）：</strong>
                                <ul>
                                    <li>输入门：记住时间信息</li>
                                    <li>C中存储：time=昨天</li>
                                </ul>
                            </li>
                            <li><strong>时刻2（"今天"）：</strong>
                                <ul>
                                    <li>遗忘门：忘记"昨天"</li>
                                    <li>输入门：记住"今天"</li>
                                    <li>C更新为：time=今天</li>
                                </ul>
                            </li>
                        </ul>
                        <p class="mt-2"><strong>思考：</strong>如果门控制失效会怎样？</p>
                    </div>

                    <div class="code-from-math">
                        <h6>💻 LSTM前向传播代码：</h6>
                        <pre style="color: #e6edf3; margin: 0;">
<span class="keyword">class</span> <span class="function">LSTMCell</span>:
    <span class="keyword">def</span> <span class="function">forward</span>(self, x_t, h_prev, C_prev):
        <span class="string">"""LSTM单元的前向传播"""</span>
        <span class="comment"># 拼接输入</span>
        concat = np.concatenate([h_prev, x_t], axis=0)

        <span class="comment"># 计算三个门</span>
        f_t = sigmoid(np.dot(self.W_f, concat) + self.b_f)  <span class="comment"># 遗忘门</span>
        i_t = sigmoid(np.dot(self.W_i, concat) + self.b_i)  <span class="comment"># 输入门</span>
        o_t = sigmoid(np.dot(self.W_o, concat) + self.b_o)  <span class="comment"># 输出门</span>

        <span class="comment"># 候选细胞状态</span>
        C_tilde = np.tanh(np.dot(self.W_C, concat) + self.b_C)

        <span class="comment"># 更新细胞状态（核心！）</span>
        C_t = f_t * C_prev + i_t * C_tilde

        <span class="comment"># 计算输出</span>
        h_t = o_t * np.tanh(C_t)

        <span class="keyword">return</span> h_t, C_t</pre>
                    </div>

                    <div class="think-box mt-3">
                        <h4>🤔 代码背后的深层思考</h4>
                        <ul>
                            <li><strong>为什么细胞状态更新是加法？</strong>这创建了梯度的"高速公路"</li>
                            <li><strong>为什么门使用sigmoid？</strong>输出[0,1]正好作为"开关"程度</li>
                            <li><strong>如果把乘法改成加法会怎样？</strong>失去选择性，所有信息都被保留</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="deep-think-box mt-4">
                <h4>LSTM解决梯度消失的秘密</h4>
                <div class="think-item">
                    <h5>🛣️ 梯度高速公路</h5>
                    <p>关键创新：$C_t = f_t * C_{t-1} + ...$</p>
                    <ul>
                        <li>当$f_t ≈ 1$时，梯度可以无损传递</li>
                        <li>这是一个<strong>线性</strong>传递路径</li>
                        <li>类似ResNet的残差连接思想</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🎭 选择性记忆的智慧</h5>
                    <p>不是所有信息都值得长期保存：</p>
                    <ul>
                        <li>重要信息：$f_t ≈ 1$，长期保持</li>
                        <li>临时信息：$f_t ≈ 0$，快速遗忘</li>
                        <li>网络学会了什么值得记住！</li>
                    </ul>
                </div>
            </div>

            <div class="tip success mt-4">
                <span class="tip-icon">✨</span>
                <strong>LSTM的魔力</strong>
                <p class="mt-2">
                    细胞状态$C_t$就像一条"高速公路"，信息可以几乎无损地在上面流动。
                    这解决了梯度消失问题，让网络能够记住几百个时间步之前的信息！
                </p>
            </div>
        </section>

        <!-- GRU -->
        <section id="gru" class="section-card">
            <h2>🚀 GRU：门控循环单元</h2>

            <div class="story-card">
                <span class="story-icon">🎯</span>
                <p><strong>LSTM的精简版</strong></p>
                <p class="mt-2">
                    如果说LSTM是功能齐全的豪华轿车，那么GRU就是轻便实用的跑车。
                    它只有两个门，但性能几乎一样好！
                </p>
            </div>

            <div class="deep-think-box">
                <h4>GRU的设计理念：少即是多</h4>
                <div class="think-item">
                    <h5>🎨 优雅的简化</h5>
                    <p>GRU的创新在于发现了LSTM的冗余：</p>
                    <ul>
                        <li>细胞状态和隐藏状态可以合并</li>
                        <li>遗忘门和输入门高度相关（忘记多少≈记住多少）</li>
                        <li>用一个更新门就能同时控制两者</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>⚖️ 权衡的艺术</h5>
                    <p>简化带来的好处与代价：</p>
                    <ul>
                        <li><strong>好处：</strong>参数更少，训练更快，不易过拟合</li>
                        <li><strong>代价：</strong>表达能力略有下降</li>
                        <li><strong>适用场景：</strong>数据量较小或需要快速训练时</li>
                    </ul>
                </div>
            </div>

            <div class="comparison-table">
                <table>
                    <thead>
                    <tr>
                        <th>特性</th>
                        <th>LSTM</th>
                        <th>GRU</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>门的数量</td>
                        <td>3个（遗忘、输入、输出）</td>
                        <td>2个（重置、更新）</td>
                    </tr>
                    <tr>
                        <td>记忆机制</td>
                        <td>分离的细胞状态和隐藏状态</td>
                        <td>统一的隐藏状态</td>
                    </tr>
                    <tr>
                        <td>参数数量</td>
                        <td>较多</td>
                        <td>较少（约LSTM的75%）</td>
                    </tr>
                    <tr>
                        <td>训练速度</td>
                        <td>较慢</td>
                        <td>较快</td>
                    </tr>
                    <tr>
                        <td>性能表现</td>
                        <td>长序列略优</td>
                        <td>短序列相当</td>
                    </tr>
                    </tbody>
                </table>
            </div>

            <div class="math-display mt-4">
                <h4 class="mb-3" style="color: var(--primary-light);">GRU的数学公式</h4>

                <p class="math-formula">
                    $z_t = \sigma(W_z \cdot [h_{t-1}, x_t])$
                    $r_t = \sigma(W_r \cdot [h_{t-1}, x_t])$
                    $\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t])$
                    $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$
                </p>

                <div class="formula-explanation">
                    <h5>📖 GRU的简洁之美</h5>

                    <table class="formula-table">
                        <tr>
                            <td class="formula-part">更新门 $z_t$</td>
                            <td class="formula-meaning">决定保留多少历史信息（类似LSTM的遗忘门+输入门）</td>
                        </tr>
                        <tr>
                            <td class="formula-part">重置门 $r_t$</td>
                            <td class="formula-meaning">决定忽略多少历史信息来计算候选状态</td>
                        </tr>
                        <tr>
                            <td class="formula-part">$h_t$ 更新</td>
                            <td class="formula-meaning">线性插值：旧信息×(1-z) + 新信息×z</td>
                        </tr>
                    </table>

                    <p class="mt-3"><strong>💡 核心创新：</strong>用更新门$z_t$同时控制遗忘和输入，简化了结构但保持了性能。</p>

                    <div class="deep-think-box mt-3">
                        <h4>理解GRU的精妙设计</h4>
                        <div class="think-item">
                            <h5>🔄 更新门的双重作用</h5>
                            <p>观察更新公式：$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$</p>
                            <ul>
                                <li>当$z_t = 0$：完全保留历史，忽略新输入</li>
                                <li>当$z_t = 1$：完全更新，忘记历史</li>
                                <li>当$z_t = 0.5$：历史和新信息各占一半</li>
                            </ul>
                            <p class="mt-2"><strong>巧妙之处：</strong>用一个门实现了LSTM两个门的功能！</p>
                        </div>
                        <div class="think-item">
                            <h5>🎭 重置门的作用</h5>
                            <p>为什么需要重置门？</p>
                            <ul>
                                <li>允许网络"重新开始"</li>
                                <li>在计算候选状态时选择性忽略历史</li>
                                <li>特别适合处理不相关的序列片段</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>实验：LSTM vs GRU</h4>
                <p><strong>对比实验设置：</strong></p>
                <ol>
                    <li>任务：情感分析（判断评论是正面还是负面）</li>
                    <li>数据集：1000条电影评论</li>
                    <li>对比指标：
                        <ul>
                            <li>训练时间</li>
                            <li>参数数量</li>
                            <li>最终准确率</li>
                            <li>对长句子的处理能力</li>
                        </ul>
                    </li>
                </ol>
                <p class="mt-3"><strong>预期结果：</strong></p>
                <ul>
                    <li>GRU训练速度快约25%</li>
                    <li>准确率相差不到1%</li>
                    <li>LSTM在超长序列（>100词）上略优</li>
                </ul>
            </div>

            <div class="think-box mt-4">
                <h4>🤔 选择建议</h4>
                <ul>
                    <li><strong>选择GRU：</strong>数据量小、需要快速原型、序列较短</li>
                    <li><strong>选择LSTM：</strong>数据量大、序列很长、需要最佳性能</li>
                    <li><strong>都试试：</strong>实践中，性能差异往往很小，选择更多取决于具体任务</li>
                </ul>
            </div>
        </section>

        <!-- 文本生成 -->
        <section id="text-generation" class="section-card">
            <h2>✍️ 应用：文本生成</h2>

            <div class="story-card">
                <span class="story-icon">🎨</span>
                <p><strong>让AI写诗</strong></p>
                <p class="mt-2">
                    训练RNN生成文本就像教小孩说话：先让它大量阅读，然后它就能模仿着创作了。
                </p>
            </div>

            <div class="deep-think-box">
                <h4>文本生成的哲学思考</h4>
                <div class="think-item">
                    <h5>🎭 创造还是模仿？</h5>
                    <p>RNN生成的文本是真正的"创作"吗？</p>
                    <ul>
                        <li>学习了语言的统计规律</li>
                        <li>能组合出训练集中没有的句子</li>
                        <li>但本质上是基于概率的采样</li>
                    </ul>
                    <p class="mt-2"><strong>思考：</strong>人类的创作是否也是基于过去经验的重组？</p>
                </div>
                <div class="think-item">
                    <h5>🌡️ Temperature的哲学</h5>
                    <p>温度参数控制着"创造力"：</p>
                    <ul>
                        <li>低温：保守、可预测、"正确"</li>
                        <li>高温：冒险、新颖、可能"错误"</li>
                        <li>这是否反映了人类创作中理性与感性的平衡？</li>
                    </ul>
                </div>
            </div>

            <div class="demo-container">
                <h3 class="mb-3">🎮 交互式文本生成器</h3>

                <input type="text" id="seed-text" class="demo-input"
                       placeholder="输入开头，如：春天来了" value="春天来了">

                <div class="mt-3">
                    <label style="color: var(--text-secondary);">创造力（Temperature）：</label>
                    <input type="range" min="0.1" max="2.0" step="0.1" value="1.0"
                           style="width: 100%; margin: 0.5rem 0;">
                    <div style="display: flex; justify-content: space-between; color: var(--text-muted); font-size: 0.875rem;">
                        <span>保守</span>
                        <span>平衡</span>
                        <span>创新</span>
                    </div>
                </div>

                <button class="demo-button mt-3" onclick="generateText()">
                    生成文本
                </button>

                <div id="generation-result" class="demo-result">
                    <p class="text-muted">生成的文本将显示在这里...</p>
                </div>
            </div>

            <div class="code-block mt-4">
                <div class="code-header">
                    <span class="code-lang">Python - 字符级别的文本生成</span>
                    <div class="code-actions">
                        <button class="code-btn" onclick="toggleCode(this)">展开</button>
                        <button class="code-btn" onclick="copyCode(this)">复制</button>
                    </div>
                </div>
                <div class="code-content collapsed">
                    <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="keyword">class</span> <span class="function">CharRNN</span>:
    <span class="keyword">def</span> <span class="function">__init__</span>(self, vocab_size, hidden_size=128):
        <span class="string">"""字符级RNN文本生成器"""</span>
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size

        <span class="comment"># 初始化参数</span>
        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01
        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01
        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01
        self.bh = np.zeros((hidden_size, 1))
        self.by = np.zeros((vocab_size, 1))

    <span class="keyword">def</span> <span class="function">sample</span>(self, seed_idx, n_chars=100, temperature=1.0):
        <span class="string">"""
        生成文本
        seed_idx: 种子字符的索引
        n_chars: 生成字符数
        temperature: 控制随机性（0=确定性，1=正常，>1=更随机）
        """</span>
        h = np.zeros((self.hidden_size, 1))
        x = np.zeros((self.vocab_size, 1))
        x[seed_idx] = 1

        generated_idxs = []

        <span class="keyword">for</span> t <span class="keyword">in</span> range(n_chars):
            <span class="comment"># 前向传播</span>
            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)
            y = np.dot(self.Why, h) + self.by

            <span class="comment"># 应用温度</span>
            p = np.exp(y / temperature) / np.sum(np.exp(y / temperature))

            <span class="comment"># 采样下一个字符</span>
            ix = np.random.choice(range(self.vocab_size), p=p.ravel())

            <span class="comment"># 准备下一步的输入</span>
            x = np.zeros((self.vocab_size, 1))
            x[ix] = 1

            generated_idxs.append(ix)

        <span class="keyword">return</span> generated_idxs

<span class="comment"># 训练循环（简化版）</span>
<span class="keyword">def</span> <span class="function">train</span>(model, data, epochs=100, seq_length=25):
    <span class="string">"""训练文本生成模型"""</span>
    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
        <span class="comment"># 准备训练数据</span>
        <span class="keyword">for</span> i <span class="keyword">in</span> range(0, len(data) - seq_length, seq_length):
            inputs = data[i:i+seq_length]
            targets = data[i+1:i+seq_length+1]

            <span class="comment"># 前向传播</span>
            loss = 0
            h = np.zeros((model.hidden_size, 1))

            <span class="keyword">for</span> t <span class="keyword">in</span> range(len(inputs)):
                <span class="comment"># 将字符转为one-hot</span>
                x = np.zeros((model.vocab_size, 1))
                x[inputs[t]] = 1

                <span class="comment"># RNN前向传播</span>
                h = np.tanh(np.dot(model.Wxh, x) +
                           np.dot(model.Whh, h) + model.bh)
                y = np.dot(model.Why, h) + model.by

                <span class="comment"># 计算损失</span>
                p = np.exp(y) / np.sum(np.exp(y))
                loss += -np.log(p[targets[t], 0])

            <span class="comment"># 反向传播和参数更新...</span>

        <span class="keyword">if</span> epoch % 10 == 0:
            print(<span class="string">f"Epoch {epoch}, Loss: {loss/seq_length:.4f}"</span>)

<span class="comment"># 使用示例</span>
text = <span class="string">"春天来了，花儿开了，鸟儿唱歌了。夏天来了，天气热了，知了叫了。"</span>
chars = list(set(text))
char_to_idx = {ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(chars)}
idx_to_char = {i: ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(chars)}

<span class="comment"># 创建模型</span>
model = CharRNN(vocab_size=len(chars))

<span class="comment"># 生成文本</span>
seed = <span class="string">"春"</span>
generated_idxs = model.sample(char_to_idx[seed], n_chars=50)
generated_text = <span class="string">''</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> generated_idxs])
print(<span class="string">f"生成的文本: {seed}{generated_text}"</span>)</pre>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>深入实验：探索生成质量</h4>
                <p><strong>实验1：训练数据的影响</strong></p>
                <ul>
                    <li>用不同风格的文本训练（诗歌、小说、新闻）</li>
                    <li>观察生成文本的风格变化</li>
                    <li>思考：模型真的"理解"了风格吗？</li>
                </ul>
                <p class="mt-3"><strong>实验2：序列长度的影响</strong></p>
                <ul>
                    <li>比较字符级、词级、句子级的生成</li>
                    <li>哪个级别生成的文本更连贯？</li>
                    <li>权衡：粒度vs连贯性</li>
                </ul>
            </div>

            <div class="think-box mt-4">
                <h4>🤔 思考：Temperature参数的作用</h4>
                <ul>
                    <li>Temperature = 0.5：保守，生成常见的组合</li>
                    <li>Temperature = 1.0：平衡，正常的创造性</li>
                    <li>Temperature = 2.0：冒险，可能产生新奇但奇怪的结果</li>
                </ul>
                <p class="mt-3"><strong>深层问题：</strong>这是否类似于人类创作时的"灵感"状态？</p>
            </div>
        </section>

        <!-- 情感分析 -->
        <section id="sentiment-analysis" class="section-card">
            <h2>😊 应用：情感分析</h2>

            <div class="story-card">
                <span class="story-icon">🎭</span>
                <p><strong>理解文字背后的情绪</strong></p>
                <p class="mt-2">
                    RNN不仅能生成文本，还能理解文本的情感。这在商业评论分析、社交媒体监控等场景非常有用。
                </p>
            </div>

            <div class="deep-think-box">
                <h4>情感分析的深层挑战</h4>
                <div class="think-item">
                    <h5>🎭 情感的复杂性</h5>
                    <p>人类情感远比"正面/负面"复杂：</p>
                    <ul>
                        <li>讽刺："这服务真是'太好了'（反话）"</li>
                        <li>矛盾："爱恨交加"</li>
                        <li>文化差异：同样的表达在不同文化中含义不同</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🔍 上下文的重要性</h5>
                    <p>为什么RNN特别适合情感分析？</p>
                    <ul>
                        <li>"不"字的位置完全改变句子含义</li>
                        <li>转折词（但是、虽然）需要理解前后关系</li>
                        <li>情感可能在句子末尾才揭示</li>
                    </ul>
                </div>
            </div>

            <div class="demo-container">
                <h3 class="mb-3">情感分析示例</h3>

                <div class="card-grid">
                    <div class="info-card">
                        <p>"这部电影太棒了！演员演技精湛，剧情扣人心弦。"</p>
                        <div class="mt-2" style="text-align: center;">
                            <span style="font-size: 2rem;">😊</span>
                            <p style="color: var(--success);">积极情感: 95%</p>
                        </div>
                    </div>

                    <div class="info-card">
                        <p>"服务态度极差，等了一个小时都没人理。"</p>
                        <div class="mt-2" style="text-align: center;">
                            <span style="font-size: 2rem;">😠</span>
                            <p style="color: var(--danger);">消极情感: 88%</p>
                        </div>
                    </div>

                    <div class="info-card">
                        <p>"还行吧，没有特别出彩的地方。"</p>
                        <div class="mt-2" style="text-align: center;">
                            <span style="font-size: 2rem;">😐</span>
                            <p style="color: var(--warning);">中性情感: 72%</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="explore-box mt-4">
                <h4>挑战：识别复杂情感</h4>
                <p>试着分析这些句子的情感：</p>
                <ol>
                    <li>"我不是不喜欢这部电影"（双重否定）</li>
                    <li>"除了价格，其他都很完美"（部分否定）</li>
                    <li>"要是服务能像食物一样好就好了"（隐含批评）</li>
                    <li>"😊😊😊"（纯表情）</li>
                </ol>
                <p class="mt-3"><strong>思考：</strong>RNN如何处理这些复杂情况？需要什么样的训练数据？</p>
            </div>

            <div class="algorithm-steps mt-4">
                <h4>使用RNN进行情感分析的步骤</h4>

                <div class="step-item">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h5>文本预处理</h5>
                        <ul>
                            <li>分词：将句子分成词或字</li>
                            <li>转换为词向量</li>
                            <li>padding：统一序列长度</li>
                        </ul>
                        <div class="think-box mt-2">
                            <h4>🤔 预处理的艺术</h4>
                            <ul>
                                <li>是否保留标点符号？（！和。传达不同情感）</li>
                                <li>如何处理表情符号？</li>
                                <li>大小写是否重要？（"好"vs"好！！！"）</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h5>通过RNN编码</h5>
                        <ul>
                            <li>逐词输入RNN</li>
                            <li>最后的隐藏状态包含整句信息</li>
                            <li>或使用双向RNN获得更好效果</li>
                        </ul>
                        <div class="deep-think-box mt-2">
                            <h4>双向RNN的智慧</h4>
                            <p>为什么双向RNN效果更好？</p>
                            <ul>
                                <li>正向：理解"虽然...但是..."的转折</li>
                                <li>反向：提前知道句子的结论</li>
                                <li>结合：全局理解整句话的情感</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h5>分类输出</h5>
                        <ul>
                            <li>将最后的隐藏状态输入分类器</li>
                            <li>输出情感概率分布</li>
                            <li>选择概率最高的情感类别</li>
                        </ul>
                        <div class="explore-box mt-2">
                            <h4>扩展思考</h4>
                            <p>如果不只是二分类（正/负），而是：</p>
                            <ul>
                                <li>5级情感（非常负面→非常正面）</li>
                                <li>多维情感（快乐、悲伤、愤怒、惊讶...）</li>
                                <li>情感强度（0-10分）</li>
                            </ul>
                            <p class="mt-2">网络架构需要如何调整？</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- 更多应用 -->
        <section id="applications" class="section-card">
            <h2>🚀 更多RNN应用案例</h2>

            <div class="deep-think-box mb-4">
                <h4>RNN应用的共同特征</h4>
                <div class="think-item">
                    <h5>📊 序列数据的普遍性</h5>
                    <p>几乎所有涉及时间的数据都可以用RNN处理：</p>
                    <ul>
                        <li>自然序列：语言、音乐、视频</li>
                        <li>时间序列：股价、天气、传感器数据</li>
                        <li>行为序列：用户点击、购买历史</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🎯 记忆的价值</h5>
                    <p>什么时候RNN特别有用？</p>
                    <ul>
                        <li>当前状态依赖历史</li>
                        <li>存在长期模式</li>
                        <li>上下文信息关键</li>
                    </ul>
                </div>
            </div>

            <div class="card-grid">
                <div class="info-card">
                    <span class="card-icon">🎵</span>
                    <h3 class="card-title">音乐生成</h3>
                    <p>训练RNN学习音符序列，生成新的旋律</p>
                    <ul class="mt-2">
                        <li>学习音乐风格</li>
                        <li>生成和弦进行</li>
                        <li>创作完整乐曲</li>
                    </ul>
                    <div class="think-box mt-2">
                        <h4>🤔 思考</h4>
                        <p>音乐的哪些特征适合RNN学习？节奏、旋律、和声？</p>
                    </div>
                </div>

                <div class="info-card">
                    <span class="card-icon">🌐</span>
                    <h3 class="card-title">机器翻译</h3>
                    <p>Seq2Seq模型：编码器-解码器架构</p>
                    <ul class="mt-2">
                        <li>编码器：理解源语言</li>
                        <li>解码器：生成目标语言</li>
                        <li>注意力机制改进</li>
                    </ul>
                    <div class="explore-box mt-2">
                        <h4>挑战</h4>
                        <p>如何处理语序完全不同的语言对？</p>
                    </div>
                </div>

                <div class="info-card">
                    <span class="card-icon">🗣️</span>
                    <h3 class="card-title">语音识别</h3>
                    <p>将音频信号转换为文字</p>
                    <ul class="mt-2">
                        <li>处理可变长度音频</li>
                        <li>捕获时序依赖</li>
                        <li>端到端识别</li>
                    </ul>
                    <div class="think-box mt-2">
                        <h4>🤔 难点</h4>
                        <p>同音字、口音、背景噪声如何处理？</p>
                    </div>
                </div>

                <div class="info-card">
                    <span class="card-icon">📈</span>
                    <h3 class="card-title">股票预测</h3>
                    <p>分析历史价格预测未来走势</p>
                    <ul class="mt-2">
                        <li>时间序列建模</li>
                        <li>捕获市场模式</li>
                        <li style="color: var(--danger);">风险提示：仅供参考！</li>
                    </ul>
                    <div class="tip warning mt-2">
                        <span class="tip-icon">⚠️</span>
                        <p>市场受多种因素影响，纯技术分析有局限性</p>
                    </div>
                </div>

                <div class="info-card">
                    <span class="card-icon">🏥</span>
                    <h3 class="card-title">健康监测</h3>
                    <p>分析生理信号序列</p>
                    <ul class="mt-2">
                        <li>心电图异常检测</li>
                        <li>睡眠阶段分类</li>
                        <li>疾病早期预警</li>
                    </ul>
                    <div class="explore-box mt-2">
                        <h4>潜力</h4>
                        <p>结合可穿戴设备，实现实时健康监控</p>
                    </div>
                </div>

                <div class="info-card">
                    <span class="card-icon">🎮</span>
                    <h3 class="card-title">游戏AI</h3>
                    <p>学习玩家行为模式</p>
                    <ul class="mt-2">
                        <li>预测玩家动作</li>
                        <li>生成NPC对话</li>
                        <li>动态难度调整</li>
                    </ul>
                    <div class="think-box mt-2">
                        <h4>🤔 创新</h4>
                        <p>如何让NPC的行为更像真人？</p>
                    </div>
                </div>
            </div>

            <div class="deep-think-box mt-4">
                <h4>未来展望：RNN的局限与突破</h4>
                <div class="think-item">
                    <h5>🚧 当前局限</h5>
                    <ul>
                        <li>难以并行化（必须顺序处理）</li>
                        <li>长序列仍有挑战</li>
                        <li>计算效率不高</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🚀 新方向</h5>
                    <ul>
                        <li><strong>Transformer：</strong>注意力机制，可并行</li>
                        <li><strong>时间卷积：</strong>用CNN处理序列</li>
                        <li><strong>神经图灵机：</strong>可读写的外部记忆</li>
                    </ul>
                    <p class="mt-2"><strong>思考：</strong>这些新方法真的完全替代了RNN吗？</p>
                </div>
            </div>
        </section>

        <!-- 本章总结 -->
        <section id="summary" class="section-card">
            <h2>📚 本章总结</h2>

            <div class="story-card">
                <span class="story-icon">🎓</span>
                <p><strong>从记忆到理解</strong></p>
                <p class="mt-2">
                    我们从"我爱你"的简单例子开始，逐步深入到RNN的核心原理。
                    从基础RNN到LSTM/GRU，我们见证了如何让机器拥有"记忆"。
                </p>
            </div>

            <div class="card-grid mt-4">
                <div class="info-card">
                    <span class="card-icon">🧠</span>
                    <h3 class="card-title">核心概念</h3>
                    <ul>
                        <li><strong>隐藏状态：</strong>RNN的记忆机制</li>
                        <li><strong>BPTT：</strong>时间维度的反向传播</li>
                        <li><strong>梯度消失：</strong>长序列的挑战</li>
                        <li><strong>门控机制：</strong>LSTM/GRU的解决方案</li>
                    </ul>
                </div>

                <div class="info-card">
                    <span class="card-icon">🛠️</span>
                    <h3 class="card-title">实践要点</h3>
                    <ul>
                        <li>选择合适的RNN变体</li>
                        <li>注意梯度裁剪防止爆炸</li>
                        <li>使用双向RNN提升性能</li>
                        <li>合理设置序列长度</li>
                    </ul>
                </div>

                <div class="info-card">
                    <span class="card-icon">💡</span>
                    <h3 class="card-title">关键洞察</h3>
                    <ul>
                        <li>序列的本质是依赖关系</li>
                        <li>记忆需要选择性</li>
                        <li>简单不一定差（GRU vs LSTM）</li>
                        <li>应用决定架构</li>
                    </ul>
                </div>
            </div>

            <div class="deep-think-box mt-4">
                <h4>回顾与展望</h4>
                <div class="think-item">
                    <h5>🎯 我们学到了什么？</h5>
                    <ul>
                        <li>理解了序列数据的特殊性</li>
                        <li>掌握了RNN的工作原理</li>
                        <li>认识了梯度消失问题及解决方案</li>
                        <li>了解了RNN的实际应用</li>
                    </ul>
                </div>
                <div class="think-item">
                    <h5>🚀 下一步是什么？</h5>
                    <ul>
                        <li><strong>注意力机制：</strong>让模型知道"看哪里"</li>
                        <li><strong>Transformer：</strong>革命性的新架构</li>
                        <li><strong>BERT/GPT：</strong>预训练语言模型</li>
                        <li><strong>多模态：</strong>结合文本、图像、语音</li>
                    </ul>
                </div>
            </div>

            <div class="tip success mt-4">
                <span class="tip-icon">🎉</span>
                <strong>恭喜你！</strong>
                <p class="mt-2">
                    你已经掌握了RNN的核心概念。记住，深度学习的精髓不在于记住公式，
                    而在于理解背后的思想。继续保持好奇心，探索更多可能！
                </p>
            </div>
        </section>

        <!-- 练习题 -->
        <section id="exercises" class="section-card">
            <h2>📝 练习题</h2>

            <div class="algorithm-steps">
                <div class="step-item">
                    <div class="step-number">1</div>
                    <div class="step-content">
                        <h5>概念理解</h5>
                        <p>解释为什么RNN适合处理序列数据，而CNN更适合处理图像数据？</p>
                        <div class="think-box mt-2">
                            <h4>提示</h4>
                            <p>考虑数据的结构特征和网络的设计理念</p>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">2</div>
                    <div class="step-content">
                        <h5>计算练习</h5>
                        <p>给定一个2层的RNN，隐藏层维度为3，输入维度为2，手动计算第一个时间步的输出。</p>
                        <div class="code-block mt-2">
                            <div class="code-header">
                                <span class="code-lang">给定参数</span>
                            </div>
                            <div class="code-content">
                                <pre>W_xh = [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]
W_hh = [[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]]
x_0 = [1, 0]
h_{-1} = [0, 0, 0]</pre>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">3</div>
                    <div class="step-content">
                        <h5>编程实践</h5>
                        <p>实现一个简单的字符级RNN，用于生成人名。</p>
                        <ul>
                            <li>收集100个人名作为训练数据</li>
                            <li>实现前向传播和反向传播</li>
                            <li>训练模型并生成新的人名</li>
                        </ul>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">4</div>
                    <div class="step-content">
                        <h5>深度思考</h5>
                        <p>如果要处理1000个时间步的序列，你会选择什么架构？为什么？</p>
                        <div class="explore-box mt-2">
                            <h4>考虑因素</h4>
                            <ul>
                                <li>梯度消失问题</li>
                                <li>计算效率</li>
                                <li>内存消耗</li>
                                <li>并行化可能</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <div class="step-item">
                    <div class="step-number">5</div>
                    <div class="step-content">
                        <h5>创新应用</h5>
                        <p>设计一个使用RNN的新应用场景，并说明：</p>
                        <ul>
                            <li>为什么这个场景适合用RNN？</li>
                            <li>输入输出是什么？</li>
                            <li>可能遇到什么挑战？</li>
                            <li>如何评估模型效果？</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="tip info mt-4">
                <span class="tip-icon">💡</span>
                <strong>学习建议</strong>
                <p class="mt-2">
                    不要急于看答案，先自己思考和尝试。错误是最好的老师，
                    通过解决问题，你会对RNN有更深的理解。
                </p>
            </div>
        </section>
    </div>
</main>

<!-- JavaScript交互 -->
<script>
    // 侧边栏开关
    document.getElementById('toggle-sidebar').addEventListener('click', function() {
        document.getElementById('sidebar').classList.toggle('open');
    });

    // 进度条更新
    window.addEventListener('scroll', function() {
        const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
        const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        const scrolled = (winScroll / height) * 100;
        document.getElementById('progress-bar').style.width = scrolled + '%';
    });

    // 代码块展开/折叠
    function toggleCode(btn) {
        const codeContent = btn.closest('.code-block').querySelector('.code-content');
        codeContent.classList.toggle('collapsed');
        btn.textContent = codeContent.classList.contains('collapsed') ? '展开' : '折叠';
    }

    // 复制代码
    function copyCode(btn) {
        const code = btn.closest('.code-block').querySelector('pre').textContent;
        navigator.clipboard.writeText(code).then(() => {
            const originalText = btn.textContent;
            btn.textContent = '已复制！';
            setTimeout(() => btn.textContent = originalText, 2000);
        });
    }

    // 文本生成演示
    function generateText() {
        const seedText = document.getElementById('seed-text').value;
        const resultDiv = document.getElementById('generation-result');

        // 模拟生成过程
        resultDiv.innerHTML = '<p class="text-muted">正在生成...</p>';

        setTimeout(() => {
            // 模拟生成的文本
            const generated = seedText + '，万物复苏，大地回春。柳树发芽了，小草探出了头，整个世界充满了生机与活力。';
            resultDiv.innerHTML = `
                <p><strong>生成的文本：</strong></p>
                <p style="font-style: italic; color: var(--primary-light);">${generated}</p>
                <p class="text-muted mt-2">（这是模拟的生成结果，实际效果取决于模型训练）</p>
            `;
        }, 1500);
    }

    // 快速导航高亮
    const sections = document.querySelectorAll('section[id]');
    const navItems = document.querySelectorAll('.quick-nav-item');

    window.addEventListener('scroll', () => {
        let current = '';
        sections.forEach(section => {
            const sectionTop = section.offsetTop;
            const sectionHeight = section.clientHeight;
            if (pageYOffset >= sectionTop - 200) {
                current = section.getAttribute('id');
            }
        });

        navItems.forEach(item => {
            item.classList.remove('active');
            if (item.dataset.section === current) {
                item.classList.add('active');
            }
        });
    });
</script>

</body>
</html>