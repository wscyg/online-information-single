<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬11ç« ï¼šTransformerå®Œæ•´å®ç° - ä»é›¶åˆ°ä¸€æ„å»ºæ”¹å˜ä¸–ç•Œçš„æ¨¡å‹</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&display=swap');

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
            color: #e2e8f0;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        /* åŠ¨ç”»æ•ˆæœ */
        @keyframes code-appear {
            0% {
                opacity: 0;
                transform: translateY(20px);
            }
            100% {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes flow-connection {
            0% {
                stroke-dashoffset: 100;
            }
            100% {
                stroke-dashoffset: 0;
            }
        }

        @keyframes pulse-glow {
            0%, 100% {
                box-shadow: 0 0 20px rgba(59, 130, 246, 0.5);
            }
            50% {
                box-shadow: 0 0 40px rgba(59, 130, 246, 0.8);
            }
        }

        @keyframes typing {
            from {
                width: 0;
            }
            to {
                width: 100%;
            }
        }

        /* å®¹å™¨æ ·å¼ */
        .chapter-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* ç« èŠ‚å¤´éƒ¨ */
        .chapter-header {
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(147, 51, 234, 0.1));
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
            border: 1px solid rgba(59, 130, 246, 0.3);
            text-align: center;
        }

        .chapter-header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(59, 130, 246, 0.1) 0%, transparent 70%);
            animation: pulse-glow 4s ease-in-out infinite;
        }

        .chapter-header h1 {
            font-size: 2.8rem;
            margin-bottom: 1.5rem;
            background: linear-gradient(45deg, #3b82f6, #8b5cf6, #9333ea);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            position: relative;
            z-index: 2;
        }

        .chapter-header p {
            font-size: 1.3rem;
            color: #cbd5e1;
            margin-bottom: 2rem;
            position: relative;
            z-index: 2;
        }

        .meta-tags {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            position: relative;
            z-index: 2;
        }

        .meta-tag {
            background: rgba(59, 130, 246, 0.2);
            color: #3b82f6;
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            font-weight: bold;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            border: 1px solid rgba(59, 130, 246, 0.4);
        }

        /* ä¸»è¦åŒºå—æ ·å¼ */
        .implementation-section {
            background: linear-gradient(135deg, rgba(15, 23, 42, 0.8), rgba(30, 41, 59, 0.6));
            border-radius: 20px;
            padding: 2.5rem;
            margin: 3rem 0;
            border: 2px solid rgba(59, 130, 246, 0.2);
            position: relative;
            overflow: hidden;
            box-shadow: 0 16px 64px rgba(0, 0, 0, 0.3);
        }

        /* æ¶æ„æ€»è§ˆæ ·å¼ */
        .architecture-overview {
            background: rgba(15, 23, 42, 0.9);
            border-radius: 16px;
            padding: 2rem;
            margin: 2rem 0;
            border: 1px solid rgba(59, 130, 246, 0.3);
        }

        .architecture-flow {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
            align-items: center;
            margin: 2rem 0;
        }

        .component-card {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 12px;
            padding: 1.5rem;
            width: 100%;
            max-width: 600px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            transition: all 0.3s ease;
        }

        .component-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
            border-color: rgba(59, 130, 246, 0.4);
        }

        /* ä»£ç å®ç°æ ·å¼ */
        .code-implementation {
            background: rgba(15, 23, 42, 0.95);
            border: 1px solid rgba(147, 51, 234, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            font-family: 'JetBrains Mono', monospace;
            overflow-x: auto;
            position: relative;
        }

        .code-implementation pre {
            margin: 0;
            color: #e2e8f0;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .code-title {
            color: #3b82f6;
            font-weight: bold;
            font-size: 1.1rem;
        }

        .code-filename {
            color: #64748b;
            font-size: 0.9rem;
            background: rgba(255, 255, 255, 0.05);
            padding: 0.3rem 0.8rem;
            border-radius: 6px;
        }

        /* ä»£ç é«˜äº® */
        .code-keyword {
            color: #9333ea;
            font-weight: bold;
        }

        .code-function {
            color: #3b82f6;
            font-weight: bold;
        }

        .code-string {
            color: #fbbf24;
        }

        .code-comment {
            color: #64748b;
            font-style: italic;
        }

        .code-number {
            color: #22c55e;
        }

        .code-class {
            color: #06b6d4;
            font-weight: bold;
        }

        /* æ­¥éª¤æ ·å¼ */
        .implementation-step {
            background: rgba(15, 23, 42, 0.9);
            border: 1px solid rgba(59, 130, 246, 0.3);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
            position: relative;
        }

        .step-header {
            display: flex;
            align-items: center;
            margin-bottom: 1.5rem;
        }

        .step-number {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
            background: linear-gradient(45deg, #3b82f6, #8b5cf6);
            color: white;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 1rem;
            font-size: 1.2rem;
        }

        .step-title {
            color: #3b82f6;
            font-weight: bold;
            font-size: 1.3rem;
        }

        /* æ¶æ„å›¾æ ·å¼ */
        .transformer-diagram {
            background: rgba(15, 23, 42, 0.95);
            border-radius: 16px;
            padding: 2rem;
            margin: 2rem 0;
            border: 1px solid rgba(59, 130, 246, 0.3);
            text-align: center;
        }

        .diagram-container {
            display: inline-block;
            position: relative;
            margin: 2rem auto;
        }

        /* çŸ¥è¯†ç‚¹å¡ç‰‡ */
        .insight-card {
            background: linear-gradient(135deg, rgba(251, 191, 36, 0.1), rgba(245, 158, 11, 0.05));
            border: 1px solid rgba(251, 191, 36, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .insight-card h4 {
            color: #fbbf24;
            margin-bottom: 0.8rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* æµ‹è¯•ä»£ç æ ·å¼ */
        .test-section {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .test-output {
            background: rgba(0, 0, 0, 0.3);
            padding: 1rem;
            border-radius: 8px;
            margin-top: 1rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            color: #22c55e;
        }

        /* å“åº”å¼è®¾è®¡ */
        @media (max-width: 768px) {
            .chapter-container {
                padding: 1rem;
            }

            .chapter-header h1 {
                font-size: 2rem;
            }

            .implementation-section {
                padding: 1.5rem;
            }

            .code-implementation {
                padding: 1rem;
                font-size: 0.8rem;
            }
        }
    </style>
</head>
<body>

<div class="chapter-container">
    <!-- ç« èŠ‚å¤´éƒ¨ -->
    <div class="chapter-header">
        <h1>ç¬¬10ç« ï¼šTransformerå®Œæ•´å®ç°</h1>
        <p>å°†æ‰€æœ‰æ‹¼å›¾ç»„åˆï¼Œæ„å»ºæ”¹å˜ä¸–ç•Œçš„æ¨¡å‹</p>
        <div class="meta-tags">
            <span class="meta-tag">
                ğŸ—ï¸ <span>å®Œæ•´å®ç°</span>
            </span>
            <span class="meta-tag">
                â±ï¸ <span>90åˆ†é’Ÿ</span>
            </span>
            <span class="meta-tag">
                ğŸ’» <span>å®æˆ˜ä»£ç </span>
            </span>
            <span class="meta-tag">
                ğŸ¯ <span>æ¶æ„ç†è§£</span>
            </span>
        </div>
    </div>

    <!-- ğŸŒŸ å¼€ç¯‡ï¼šå›é¡¾ä¸å±•æœ› -->
    <div class="implementation-section" style="border-color: #3b82f6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(147, 51, 234, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(59, 130, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸŒŸ</span>
                <h2 style="color: #3b82f6; margin: 0; font-size: 1.8rem; font-weight: bold;">ç»ˆäºåˆ°äº†æ¿€åŠ¨äººå¿ƒçš„æ—¶åˆ»ï¼</h2>
            </div>
        </div>

        <!-- æ—…ç¨‹å›é¡¾ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">ğŸ“š æˆ‘ä»¬çš„å­¦ä¹ æ—…ç¨‹</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                    ç»è¿‡å‰9ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»æ·±å…¥ç†è§£äº†ï¼š<br><br>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1rem;">
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                            <strong style="color: #22c55e;">ç¬¬1-3ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶</strong><br>
                            ä»åŸºç¡€è‡ªæ³¨æ„åŠ›åˆ°å¤šå¤´æ³¨æ„åŠ›çš„æ¼”è¿›
                        </div>
                        <div style="background: rgba(59, 130, 246, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #3b82f6;">
                            <strong style="color: #3b82f6;">ç¬¬4-5ç« ï¼šè®­ç»ƒæŠ€å·§</strong><br>
                            æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–çš„å…³é”®ä½œç”¨
                        </div>
                        <div style="background: rgba(139, 92, 246, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #8b5cf6;">
                            <strong style="color: #8b5cf6;">ç¬¬6-7ç« ï¼šå‰é¦ˆç½‘ç»œ</strong><br>
                            éçº¿æ€§å˜æ¢å’Œè¡¨è¾¾èƒ½åŠ›
                        </div>
                        <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #fbbf24;">
                            <strong style="color: #fbbf24;">ç¬¬8-9ç« ï¼šæœ€åæ‹¼å›¾</strong><br>
                            ä½ç½®ç¼–ç å’Œæ©ç æœºåˆ¶
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- æœ¬ç« ç›®æ ‡ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">ğŸ¯ æœ¬ç« ç›®æ ‡</div>

            <div style="display: grid; gap: 1rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #3b82f6; font-weight: bold; margin-bottom: 0.5rem;">1. ç†è§£æ¶æ„è®¾è®¡å“²å­¦</div>
                    <div style="color: #cbd5e1;">ä¸ºä»€ä¹ˆTransformerè¦è¿™æ ·è®¾è®¡ï¼Ÿæ¯ä¸ªç»„ä»¶å¦‚ä½•ååŒå·¥ä½œï¼Ÿ</div>
                </div>
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">2. æ„å»ºå®Œæ•´å®ç°</div>
                    <div style="color: #cbd5e1;">ä»é›¶å¼€å§‹ï¼Œä¸€æ­¥æ­¥æ„å»ºå¯è¿è¡Œçš„Transformeræ¨¡å‹</div>
                </div>
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">3. å®æˆ˜æµ‹è¯•</div>
                    <div style="color: #cbd5e1;">ç”¨å®é™…ä¾‹å­éªŒè¯æˆ‘ä»¬çš„å®ç°ï¼Œçœ‹åˆ°æ¨¡å‹çœŸæ­£å·¥ä½œèµ·æ¥</div>
                </div>
            </div>
        </div>
    </div>

    <!-- ğŸ—ï¸ ç†è§£æ¶æ„ï¼šä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ -->
    <div class="implementation-section" style="border-color: #8b5cf6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸ—ï¸</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">ç†è§£æ¶æ„ï¼šä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ</h2>
            </div>
        </div>

        <!-- è®¾è®¡å“²å­¦ -->
        <div class="insight-card">
            <h4><span>ğŸ’¡</span> Transformerçš„è®¾è®¡å“²å­¦</h4>
            <div style="color: #cbd5e1; line-height: 1.8;">
                <strong>1. å¹¶è¡ŒåŒ–ä¼˜å…ˆï¼š</strong>æŠ›å¼ƒäº†RNNçš„åºåˆ—å¤„ç†ï¼Œé‡‡ç”¨å®Œå…¨å¹¶è¡Œçš„æ³¨æ„åŠ›æœºåˆ¶<br>
                <strong>2. æ¨¡å—åŒ–è®¾è®¡ï¼š</strong>æ¯ä¸ªç»„ä»¶ç‹¬ç«‹ä¸”å¯å¤ç”¨ï¼Œä¾¿äºå †å å’Œæ‰©å±•<br>
                <strong>3. ä¿¡æ¯æµåŠ¨ï¼š</strong>é€šè¿‡æ®‹å·®è¿æ¥ç¡®ä¿ä¿¡æ¯åœ¨æ·±å±‚ç½‘ç»œä¸­æµåŠ¨<br>
                <strong>4. è¡¨è¾¾èƒ½åŠ›ï¼š</strong>å¤šå¤´æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œæä¾›å¼ºå¤§çš„è¡¨ç¤ºå­¦ä¹ èƒ½åŠ›
            </div>
        </div>

        <!-- æ¶æ„æ€»è§ˆå›¾ -->
        <div class="transformer-diagram">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem;">ğŸ” Transformeræ¶æ„å…¨æ™¯å›¾</div>

            <div class="architecture-flow">
                <!-- è¾“å…¥å¤„ç† -->
                <div class="component-card" style="border-color: rgba(251, 191, 36, 0.4); background: rgba(251, 191, 36, 0.05);">
                    <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.8rem;">ğŸ“¥ è¾“å…¥å¤„ç†å±‚</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ Token Embeddingï¼šå°†è¯è½¬æ¢ä¸ºå‘é‡<br>
                        â€¢ Positional Encodingï¼šæ·»åŠ ä½ç½®ä¿¡æ¯<br>
                        â€¢ ä½œç”¨ï¼šè®©æ¨¡å‹ç†è§£"æ˜¯ä»€ä¹ˆ"å’Œ"åœ¨å“ªé‡Œ"
                    </div>
                </div>

                <div style="color: #64748b; font-size: 2rem; margin: 0.5rem;">â†“</div>

                <!-- ç¼–ç å™¨ -->
                <div class="component-card" style="border-color: rgba(34, 197, 94, 0.4); background: rgba(34, 197, 94, 0.05);">
                    <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.8rem;">ğŸ”„ ç¼–ç å™¨å±‚ Ã— N</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ Multi-Head Attentionï¼šå…¨å±€ä¿¡æ¯äº¤äº’<br>
                        â€¢ Feed Forwardï¼šéçº¿æ€§å˜æ¢<br>
                        â€¢ æ¯å±‚éƒ½æœ‰ï¼šæ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–<br>
                        â€¢ ä½œç”¨ï¼šæ·±åº¦ç†è§£è¾“å…¥å†…å®¹
                    </div>
                </div>

                <div style="color: #64748b; font-size: 2rem; margin: 0.5rem;">â†“</div>

                <!-- è§£ç å™¨ -->
                <div class="component-card" style="border-color: rgba(59, 130, 246, 0.4); background: rgba(59, 130, 246, 0.05);">
                    <div style="color: #3b82f6; font-weight: bold; margin-bottom: 0.8rem;">ğŸ¯ è§£ç å™¨å±‚ Ã— N</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ Masked Multi-Head Attentionï¼šè‡ªå›å½’ç”Ÿæˆ<br>
                        â€¢ Cross Attentionï¼šå…³æ³¨ç¼–ç å™¨è¾“å‡º<br>
                        â€¢ Feed Forwardï¼šéçº¿æ€§å˜æ¢<br>
                        â€¢ ä½œç”¨ï¼šåŸºäºç†è§£ç”Ÿæˆè¾“å‡º
                    </div>
                </div>

                <div style="color: #64748b; font-size: 2rem; margin: 0.5rem;">â†“</div>

                <!-- è¾“å‡ºå¤„ç† -->
                <div class="component-card" style="border-color: rgba(139, 92, 246, 0.4); background: rgba(139, 92, 246, 0.05);">
                    <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.8rem;">ğŸ“¤ è¾“å‡ºå¤„ç†å±‚</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ Linear Projectionï¼šæ˜ å°„åˆ°è¯è¡¨å¤§å°<br>
                        â€¢ Softmaxï¼šè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ<br>
                        â€¢ ä½œç”¨ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
                    </div>
                </div>
            </div>
        </div>

        <!-- å…³é”®è®¾è®¡å†³ç­– -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-top: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">ğŸ¨ å…³é”®è®¾è®¡å†³ç­–</div>

            <div style="display: grid; gap: 1rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #3b82f6; font-weight: bold; margin-bottom: 0.5rem;">ä¸ºä»€ä¹ˆç”¨è‡ªæ³¨æ„åŠ›è€Œä¸æ˜¯CNN/RNNï¼Ÿ</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ <strong>å¹¶è¡Œè®¡ç®—ï¼š</strong>æ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼Œè®­ç»ƒé€Ÿåº¦å¿«<br>
                        â€¢ <strong>é•¿è·ç¦»ä¾èµ–ï¼š</strong>ä»»æ„ä¸¤ä¸ªä½ç½®ç›´æ¥äº¤äº’ï¼Œä¸å—è·ç¦»é™åˆ¶<br>
                        â€¢ <strong>è®¡ç®—å¤æ‚åº¦ï¼š</strong>O(nÂ²)ä½†å®é™…æ›´é«˜æ•ˆ
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ <strong>å¤šè§’åº¦ç†è§£ï¼š</strong>ä¸åŒçš„å¤´å…³æ³¨ä¸åŒçš„æ¨¡å¼<br>
                        â€¢ <strong>è¡¨è¾¾èƒ½åŠ›ï¼š</strong>ç±»ä¼¼CNNçš„å¤šé€šé“ï¼Œå¢å¼ºè¡¨ç¤ºèƒ½åŠ›<br>
                        â€¢ <strong>ç¨³å®šè®­ç»ƒï¼š</strong>é™ä½å•ä¸€æ³¨æ„åŠ›çš„éšæœºæ€§
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">ä¸ºä»€ä¹ˆè¦å †å å¤šå±‚ï¼Ÿ</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        â€¢ <strong>å±‚æ¬¡åŒ–ç‰¹å¾ï¼š</strong>åº•å±‚å­¦ä¹ å±€éƒ¨æ¨¡å¼ï¼Œé«˜å±‚å­¦ä¹ å…¨å±€è¯­ä¹‰<br>
                        â€¢ <strong>ç»„åˆèƒ½åŠ›ï¼š</strong>é€šè¿‡æ·±åº¦å®ç°å¤æ‚çš„ç‰¹å¾ç»„åˆ<br>
                        â€¢ <strong>å®è·µè¯æ˜ï¼š</strong>6-12å±‚åœ¨å¤šæ•°ä»»åŠ¡ä¸Šæ•ˆæœæœ€ä½³
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- ğŸ’» Step 1: åŸºç¡€ç»„ä»¶å®ç° -->
    <div class="implementation-section" style="border-color: #22c55e;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">ğŸ’»</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">å¼€å§‹å®ç°ï¼šåŸºç¡€ç»„ä»¶</h2>
            </div>
        </div>

        <!-- Step 1.1: å¯¼å…¥å’Œé…ç½® -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">1</div>
                <div class="step-title">ç¯å¢ƒå‡†å¤‡å’ŒåŸºç¡€é…ç½®</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å¯¼å…¥å¿…è¦çš„åº“å’Œè®¾ç½®é…ç½®</div>
                    <div class="code-filename">transformer.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> torch.nn.functional <span class="code-keyword">as</span> F
<span class="code-keyword">import</span> math
<span class="code-keyword">import</span> numpy <span class="code-keyword">as</span> np
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Optional, Tuple

<span class="code-comment"># è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°</span>
torch.manual_seed(<span class="code-number">42</span>)
np.random.seed(<span class="code-number">42</span>)

<span class="code-comment"># æ¨¡å‹é…ç½®ç±»</span>
<span class="code-keyword">class</span> <span class="code-class">TransformerConfig</span>:
    <span class="code-string">"""Transformeræ¨¡å‹çš„é…ç½®å‚æ•°"""</span>
    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 vocab_size: int = <span class="code-number">10000</span>,      <span class="code-comment"># è¯è¡¨å¤§å°</span>
                 d_model: int = <span class="code-number">512</span>,          <span class="code-comment"># æ¨¡å‹ç»´åº¦</span>
                 n_heads: int = <span class="code-number">8</span>,            <span class="code-comment"># æ³¨æ„åŠ›å¤´æ•°</span>
                 n_encoder_layers: int = <span class="code-number">6</span>,   <span class="code-comment"># ç¼–ç å™¨å±‚æ•°</span>
                 n_decoder_layers: int = <span class="code-number">6</span>,   <span class="code-comment"># è§£ç å™¨å±‚æ•°</span>
                 d_ff: int = <span class="code-number">2048</span>,            <span class="code-comment"># å‰é¦ˆç½‘ç»œç»´åº¦</span>
                 max_seq_length: int = <span class="code-number">100</span>,   <span class="code-comment"># æœ€å¤§åºåˆ—é•¿åº¦</span>
                 dropout: float = <span class="code-number">0.1</span>,        <span class="code-comment"># Dropoutæ¯”ç‡</span>
                 pad_idx: int = <span class="code-number">0</span>):           <span class="code-comment"># å¡«å……tokençš„ç´¢å¼•</span>

        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.n_encoder_layers = n_encoder_layers
        self.n_decoder_layers = n_decoder_layers
        self.d_ff = d_ff
        self.max_seq_length = max_seq_length
        self.dropout = dropout
        self.pad_idx = pad_idx

        <span class="code-comment"># ç¡®ä¿d_modelå¯ä»¥è¢«n_headsæ•´é™¤</span>
        assert d_model % n_heads == <span class="code-number">0</span>, <span class="code-string">"d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"</span>
        self.d_k = d_model // n_heads  <span class="code-comment"># æ¯ä¸ªå¤´çš„ç»´åº¦</span></code></pre>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">ğŸ’¡ é…ç½®è¯´æ˜ï¼š</div>
                <div style="color: #cbd5e1; font-size: 0.95rem;">
                    â€¢ <code>d_model=512</code>ï¼šè¿™æ˜¯åŸè®ºæ–‡çš„æ ‡å‡†é…ç½®ï¼Œå¹³è¡¡äº†æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡<br>
                    â€¢ <code>n_heads=8</code>ï¼š8ä¸ªæ³¨æ„åŠ›å¤´ï¼Œæ¯ä¸ªå¤´çœ‹åˆ°64ç»´çš„è¡¨ç¤º<br>
                    â€¢ <code>d_ff=2048</code>ï¼šå‰é¦ˆç½‘ç»œæ˜¯æ¨¡å‹ç»´åº¦çš„4å€ï¼Œæä¾›è¶³å¤Ÿçš„éçº¿æ€§å˜æ¢èƒ½åŠ›
                </div>
            </div>
        </div>

        <!-- Step 1.2: ä½ç½®ç¼–ç  -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">2</div>
                <div class="step-title">ä½ç½®ç¼–ç å®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°æ­£å¼¦ä½ç½®ç¼–ç </div>
                    <div class="code-filename">components/positional_encoding.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">PositionalEncoding</span>(nn.Module):
    <span class="code-string">"""ä½ç½®ç¼–ç å±‚

    ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿç†è§£åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, max_seq_length: int = <span class="code-number">5000</span>):
        super().__init__()
        self.d_model = d_model

        <span class="code-comment"># åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ</span>
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(<span class="code-number">0</span>, max_seq_length).unsqueeze(<span class="code-number">1</span>).float()

        <span class="code-comment"># è®¡ç®—è§’åº¦çš„åˆ†æ¯é¡¹</span>
        div_term = torch.exp(torch.arange(<span class="code-number">0</span>, d_model, <span class="code-number">2</span>).float() *
                           -(math.log(<span class="code-number">10000.0</span>) / d_model))

        <span class="code-comment"># åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°</span>
        pe[:, <span class="code-number">0</span>::<span class="code-number">2</span>] = torch.sin(position * div_term)  <span class="code-comment"># å¶æ•°ä½ç½®</span>
        pe[:, <span class="code-number">1</span>::<span class="code-number">2</span>] = torch.cos(position * div_term)  <span class="code-comment"># å¥‡æ•°ä½ç½®</span>

        <span class="code-comment"># æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸æ¢¯åº¦è®¡ç®—</span>
        self.register_buffer(<span class="code-string">'pe'</span>, pe.unsqueeze(<span class="code-number">0</span>))

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_length, d_model]

        Returns:
            æ·»åŠ ä½ç½®ç¼–ç åçš„å¼ é‡
        """</span>
        seq_length = x.size(<span class="code-number">1</span>)
        <span class="code-comment"># æ·»åŠ ä½ç½®ç¼–ç ï¼ˆè‡ªåŠ¨å¹¿æ’­åˆ°batchç»´åº¦ï¼‰</span>
        x = x + self.pe[:, :seq_length, :]
        <span class="code-keyword">return</span> x</code></pre>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ¯</span> ä¸ºä»€ä¹ˆç”¨æ­£å¼¦/ä½™å¼¦ï¼Ÿ</h4>
                <div style="color: #cbd5e1;">
                    â€¢ <strong>å‘¨æœŸæ€§ï¼š</strong>ä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢èƒ½ç¼–ç ä¸åŒå°ºåº¦çš„ä½ç½®ä¿¡æ¯<br>
                    â€¢ <strong>å¤–æ¨æ€§ï¼š</strong>å¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„æ›´é•¿åºåˆ—<br>
                    â€¢ <strong>ç›¸å¯¹ä½ç½®ï¼š</strong>ä½ç½®iå’Œi+kä¹‹é—´çš„ç¼–ç å·®å¼‚æ˜¯å›ºå®šçš„
                </div>
            </div>
        </div>

        <!-- Step 1.3: å¤šå¤´æ³¨æ„åŠ› -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">3</div>
                <div class="step-title">å¤šå¤´æ³¨æ„åŠ›å®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶</div>
                    <div class="code-filename">components/multi_head_attention.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">MultiHeadAttention</span>(nn.Module):
    <span class="code-string">"""å¤šå¤´æ³¨æ„åŠ›å±‚

    å°†æ³¨æ„åŠ›åˆ†æˆå¤šä¸ªå¤´ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()
        self.d_model = config.d_model
        self.n_heads = config.n_heads
        self.d_k = config.d_k

        <span class="code-comment"># Qã€Kã€Vçš„çº¿æ€§å˜æ¢</span>
        self.W_q = nn.Linear(self.d_model, self.d_model)
        self.W_k = nn.Linear(self.d_model, self.d_model)
        self.W_v = nn.Linear(self.d_model, self.d_model)

        <span class="code-comment"># è¾“å‡ºçš„çº¿æ€§å˜æ¢</span>
        self.W_o = nn.Linear(self.d_model, self.d_model)

        self.dropout = nn.Dropout(config.dropout)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                query: torch.Tensor,
                key: torch.Tensor,
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            query: [batch_size, seq_len, d_model]
            key: [batch_size, seq_len, d_model]
            value: [batch_size, seq_len, d_model]
            mask: [batch_size, seq_len, seq_len] or [batch_size, 1, seq_len, seq_len]

        Returns:
            æ³¨æ„åŠ›è¾“å‡º [batch_size, seq_len, d_model]
        """</span>
        batch_size = query.size(<span class="code-number">0</span>)
        seq_len = query.size(<span class="code-number">1</span>)

        <span class="code-comment"># 1. çº¿æ€§å˜æ¢å¹¶é‡å¡‘ä¸ºå¤šå¤´</span>
        <span class="code-comment"># [batch_size, seq_len, d_model] -> [batch_size, n_heads, seq_len, d_k]</span>
        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

        <span class="code-comment"># 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span>
        scores = torch.matmul(Q, K.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) / math.sqrt(self.d_k)

        <span class="code-comment"># 3. åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰</span>
        <span class="code-keyword">if</span> mask <span class="code-keyword">is not None</span>:
            <span class="code-comment"># æ‰©å±•maskä»¥åŒ¹é…å¤šå¤´ç»´åº¦</span>
            <span class="code-keyword">if</span> mask.dim() == <span class="code-number">3</span>:
                mask = mask.unsqueeze(<span class="code-number">1</span>)  <span class="code-comment"># [batch, 1, seq_len, seq_len]</span>
            scores = scores.masked_fill(mask == <span class="code-number">0</span>, -<span class="code-number">1e9</span>)

        <span class="code-comment"># 4. Softmaxè·å¾—æ³¨æ„åŠ›æƒé‡</span>
        attention_weights = F.softmax(scores, dim=-<span class="code-number">1</span>)
        attention_weights = self.dropout(attention_weights)

        <span class="code-comment"># 5. åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°Value</span>
        attention_output = torch.matmul(attention_weights, V)

        <span class="code-comment"># 6. é‡å¡‘å¹¶é€šè¿‡è¾“å‡ºæŠ•å½±</span>
        <span class="code-comment"># [batch_size, n_heads, seq_len, d_k] -> [batch_size, seq_len, d_model]</span>
        attention_output = attention_output.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>).contiguous().view(
            batch_size, seq_len, self.d_model
        )

        output = self.W_o(attention_output)

        <span class="code-keyword">return</span> output</code></pre>
            </div>

            <div style="background: rgba(59, 130, 246, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                <div style="color: #3b82f6; font-weight: bold; margin-bottom: 0.5rem;">ğŸ” å®ç°ç»†èŠ‚ï¼š</div>
                <div style="color: #cbd5e1; font-size: 0.95rem;">
                    â€¢ <strong>ç¼©æ”¾å› å­ï¼š</strong><code>1/âˆšd_k</code>é˜²æ­¢softmaxçš„æ¢¯åº¦æ¶ˆå¤±<br>
                    â€¢ <strong>æ©ç å€¼ï¼š</strong>ä½¿ç”¨-1e9è€Œä¸æ˜¯-âˆï¼Œé¿å…æ•°å€¼é—®é¢˜<br>
                    â€¢ <strong>Dropoutï¼š</strong>åº”ç”¨åœ¨æ³¨æ„åŠ›æƒé‡ä¸Šï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
                </div>
            </div>
        </div>

        <!-- Step 1.4: å‰é¦ˆç½‘ç»œ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">4</div>
                <div class="step-title">å‰é¦ˆç½‘ç»œå®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°ä½ç½®å‰é¦ˆç½‘ç»œ</div>
                    <div class="code-filename">components/feed_forward.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">FeedForward</span>(nn.Module):
    <span class="code-string">"""å‰é¦ˆç½‘ç»œå±‚

    ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œä¸­é—´ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()

        <span class="code-comment"># ä¸¤å±‚çº¿æ€§å˜æ¢</span>
        self.linear1 = nn.Linear(config.d_model, config.d_ff)
        self.linear2 = nn.Linear(config.d_ff, config.d_model)

        self.dropout = nn.Dropout(config.dropout)
        self.activation = nn.ReLU()

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            x: [batch_size, seq_len, d_model]

        Returns:
            å‰é¦ˆç½‘ç»œè¾“å‡º [batch_size, seq_len, d_model]
        """</span>
        <span class="code-comment"># ç¬¬ä¸€å±‚ï¼šæ‰©å±•ç»´åº¦</span>
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout(x)

        <span class="code-comment"># ç¬¬äºŒå±‚ï¼šæ¢å¤ç»´åº¦</span>
        x = self.linear2(x)
        x = self.dropout(x)

        <span class="code-keyword">return</span> x</code></pre>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ’¡</span> å‰é¦ˆç½‘ç»œçš„ä½œç”¨</h4>
                <div style="color: #cbd5e1;">
                    â€¢ <strong>éçº¿æ€§å˜æ¢ï¼š</strong>ReLUæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§<br>
                    â€¢ <strong>ç‰¹å¾å¤„ç†ï¼š</strong>å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œç‰¹å¾å˜æ¢<br>
                    â€¢ <strong>ç»´åº¦æ‰©å±•ï¼š</strong>å…ˆæ‰©å±•åˆ°4å€ç»´åº¦ï¼Œå†å‹ç¼©å›åŸç»´åº¦ï¼Œå¢åŠ æ¨¡å‹å®¹é‡
                </div>
            </div>
        </div>

        <!-- Step 1.5: å±‚å½’ä¸€åŒ– -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">5</div>
                <div class="step-title">æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°æ®‹å·®è¿æ¥åŒ…è£…å™¨ï¼ˆä¸nn.Transformerä¸€è‡´ï¼‰</div>
                    <div class="code-filename">components/residual_layer.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">ResidualConnection</span>(nn.Module):
    <span class="code-string">"""æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–

    å®ç°: LayerNorm(x + Sublayer(x)) - ä¸nn.Transformerä¿æŒä¸€è‡´
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()
        self.layer_norm = nn.LayerNorm(config.d_model)
        self.dropout = nn.Dropout(config.dropout)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            x: è¾“å…¥å¼ é‡
            sublayer: å­å±‚ï¼ˆæ³¨æ„åŠ›æˆ–å‰é¦ˆç½‘ç»œï¼‰

        Returns:
            åº”ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–åçš„è¾“å‡º
        """</span>
        <span class="code-comment"># Post-LNæ¶æ„ï¼ˆä¸nn.Transformerä¸€è‡´ï¼‰</span>
        output = sublayer(x)
        output = self.dropout(output)

        <span class="code-comment"># æ®‹å·®è¿æ¥åè¿›è¡Œå±‚å½’ä¸€åŒ–</span>
        <span class="code-keyword">return</span> self.layer_norm(x + output)</code></pre>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">ğŸ“ ä¸nn.Transformerä¿æŒä¸€è‡´ï¼š</div>
                <div style="color: #cbd5e1; font-size: 0.95rem;">
                    â€¢ <strong>Post-LNæ¶æ„ï¼š</strong>LN(x + sublayer(x))<br>
                    â€¢ <strong>è¿™æ˜¯åŸå§‹è®ºæ–‡çš„è®¾è®¡</strong><br>
                    â€¢ <strong>æ³¨æ„ï¼š</strong>è™½ç„¶Pre-LNæ›´ç¨³å®šï¼Œä½†ä¸ºäº†ä¸å®˜æ–¹å®ç°ä¸€è‡´ï¼Œæˆ‘ä»¬ä½¿ç”¨Post-LN
                </div>
            </div>
        </div>
    </div>

    <!-- ğŸ—ï¸ Step 2: ç¼–ç å™¨å’Œè§£ç å™¨å®ç° -->
    <div class="implementation-section" style="border-color: #06b6d4;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(6, 182, 212, 0.2), rgba(14, 165, 233, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(6, 182, 212, 0.4);">
                <span style="font-size: 2rem;">ğŸ—ï¸</span>
                <h2 style="color: #06b6d4; margin: 0; font-size: 1.8rem; font-weight: bold;">æ„å»ºç¼–ç å™¨å’Œè§£ç å™¨</h2>
            </div>
        </div>

        <!-- Step 2.1: ç¼–ç å™¨å±‚ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">6</div>
                <div class="step-title">ç¼–ç å™¨å±‚å®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å•ä¸ªç¼–ç å™¨å±‚</div>
                    <div class="code-filename">layers/encoder_layer.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">EncoderLayer</span>(nn.Module):
    <span class="code-string">"""Transformerç¼–ç å™¨å±‚

    åŒ…å«ï¼šè‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œï¼Œæ¯ä¸ªéƒ½æœ‰æ®‹å·®è¿æ¥
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()

        <span class="code-comment"># å­å±‚</span>
        self.self_attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)

        <span class="code-comment"># æ®‹å·®è¿æ¥</span>
        self.residual1 = ResidualConnection(config)
        self.residual2 = ResidualConnection(config)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            x: [batch_size, seq_len, d_model]
            mask: [batch_size, 1, 1, seq_len] padding mask

        Returns:
            ç¼–ç å™¨å±‚è¾“å‡º [batch_size, seq_len, d_model]
        """</span>
        <span class="code-comment"># 1. è‡ªæ³¨æ„åŠ›ï¼ˆå¸¦æ®‹å·®ï¼‰</span>
        x = self.residual1(x, <span class="code-keyword">lambda</span> x: self.self_attention(x, x, x, mask))

        <span class="code-comment"># 2. å‰é¦ˆç½‘ç»œï¼ˆå¸¦æ®‹å·®ï¼‰</span>
        x = self.residual2(x, self.feed_forward)

        <span class="code-keyword">return</span> x</code></pre>
            </div>
        </div>

        <!-- Step 2.2: è§£ç å™¨å±‚ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">7</div>
                <div class="step-title">è§£ç å™¨å±‚å®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å•ä¸ªè§£ç å™¨å±‚</div>
                    <div class="code-filename">layers/decoder_layer.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">DecoderLayer</span>(nn.Module):
    <span class="code-string">"""Transformerè§£ç å™¨å±‚

    åŒ…å«ï¼šæ©ç è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()

        <span class="code-comment"># å­å±‚</span>
        self.self_attention = MultiHeadAttention(config)
        self.cross_attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)

        <span class="code-comment"># æ®‹å·®è¿æ¥</span>
        self.residual1 = ResidualConnection(config)
        self.residual2 = ResidualConnection(config)
        self.residual3 = ResidualConnection(config)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                x: torch.Tensor,
                encoder_output: torch.Tensor,
                self_mask: Optional[torch.Tensor] = None,
                cross_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            x: è§£ç å™¨è¾“å…¥ [batch_size, tgt_len, d_model]
            encoder_output: ç¼–ç å™¨è¾“å‡º [batch_size, src_len, d_model]
            self_mask: è‡ªæ³¨æ„åŠ›æ©ç ï¼ˆå› æœ+paddingï¼‰
            cross_mask: äº¤å‰æ³¨æ„åŠ›æ©ç ï¼ˆpaddingï¼‰

        Returns:
            è§£ç å™¨å±‚è¾“å‡º [batch_size, tgt_len, d_model]
        """</span>
        <span class="code-comment"># 1. æ©ç è‡ªæ³¨æ„åŠ›</span>
        x = self.residual1(x, <span class="code-keyword">lambda</span> x: self.self_attention(x, x, x, self_mask))

        <span class="code-comment"># 2. äº¤å‰æ³¨æ„åŠ›ï¼ˆQueryæ¥è‡ªè§£ç å™¨ï¼ŒKey/Valueæ¥è‡ªç¼–ç å™¨ï¼‰</span>
        x = self.residual2(x, <span class="code-keyword">lambda</span> x: self.cross_attention(
            x, encoder_output, encoder_output, cross_mask
        ))

        <span class="code-comment"># 3. å‰é¦ˆç½‘ç»œ</span>
        x = self.residual3(x, self.feed_forward)

        <span class="code-keyword">return</span> x</code></pre>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ”„</span> è§£ç å™¨çš„ä¸‰ä¸ªæ³¨æ„åŠ›é˜¶æ®µ</h4>
                <div style="color: #cbd5e1;">
                    <strong>1. è‡ªæ³¨æ„åŠ›ï¼š</strong>ç†è§£å·²ç”Ÿæˆçš„å†…å®¹ï¼ˆå¸¦å› æœæ©ç ï¼‰<br>
                    <strong>2. äº¤å‰æ³¨æ„åŠ›ï¼š</strong>å…³æ³¨ç¼–ç å™¨çš„è¾“å‡ºï¼Œç†è§£æºä¿¡æ¯<br>
                    <strong>3. å‰é¦ˆç½‘ç»œï¼š</strong>åŸºäºæ‰€æœ‰ä¿¡æ¯è¿›è¡Œç‰¹å¾å˜æ¢
                </div>
            </div>
        </div>

        <!-- Step 2.3: å®Œæ•´ç¼–ç å™¨ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">8</div>
                <div class="step-title">å®Œæ•´ç¼–ç å™¨å®ç°ï¼ˆä¸nn.Transformerä¸€è‡´ï¼‰</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å †å å¤šä¸ªç¼–ç å™¨å±‚</div>
                    <div class="code-filename">models/encoder.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">TransformerEncoder</span>(nn.Module):
    <span class="code-string">"""å®Œæ•´çš„Transformerç¼–ç å™¨ï¼ˆä¸nn.Transformerä¿æŒä¸€è‡´ï¼‰

    åªåŒ…å«ç¼–ç å™¨å±‚çš„å †å ï¼Œä¸åŒ…å«åµŒå…¥å±‚
    è¾“å…¥è¾“å‡ºæ ¼å¼ï¼š[seq_len, batch_size, d_model]
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()

        <span class="code-comment"># ç¼–ç å™¨å±‚å †å </span>
        self.layers = nn.ModuleList([
            EncoderLayer(config) <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(config.n_encoder_layers)
        ])

        <span class="code-comment"># æœ€åçš„å±‚å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰</span>
        self.norm = nn.LayerNorm(config.d_model)

        self.d_model = config.d_model

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                src: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            src: æºåºåˆ—åµŒå…¥ [seq_len, batch_size, d_model]
            src_mask: æ³¨æ„åŠ›æ©ç  [seq_len, seq_len]
            src_key_padding_mask: paddingæ©ç  [batch_size, seq_len]

        Returns:
            ç¼–ç å™¨è¾“å‡º [seq_len, batch_size, d_model]
        """</span>
        <span class="code-comment"># ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®</span>
        <span class="code-keyword">assert</span> src.dim() == <span class="code-number">3</span> <span class="code-keyword">and</span> src.size(-<span class="code-number">1</span>) == self.d_model

        output = src

        <span class="code-comment"># è½¬æ¢padding maskæ ¼å¼ä»¥é€‚é…æˆ‘ä»¬çš„å¤šå¤´æ³¨æ„åŠ›</span>
        <span class="code-keyword">if</span> src_key_padding_mask <span class="code-keyword">is not None</span>:
            <span class="code-comment"># [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]</span>
            src_mask_expanded = (~src_key_padding_mask).unsqueeze(<span class="code-number">1</span>).unsqueeze(<span class="code-number">2</span>).float()
        <span class="code-keyword">else</span>:
            src_mask_expanded = None

        <span class="code-comment"># é€šè¿‡Nä¸ªç¼–ç å™¨å±‚</span>
        <span class="code-keyword">for</span> layer <span class="code-keyword">in</span> self.layers:
            <span class="code-comment"># éœ€è¦è½¬æ¢æ ¼å¼ï¼š[seq_len, batch, d_model] -> [batch, seq_len, d_model]</span>
            output = output.transpose(<span class="code-number">0</span>, <span class="code-number">1</span>)
            output = layer(output, src_mask_expanded)
            <span class="code-comment"># è½¬å›ï¼š[batch, seq_len, d_model] -> [seq_len, batch, d_model]</span>
            output = output.transpose(<span class="code-number">0</span>, <span class="code-number">1</span>)

        <span class="code-comment"># æœ€ç»ˆå½’ä¸€åŒ–</span>
        output = self.norm(output)

        <span class="code-keyword">return</span> output</code></pre>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">ğŸ“ ä¸nn.Transformerçš„ä¸€è‡´æ€§ï¼š</div>
                <div style="color: #cbd5e1; font-size: 0.95rem;">
                    â€¢ <strong>è¾“å…¥æ ¼å¼ï¼š</strong>[seq_len, batch_size, d_model] - åºåˆ—é•¿åº¦åœ¨å‰<br>
                    â€¢ <strong>ä¸åŒ…å«åµŒå…¥å±‚ï¼š</strong>æœŸæœ›å·²ç»åµŒå…¥çš„å‘é‡ä½œä¸ºè¾“å…¥<br>
                    â€¢ <strong>æ©ç æ ¼å¼ï¼š</strong>src_key_padding_maskæ˜¯[batch_size, seq_len]<br>
                    â€¢ <strong>å†…éƒ¨è½¬æ¢ï¼š</strong>æˆ‘ä»¬çš„æ³¨æ„åŠ›å±‚æœŸæœ›batchåœ¨å‰ï¼Œæ‰€ä»¥å†…éƒ¨éœ€è¦è½¬ç½®
                </div>
            </div>
        </div>

        <!-- Step 2.4: å®Œæ•´è§£ç å™¨ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">9</div>
                <div class="step-title">å®Œæ•´è§£ç å™¨å®ç°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å †å å¤šä¸ªè§£ç å™¨å±‚</div>
                    <div class="code-filename">models/decoder.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">TransformerDecoder</span>(nn.Module):
    <span class="code-string">"""å®Œæ•´çš„Transformerè§£ç å™¨

    åŒ…å«ï¼šè¯åµŒå…¥ + ä½ç½®ç¼–ç  + Nä¸ªè§£ç å™¨å±‚ + è¾“å‡ºæŠ•å½±
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()

        <span class="code-comment"># åµŒå…¥å±‚</span>
        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model,
                                          padding_idx=config.pad_idx)
        self.positional_encoding = PositionalEncoding(config.d_model,
                                                     config.max_seq_length)

        <span class="code-comment"># è§£ç å™¨å±‚å †å </span>
        self.layers = nn.ModuleList([
            DecoderLayer(config) <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(config.n_decoder_layers)
        ])

        <span class="code-comment"># æœ€åçš„å±‚å½’ä¸€åŒ–</span>
        self.final_norm = nn.LayerNorm(config.d_model)

        <span class="code-comment"># è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨</span>
        self.output_projection = nn.Linear(config.d_model, config.vocab_size)

        self.dropout = nn.Dropout(config.dropout)
        self.d_model = config.d_model

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                tgt: torch.Tensor,
                encoder_output: torch.Tensor,
                tgt_mask: Optional[torch.Tensor] = None,
                src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            tgt: ç›®æ ‡åºåˆ—token ids [batch_size, tgt_len]
            encoder_output: ç¼–ç å™¨è¾“å‡º [batch_size, src_len, d_model]
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç ï¼ˆå› æœ+paddingï¼‰
            src_mask: æºåºåˆ—æ©ç ï¼ˆç”¨äºäº¤å‰æ³¨æ„åŠ›ï¼‰

        Returns:
            è§£ç å™¨è¾“å‡ºlogits [batch_size, tgt_len, vocab_size]
        """</span>
        <span class="code-comment"># 1. è¯åµŒå…¥å’Œä½ç½®ç¼–ç </span>
        x = self.token_embedding(tgt) * math.sqrt(self.d_model)
        x = self.positional_encoding(x)
        x = self.dropout(x)

        <span class="code-comment"># 2. é€šè¿‡Nä¸ªè§£ç å™¨å±‚</span>
        <span class="code-keyword">for</span> layer <span class="code-keyword">in</span> self.layers:
            x = layer(x, encoder_output, tgt_mask, src_mask)

        <span class="code-comment"># 3. æœ€ç»ˆå½’ä¸€åŒ–</span>
        x = self.final_norm(x)

        <span class="code-comment"># 4. æŠ•å½±åˆ°è¯è¡¨å¤§å°</span>
        logits = self.output_projection(x)

        <span class="code-keyword">return</span> logits</code></pre>
            </div>
        </div>
    </div>

    <!-- ğŸš€ Step 3: å®Œæ•´æ¨¡å‹å’Œå·¥å…·å‡½æ•° -->
    <div class="implementation-section" style="border-color: #fbbf24;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(251, 191, 36, 0.2), rgba(245, 158, 11, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(251, 191, 36, 0.4);">
                <span style="font-size: 2rem;">ğŸš€</span>
                <h2 style="color: #fbbf24; margin: 0; font-size: 1.8rem; font-weight: bold;">ç»„è£…å®Œæ•´çš„Transformer</h2>
            </div>
        </div>

        <!-- Step 3.1: æ©ç ç”Ÿæˆ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">10</div>
                <div class="step-title">æ©ç ç”Ÿæˆå·¥å…·</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">ç”Ÿæˆå„ç§æ©ç çš„å·¥å…·å‡½æ•°</div>
                    <div class="code-filename">utils/masking.py</div>
                </div>
                <pre><code><span class="code-keyword">def</span> <span class="code-function">create_padding_mask</span>(seq: torch.Tensor, pad_idx: int = <span class="code-number">0</span>) -> torch.Tensor:
    <span class="code-string">"""åˆ›å»ºpaddingæ©ç 

    Args:
        seq: tokenåºåˆ— [batch_size, seq_len]
        pad_idx: padding tokençš„ç´¢å¼•

    Returns:
        paddingæ©ç  [batch_size, 1, 1, seq_len]
    """</span>
    <span class="code-comment"># Trueè¡¨ç¤ºä¿ç•™ï¼ŒFalseè¡¨ç¤ºå±è”½</span>
    mask = (seq != pad_idx).unsqueeze(<span class="code-number">1</span>).unsqueeze(<span class="code-number">2</span>)
    <span class="code-keyword">return</span> mask.float()

<span class="code-keyword">def</span> <span class="code-function">create_causal_mask</span>(size: int) -> torch.Tensor:
    <span class="code-string">"""åˆ›å»ºå› æœæ©ç ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰

    Args:
        size: åºåˆ—é•¿åº¦

    Returns:
        å› æœæ©ç  [size, size]
    """</span>
    mask = torch.triu(torch.ones(size, size), diagonal=<span class="code-number">1</span>)
    <span class="code-keyword">return</span> (mask == <span class="code-number">0</span>).float()

<span class="code-keyword">def</span> <span class="code-function">create_decoder_mask</span>(tgt: torch.Tensor, pad_idx: int = <span class="code-number">0</span>) -> torch.Tensor:
    <span class="code-string">"""åˆ›å»ºè§£ç å™¨çš„ç»„åˆæ©ç 

    ç»“åˆpaddingæ©ç å’Œå› æœæ©ç 

    Args:
        tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len]
        pad_idx: padding tokençš„ç´¢å¼•

    Returns:
        ç»„åˆæ©ç  [batch_size, 1, tgt_len, tgt_len]
    """</span>
    batch_size, tgt_len = tgt.size()

    <span class="code-comment"># 1. Paddingæ©ç </span>
    padding_mask = create_padding_mask(tgt, pad_idx)  <span class="code-comment"># [batch, 1, 1, tgt_len]</span>

    <span class="code-comment"># 2. å› æœæ©ç </span>
    causal_mask = create_causal_mask(tgt_len).unsqueeze(<span class="code-number">0</span>).unsqueeze(<span class="code-number">0</span>)  <span class="code-comment"># [1, 1, tgt_len, tgt_len]</span>
    causal_mask = causal_mask.expand(batch_size, <span class="code-number">1</span>, tgt_len, tgt_len)

    <span class="code-comment"># 3. ç»„åˆï¼šä¸¤ä¸ªæ©ç éƒ½ä¸ºTrueçš„ä½ç½®æ‰ä¿ç•™</span>
    <span class="code-comment"># æ‰©å±•padding_maskä»¥åŒ¹é…causal_maskçš„å½¢çŠ¶</span>
    padding_mask_expanded = padding_mask.expand(-<span class="code-number">1</span>, -<span class="code-number">1</span>, tgt_len, -<span class="code-number">1</span>)
    combined_mask = padding_mask_expanded * causal_mask

    <span class="code-keyword">return</span> combined_mask</code></pre>
            </div>
        </div>

        <!-- Step 3.2: å®Œæ•´Transformeræ¨¡å‹ -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">11</div>
                <div class="step-title">å®Œæ•´çš„Transformeræ¨¡å‹</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">ç»„è£…æ‰€æœ‰ç»„ä»¶</div>
                    <div class="code-filename">transformer.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">Transformer</span>(nn.Module):
    <span class="code-string">"""å®Œæ•´çš„Transformeræ¨¡å‹

    ç”¨äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼ˆå¦‚æœºå™¨ç¿»è¯‘ï¼‰
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, config: TransformerConfig):
        super().__init__()
        self.config = config

        <span class="code-comment"># ç¼–ç å™¨å’Œè§£ç å™¨</span>
        self.encoder = TransformerEncoder(config)
        self.decoder = TransformerDecoder(config)

        <span class="code-comment"># åˆå§‹åŒ–å‚æ•°</span>
        self._init_parameters()

    <span class="code-keyword">def</span> <span class="code-function">_init_parameters</span>(self):
        <span class="code-string">"""Xavieråˆå§‹åŒ–å‚æ•°"""</span>
        <span class="code-keyword">for</span> p <span class="code-keyword">in</span> self.parameters():
            <span class="code-keyword">if</span> p.dim() > <span class="code-number">1</span>:
                nn.init.xavier_uniform_(p)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                src: torch.Tensor,
                tgt: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""
        Args:
            src: æºåºåˆ— [batch_size, src_len]
            tgt: ç›®æ ‡åºåˆ— [batch_size, tgt_len]
            src_mask: æºåºåˆ—æ©ç 
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç 

        Returns:
            è¾“å‡ºlogits [batch_size, tgt_len, vocab_size]
        """</span>
        <span class="code-comment"># 1. å¦‚æœæ²¡æœ‰æä¾›æ©ç ï¼Œè‡ªåŠ¨ç”Ÿæˆ</span>
        <span class="code-keyword">if</span> src_mask <span class="code-keyword">is None</span>:
            src_mask = create_padding_mask(src, self.config.pad_idx)

        <span class="code-keyword">if</span> tgt_mask <span class="code-keyword">is None</span>:
            tgt_mask = create_decoder_mask(tgt, self.config.pad_idx)

        <span class="code-comment"># 2. ç¼–ç å™¨å‰å‘ä¼ æ’­</span>
        encoder_output = self.encoder(src, src_mask)

        <span class="code-comment"># 3. è§£ç å™¨å‰å‘ä¼ æ’­</span>
        decoder_output = self.decoder(tgt, encoder_output, tgt_mask, src_mask)

        <span class="code-keyword">return</span> decoder_output

    <span class="code-keyword">def</span> <span class="code-function">encode</span>(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""å•ç‹¬è¿è¡Œç¼–ç å™¨ï¼ˆç”¨äºæ¨ç†ï¼‰"""</span>
        <span class="code-keyword">if</span> src_mask <span class="code-keyword">is None</span>:
            src_mask = create_padding_mask(src, self.config.pad_idx)
        <span class="code-keyword">return</span> self.encoder(src, src_mask)

    <span class="code-keyword">def</span> <span class="code-function">decode</span>(self,
                tgt: torch.Tensor,
                encoder_output: torch.Tensor,
                src_mask: Optional[torch.Tensor] = None,
                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        <span class="code-string">"""å•ç‹¬è¿è¡Œè§£ç å™¨ï¼ˆç”¨äºæ¨ç†ï¼‰"""</span>
        <span class="code-keyword">if</span> tgt_mask <span class="code-keyword">is None</span>:
            tgt_mask = create_decoder_mask(tgt, self.config.pad_idx)
        <span class="code-keyword">return</span> self.decoder(tgt, encoder_output, tgt_mask, src_mask)</code></pre>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ¯</span> æ¨¡å‹ä½¿ç”¨æ–¹å¼</h4>
                <div style="color: #cbd5e1;">
                    <strong>è®­ç»ƒæ—¶ï¼š</strong>ä½¿ç”¨å®Œæ•´çš„forwardæ–¹æ³•ï¼Œè¾“å…¥æºåºåˆ—å’Œç›®æ ‡åºåˆ—<br>
                    <strong>æ¨ç†æ—¶ï¼š</strong>åˆ†åˆ«ä½¿ç”¨encodeå’Œdecodeï¼Œæ”¯æŒbeam searchç­‰è§£ç ç­–ç•¥<br>
                    <strong>æ©ç ï¼š</strong>è‡ªåŠ¨ç”Ÿæˆæˆ–æ‰‹åŠ¨æä¾›ï¼Œçµæ´»æ§åˆ¶æ³¨æ„åŠ›æ¨¡å¼
                </div>
            </div>
        </div>

        <!-- Step 3.3: æ¨ç†å‡½æ•° -->
        <div class="implementation-step">
            <div class="step-header">
                <div class="step-number">12</div>
                <div class="step-title">æ¨ç†ç”Ÿæˆå‡½æ•°</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">è´ªå©ªè§£ç æ¨ç†</div>
                    <div class="code-filename">inference.py</div>
                </div>
                <pre><code><span class="code-keyword">def</span> <span class="code-function">greedy_decode</span>(model: Transformer,
                     src: torch.Tensor,
                     max_length: int = <span class="code-number">100</span>,
                     start_token: int = <span class="code-number">1</span>,
                     end_token: int = <span class="code-number">2</span>) -> torch.Tensor:
    <span class="code-string">"""è´ªå©ªè§£ç ç”Ÿæˆåºåˆ—

    Args:
        model: Transformeræ¨¡å‹
        src: æºåºåˆ— [batch_size, src_len]
        max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
        start_token: å¼€å§‹tokençš„ID
        end_token: ç»“æŸtokençš„ID

    Returns:
        ç”Ÿæˆçš„åºåˆ— [batch_size, seq_len]
    """</span>
    model.eval()
    device = src.device
    batch_size = src.size(<span class="code-number">0</span>)

    <span class="code-keyword">with</span> torch.no_grad():
        <span class="code-comment"># 1. ç¼–ç æºåºåˆ—</span>
        encoder_output = model.encode(src)
        src_mask = create_padding_mask(src, model.config.pad_idx)

        <span class="code-comment"># 2. åˆå§‹åŒ–ç›®æ ‡åºåˆ—ä¸º[START]</span>
        tgt = torch.full((batch_size, <span class="code-number">1</span>), start_token, device=device)

        <span class="code-comment"># 3. é€æ­¥ç”Ÿæˆ</span>
        <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(max_length - <span class="code-number">1</span>):
            <span class="code-comment"># è§£ç å½“å‰åºåˆ—</span>
            logits = model.decode(tgt, encoder_output, src_mask)

            <span class="code-comment"># è·å–æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹</span>
            next_token_logits = logits[:, -<span class="code-number">1</span>, :]
            next_token = torch.argmax(next_token_logits, dim=-<span class="code-number">1</span>, keepdim=True)

            <span class="code-comment"># æ·»åŠ åˆ°ç›®æ ‡åºåˆ—</span>
            tgt = torch.cat([tgt, next_token], dim=<span class="code-number">1</span>)

            <span class="code-comment"># æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½ç”Ÿæˆäº†END token</span>
            <span class="code-keyword">if</span> (next_token == end_token).all():
                <span class="code-keyword">break</span>

    <span class="code-keyword">return</span> tgt</code></pre>
            </div>
        </div>
    </div>

    <!-- ğŸ§ª Step 4: æµ‹è¯•å’ŒéªŒè¯ -->
    <div class="implementation-section" style="border-color: #22c55e;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">ğŸ§ª</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">æµ‹è¯•æˆ‘ä»¬çš„å®ç°</h2>
            </div>
        </div>

        <!-- æµ‹è¯•ä»£ç  -->
        <div class="test-section">
            <h4 style="color: #22c55e; margin-bottom: 1rem;">âœ… è¿è¡Œä¸€ä¸ªç®€å•çš„æµ‹è¯•</h4>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">æµ‹è¯•Transformeræ¨¡å‹</div>
                    <div class="code-filename">test_transformer.py</div>
                </div>
                <pre><code><span class="code-keyword">def</span> <span class="code-function">test_transformer</span>():
    <span class="code-string">"""æµ‹è¯•Transformerçš„åŸºæœ¬åŠŸèƒ½"""</span>

    <span class="code-comment"># 1. åˆ›å»ºé…ç½®</span>
    config = TransformerConfig(
        vocab_size=<span class="code-number">1000</span>,
        d_model=<span class="code-number">256</span>,
        n_heads=<span class="code-number">8</span>,
        n_encoder_layers=<span class="code-number">2</span>,
        n_decoder_layers=<span class="code-number">2</span>,
        d_ff=<span class="code-number">1024</span>,
        max_seq_length=<span class="code-number">50</span>,
        dropout=<span class="code-number">0.1</span>
    )

    <span class="code-comment"># 2. åˆ›å»ºæ¨¡å‹</span>
    model = Transformer(config)
    print(<span class="code-string">f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}"</span>)

    <span class="code-comment"># 3. åˆ›å»ºéšæœºè¾“å…¥</span>
    batch_size = <span class="code-number">2</span>
    src_len = <span class="code-number">10</span>
    tgt_len = <span class="code-number">8</span>

    <span class="code-comment"># éšæœºç”Ÿæˆtokenï¼ˆé¿å…ä½¿ç”¨0ï¼Œå› ä¸º0æ˜¯paddingï¼‰</span>
    src = torch.randint(<span class="code-number">1</span>, config.vocab_size, (batch_size, src_len))
    tgt = torch.randint(<span class="code-number">1</span>, config.vocab_size, (batch_size, tgt_len))

    <span class="code-comment"># æ·»åŠ ä¸€äº›paddingæ¥æµ‹è¯•æ©ç </span>
    src[<span class="code-number">0</span>, <span class="code-number">-3</span>:] = <span class="code-number">0</span>  <span class="code-comment"># ç¬¬ä¸€ä¸ªæ ·æœ¬æœ€å3ä¸ªä½ç½®æ˜¯padding</span>
    src[<span class="code-number">1</span>, <span class="code-number">-5</span>:] = <span class="code-number">0</span>  <span class="code-comment"># ç¬¬äºŒä¸ªæ ·æœ¬æœ€å5ä¸ªä½ç½®æ˜¯padding</span>

    <span class="code-comment"># 4. å‰å‘ä¼ æ’­</span>
    model.eval()  <span class="code-comment"># è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼</span>
    <span class="code-keyword">with</span> torch.no_grad():
        output = model(src, tgt)

    print(<span class="code-string">f"\nè¾“å…¥å½¢çŠ¶:"</span>)
    print(<span class="code-string">f"  æºåºåˆ—: {src.shape}"</span>)
    print(<span class="code-string">f"  ç›®æ ‡åºåˆ—: {tgt.shape}"</span>)
    print(<span class="code-string">f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}"</span>)
    print(<span class="code-string">f"æœŸæœ›å½¢çŠ¶: [batch_size={batch_size}, tgt_len={tgt_len}, vocab_size={config.vocab_size}]"</span>)

    <span class="code-comment"># 5. æµ‹è¯•æ¨ç†</span>
    print(<span class="code-string">f"\næµ‹è¯•è´ªå©ªè§£ç ..."</span>)
    generated = greedy_decode(model, src[:1], max_length=<span class="code-number">15</span>)
    print(<span class="code-string">f"ç”Ÿæˆåºåˆ—å½¢çŠ¶: {generated.shape}"</span>)
    print(<span class="code-string">f"ç”Ÿæˆçš„token: {generated[0].tolist()}"</span>)

    <span class="code-comment"># 6. éªŒè¯æ©ç </span>
    print(<span class="code-string">f"\néªŒè¯æ©ç åŠŸèƒ½..."</span>)
    src_mask = create_padding_mask(src, config.pad_idx)
    tgt_mask = create_decoder_mask(tgt, config.pad_idx)
    print(<span class="code-string">f"æºåºåˆ—æ©ç å½¢çŠ¶: {src_mask.shape}"</span>)
    print(<span class="code-string">f"ç›®æ ‡åºåˆ—æ©ç å½¢çŠ¶: {tgt_mask.shape}"</span>)

    <span class="code-keyword">return</span> model

<span class="code-comment"># è¿è¡Œæµ‹è¯•</span>
<span class="code-keyword">if</span> __name__ == <span class="code-string">"__main__"</span>:
    model = test_transformer()</code></pre>
            </div>

            <div class="test-output">
                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">é¢„æœŸè¾“å‡ºï¼š</div>
                <pre>æ¨¡å‹å‚æ•°é‡: 5,928,456

è¾“å…¥å½¢çŠ¶:
  æºåºåˆ—: torch.Size([2, 10])
  ç›®æ ‡åºåˆ—: torch.Size([2, 8])

è¾“å‡ºå½¢çŠ¶: torch.Size([2, 8, 1000])
æœŸæœ›å½¢çŠ¶: [batch_size=2, tgt_len=8, vocab_size=1000]

æµ‹è¯•è´ªå©ªè§£ç ...
ç”Ÿæˆåºåˆ—å½¢çŠ¶: torch.Size([1, 15])
ç”Ÿæˆçš„token: [1, 245, 567, 123, 890, 234, 678, 345, 901, 456, 789, 321, 654, 987, 2]

éªŒè¯æ©ç åŠŸèƒ½...
æºåºåˆ—æ©ç å½¢çŠ¶: torch.Size([2, 1, 1, 10])
ç›®æ ‡åºåˆ—æ©ç å½¢çŠ¶: torch.Size([2, 1, 8, 8])</pre>
            </div>
        </div>

        <!-- è®­ç»ƒç¤ºä¾‹ -->
        <div class="test-section" style="margin-top: 2rem;">
            <h4 style="color: #22c55e; margin-bottom: 1rem;">ğŸ‹ï¸ ç®€å•çš„è®­ç»ƒå¾ªç¯</h4>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">è®­ç»ƒå¾ªç¯ç¤ºä¾‹</div>
                    <div class="code-filename">train_example.py</div>
                </div>
                <pre><code><span class="code-keyword">def</span> <span class="code-function">train_step</span>(model: Transformer,
               src: torch.Tensor,
               tgt: torch.Tensor,
               optimizer: torch.optim.Optimizer,
               criterion: nn.Module,
               pad_idx: int = <span class="code-number">0</span>) -> float:
    <span class="code-string">"""å•ä¸ªè®­ç»ƒæ­¥éª¤"""</span>
    model.train()
    optimizer.zero_grad()

    <span class="code-comment"># å‡†å¤‡è¾“å…¥å’Œç›®æ ‡</span>
    <span class="code-comment"># è§£ç å™¨è¾“å…¥ï¼šå»æ‰æœ€åä¸€ä¸ªtoken</span>
    tgt_input = tgt[:, :-<span class="code-number">1</span>]
    <span class="code-comment"># ç›®æ ‡è¾“å‡ºï¼šå»æ‰ç¬¬ä¸€ä¸ªtoken</span>
    tgt_output = tgt[:, <span class="code-number">1</span>:]

    <span class="code-comment"># å‰å‘ä¼ æ’­</span>
    logits = model(src, tgt_input)

    <span class="code-comment"># è®¡ç®—æŸå¤±ï¼ˆå¿½ç•¥paddingä½ç½®ï¼‰</span>
    loss = criterion(
        logits.reshape(-<span class="code-number">1</span>, logits.size(-<span class="code-number">1</span>)),
        tgt_output.reshape(-<span class="code-number">1</span>)
    )

    <span class="code-comment"># åå‘ä¼ æ’­</span>
    loss.backward()

    <span class="code-comment"># æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰</span>
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="code-number">1.0</span>)

    <span class="code-comment"># æ›´æ–°å‚æ•°</span>
    optimizer.step()

    <span class="code-keyword">return</span> loss.item()

<span class="code-comment"># ç®€å•çš„è®­ç»ƒå¾ªç¯</span>
<span class="code-keyword">def</span> <span class="code-function">train_model</span>(model: Transformer, num_epochs: int = <span class="code-number">10</span>):
    <span class="code-string">"""ç®€å•çš„è®­ç»ƒç¤ºä¾‹"""</span>
    <span class="code-comment"># ä¼˜åŒ–å™¨</span>
    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="code-number">0.0001</span>, betas=(<span class="code-number">0.9</span>, <span class="code-number">0.98</span>))

    <span class="code-comment"># æŸå¤±å‡½æ•°ï¼ˆå¿½ç•¥paddingï¼‰</span>
    criterion = nn.CrossEntropyLoss(ignore_index=model.config.pad_idx)

    <span class="code-keyword">for</span> epoch <span class="code-keyword">in</span> range(num_epochs):
        <span class="code-comment"># ç”Ÿæˆéšæœºæ•°æ®ï¼ˆå®é™…åº”ç”¨ä¸­ä½¿ç”¨çœŸå®æ•°æ®ï¼‰</span>
        src = torch.randint(<span class="code-number">1</span>, model.config.vocab_size, (<span class="code-number">32</span>, <span class="code-number">20</span>))
        tgt = torch.randint(<span class="code-number">1</span>, model.config.vocab_size, (<span class="code-number">32</span>, <span class="code-number">15</span>))

        loss = train_step(model, src, tgt, optimizer, criterion)

        <span class="code-keyword">if</span> epoch % <span class="code-number">2</span> == <span class="code-number">0</span>:
            print(<span class="code-string">f"Epoch {epoch}, Loss: {loss:.4f}"</span>)</code></pre>
            </div>
        </div>
    </div>

    <!-- ğŸ“ æ€»ç»“å’Œå±•æœ› -->
    <div class="implementation-section" style="border-color: #8b5cf6; background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(124, 58, 237, 0.05));">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸ“</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">æ€»ç»“ï¼šæˆ‘ä»¬æ„å»ºäº†ä»€ä¹ˆï¼Ÿ</h2>
            </div>
        </div>

        <!-- æ¶æ„æ€»ç»“ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2.5rem; border-radius: 16px; margin-bottom: 2rem;">
            <h3 style="color: #22c55e; margin-bottom: 1.5rem; text-align: center;">âœ¨ Transformeræ¶æ„çš„ç²¾é«“</h3>

            <div style="display: grid; gap: 1.5rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #3b82f6; margin-bottom: 0.8rem;">ğŸ”„ è‡ªæ³¨æ„åŠ›ï¼šå…¨å±€ä¿¡æ¯äº¤äº’</h4>
                    <div style="color: #cbd5e1;">
                        ä¸åŒäºRNNçš„åºåˆ—å¤„ç†ï¼Œè‡ªæ³¨æ„åŠ›è®©æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥ä¸å…¶ä»–æ‰€æœ‰ä½ç½®äº¤äº’ï¼Œ
                        å®ç°äº†çœŸæ­£çš„å…¨å±€ç†è§£ã€‚è¿™ç§æœºåˆ¶ç‰¹åˆ«é€‚åˆæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 0.8rem;">ğŸ¯ ç¼–ç å™¨-è§£ç å™¨ï¼šç†è§£ä¸ç”Ÿæˆ</h4>
                    <div style="color: #cbd5e1;">
                        ç¼–ç å™¨è´Ÿè´£æ·±åº¦ç†è§£è¾“å…¥ï¼Œè§£ç å™¨åŸºäºè¿™ç§ç†è§£ç”Ÿæˆè¾“å‡ºã€‚
                        é€šè¿‡äº¤å‰æ³¨æ„åŠ›ï¼Œè§£ç å™¨èƒ½å¤Ÿåœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶å‚è€ƒæºåºåˆ—çš„æ‰€æœ‰ä¿¡æ¯ã€‚
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 0.8rem;">âš¡ å¹¶è¡ŒåŒ–ï¼šè®­ç»ƒæ•ˆç‡é©å‘½</h4>
                    <div style="color: #cbd5e1;">
                        æŠ›å¼ƒäº†åºåˆ—ä¾èµ–ï¼Œæ‰€æœ‰ä½ç½®å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚é…åˆæ©ç æœºåˆ¶ï¼Œ
                        å³ä½¿æ˜¯è‡ªå›å½’çš„è§£ç å™¨ä¹Ÿèƒ½åœ¨è®­ç»ƒæ—¶å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—ã€‚
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #8b5cf6; margin-bottom: 0.8rem;">ğŸ—ï¸ æ¨¡å—åŒ–è®¾è®¡ï¼šä¼˜é›…å¯æ‰©å±•</h4>
                    <div style="color: #cbd5e1;">
                        æ¯ä¸ªç»„ä»¶ï¼ˆæ³¨æ„åŠ›ã€å‰é¦ˆã€å½’ä¸€åŒ–ï¼‰éƒ½æ˜¯ç‹¬ç«‹çš„æ¨¡å—ï¼Œ
                        é€šè¿‡æ®‹å·®è¿æ¥ç»„åˆã€‚è¿™ç§è®¾è®¡è®©æ¨¡å‹æ˜“äºç†è§£ã€è°ƒè¯•å’Œæ‰©å±•ã€‚
                    </div>
                </div>
            </div>
        </div>

        <!-- å…³é”®åˆ›æ–° -->
        <div style="background: linear-gradient(135deg, rgba(251, 191, 36, 0.15), rgba(245, 158, 11, 0.1)); padding: 2rem; border-radius: 16px; border: 1px solid rgba(251, 191, 36, 0.3); margin-bottom: 2rem;">
            <h3 style="color: #fbbf24; margin-bottom: 1.5rem; text-align: center;">ğŸŒŸ Transformerçš„å…³é”®åˆ›æ–°</h3>

            <div style="color: #f1f5f9; font-size: 1rem; line-height: 1.8;">
                <strong>1. æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºæ ¸å¿ƒï¼š</strong>å®Œå…¨åŸºäºæ³¨æ„åŠ›ï¼Œæ²¡æœ‰å·ç§¯æˆ–å¾ªç¯<br>
                <strong>2. ä½ç½®ç¼–ç ï¼š</strong>å·§å¦™åœ°å°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°æ¨¡å‹ä¸­<br>
                <strong>3. å¤šå¤´è®¾è®¡ï¼š</strong>è®©æ¨¡å‹åŒæ—¶å…³æ³¨ä¸åŒç±»å‹çš„ä¿¡æ¯<br>
                <strong>4. æ©ç æŠ€å·§ï¼š</strong>ä¼˜é›…åœ°å¤„ç†paddingå’Œå› æœå…³ç³»<br>
                <strong>5. ç¼©æ”¾ç‚¹ç§¯ï¼š</strong>ç®€å•å´æœ‰æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹å¼<br><br>

                è¿™äº›åˆ›æ–°å…±åŒé€ å°±äº†ä¸€ä¸ªå¼ºå¤§ã€çµæ´»ã€é«˜æ•ˆçš„æ¶æ„ï¼Œ
                ä¸ä»…åœ¨NLPé¢†åŸŸå–å¾—çªç ´ï¼Œè¿˜å¯å‘äº†è®¡ç®—æœºè§†è§‰ç­‰å…¶ä»–é¢†åŸŸçš„å‘å±•ã€‚
            </div>
        </div>

        <!-- åº”ç”¨å’Œæ‰©å±• -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 16px;">
            <h3 style="color: #3b82f6; margin-bottom: 1.5rem; text-align: center;">ğŸš€ ä»è¿™é‡Œèµ°å‘ä½•æ–¹ï¼Ÿ</h3>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem;">
                <div style="background: rgba(59, 130, 246, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(59, 130, 246, 0.3);">
                    <h4 style="color: #3b82f6; margin-bottom: 0.8rem;">BERTï¼šåªç”¨ç¼–ç å™¨</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        ä½¿ç”¨åŒå‘ç¼–ç å™¨è¿›è¡Œé¢„è®­ç»ƒï¼Œé©æ–°äº†NLPçš„é¢„è®­ç»ƒèŒƒå¼
                    </div>
                </div>

                <div style="background: rgba(34, 197, 94, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                    <h4 style="color: #22c55e; margin-bottom: 0.8rem;">GPTï¼šåªç”¨è§£ç å™¨</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œå±•ç°äº†æ‰©å±•è§„æ¨¡çš„æƒŠäººæ•ˆæœ
                    </div>
                </div>

                <div style="background: rgba(139, 92, 246, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(139, 92, 246, 0.3);">
                    <h4 style="color: #8b5cf6; margin-bottom: 0.8rem;">Vision Transformer</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        å°†Transformeråº”ç”¨åˆ°è®¡ç®—æœºè§†è§‰ï¼Œæ‰“ç ´CNNçš„å„æ–­
                    </div>
                </div>

                <div style="background: rgba(251, 191, 36, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(251, 191, 36, 0.3);">
                    <h4 style="color: #fbbf24; margin-bottom: 0.8rem;">å¤šæ¨¡æ€æ¨¡å‹</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        CLIPã€DALL-Eç­‰ï¼Œè¿æ¥è§†è§‰å’Œè¯­è¨€çš„æ¡¥æ¢
                    </div>
                </div>
            </div>
        </div>

        <!-- ç»“è¯­ -->
        <div style="background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(147, 51, 234, 0.1)); padding: 3rem; border-radius: 20px; margin-top: 3rem; text-align: center; border: 1px solid rgba(59, 130, 246, 0.3);">
            <h2 style="color: #8b5cf6; margin-bottom: 1.5rem;">ğŸ‰ æ­å–œä½ ï¼</h2>
            <div style="color: #f1f5f9; font-size: 1.1rem; line-height: 1.8;">
                ä½ å·²ç»ä»é›¶å¼€å§‹å®ç°äº†å®Œæ•´çš„Transformerï¼<br><br>

                é€šè¿‡è¿™ä¸ªæ—…ç¨‹ï¼Œä½ ä¸ä»…ç†è§£äº†æ¯ä¸ªç»„ä»¶çš„åŸç†ï¼Œ<br>
                æ›´é‡è¦çš„æ˜¯ç†è§£äº†å®ƒä»¬ä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡ï¼Œ<br>
                ä»¥åŠå¦‚ä½•ååŒå·¥ä½œåˆ›é€ å‡ºè¿™ä¸ªæ”¹å˜ä¸–ç•Œçš„æ¶æ„ã€‚<br><br>

                <strong style="color: #3b82f6;">ç°åœ¨ï¼Œä½ å·²ç»æŒæ¡äº†ç†è§£å’Œæ„å»ºç°ä»£AIç³»ç»Ÿçš„æ ¸å¿ƒçŸ¥è¯†ã€‚</strong><br><br>

                æ— è®ºæ˜¯ä½¿ç”¨ç°æœ‰æ¡†æ¶è¿˜æ˜¯åˆ›é€ æ–°çš„æ¶æ„ï¼Œ<br>
                è¿™äº›çŸ¥è¯†éƒ½å°†æˆä¸ºä½ åœ¨AIé¢†åŸŸæ¢ç´¢çš„åšå®åŸºç¡€ã€‚<br><br>

                <span style="font-size: 1.5rem;">ğŸš€</span><br>
                <strong style="color: #fbbf24;">The transformer revolution continues, and now you're part of it!</strong>
            </div>
        </div>
    </div>

    <!-- å¯¼èˆª -->
    <div style="display: flex; justify-content: space-between; align-items: center; margin-top: 3rem; padding: 2rem; background: rgba(255, 255, 255, 0.05); border-radius: 12px; backdrop-filter: blur(10px);">
        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #22c55e, #16a34a); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="window.history.back()">
            â† ç¬¬9ç« ï¼šæ©ç æœºåˆ¶
        </button>

        <div style="text-align: center; color: #cbd5e1;">
            <strong>ç¬¬10ç« ï¼šå®Œæ•´å®ç°</strong><br>
            <span>ä»é›¶åˆ°ä¸€æ„å»ºTransformer</span>
        </div>

        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #8b5cf6, #9333ea); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="alert('ğŸ‰ æ­å–œå®Œæˆæ•´ä¸ªæ•™ç¨‹ï¼')">
            å®Œæˆå­¦ä¹  âœ¨
        </button>
    </div>
</div>

<script>
    // æ·»åŠ äº¤äº’æ•ˆæœ
    document.addEventListener('DOMContentLoaded', function() {
        // ä»£ç å—åŠ¨ç”»
        const codeBlocks = document.querySelectorAll('.code-implementation');
        codeBlocks.forEach((block, index) => {
            block.style.opacity = '0';
            block.style.transform = 'translateY(20px)';

            setTimeout(() => {
                block.style.transition = 'all 0.6s ease-out';
                block.style.opacity = '1';
                block.style.transform = 'translateY(0)';
            }, index * 100);
        });

        // ç»„ä»¶å¡ç‰‡æ‚¬åœæ•ˆæœ
        const componentCards = document.querySelectorAll('.component-card');
        componentCards.forEach(card => {
            card.addEventListener('mouseenter', function() {
                this.style.transform = 'translateY(-5px)';
                this.style.boxShadow = '0 10px 30px rgba(0, 0, 0, 0.3)';
            });

            card.addEventListener('mouseleave', function() {
                this.style.transform = 'translateY(0)';
                this.style.boxShadow = 'none';
            });
        });

        // ä»£ç å¤åˆ¶åŠŸèƒ½
        const codeHeaders = document.querySelectorAll('.code-header');
        codeHeaders.forEach(header => {
            const copyBtn = document.createElement('button');
            copyBtn.textContent = 'ğŸ“‹ å¤åˆ¶';
            copyBtn.style.cssText = `
                background: rgba(59, 130, 246, 0.2);
                color: #3b82f6;
                border: 1px solid rgba(59, 130, 246, 0.3);
                padding: 0.3rem 0.8rem;
                border-radius: 6px;
                cursor: pointer;
                font-size: 0.85rem;
                transition: all 0.3s ease;
            `;

            copyBtn.addEventListener('click', function() {
                const codeBlock = header.nextElementSibling;
                const code = codeBlock.textContent;

                navigator.clipboard.writeText(code).then(() => {
                    copyBtn.textContent = 'âœ… å·²å¤åˆ¶';
                    setTimeout(() => {
                        copyBtn.textContent = 'ğŸ“‹ å¤åˆ¶';
                    }, 2000);
                });
            });

            copyBtn.addEventListener('mouseenter', function() {
                this.style.background = 'rgba(59, 130, 246, 0.3)';
            });

            copyBtn.addEventListener('mouseleave', function() {
                this.style.background = 'rgba(59, 130, 246, 0.2)';
            });

            header.appendChild(copyBtn);
        });

        // å®ç°æ­¥éª¤åŠ¨ç”»
        const steps = document.querySelectorAll('.implementation-step');
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateX(0)';
                }
            });
        }, { threshold: 0.1 });

        steps.forEach(step => {
            step.style.opacity = '0';
            step.style.transform = 'translateX(-20px)';
            step.style.transition = 'all 0.6s ease-out';
            observer.observe(step);
        });

        // æ¶æ„æµç¨‹åŠ¨ç”»
        const architectureFlow = document.querySelector('.architecture-flow');
        if (architectureFlow) {
            const cards = architectureFlow.querySelectorAll('.component-card');
            cards.forEach((card, index) => {
                card.style.opacity = '0';
                card.style.transform = 'scale(0.9)';

                setTimeout(() => {
                    card.style.transition = 'all 0.5s ease-out';
                    card.style.opacity = '1';
                    card.style.transform = 'scale(1)';
                }, index * 200);
            });
        }

        // æ´å¯Ÿå¡ç‰‡åŠ¨ç”»
        const insightCards = document.querySelectorAll('.insight-card');
        insightCards.forEach(card => {
            card.addEventListener('mouseenter', function() {
                this.style.transform = 'translateX(5px)';
                this.style.boxShadow = '0 5px 20px rgba(251, 191, 36, 0.2)';
            });

            card.addEventListener('mouseleave', function() {
                this.style.transform = 'translateX(0)';
                this.style.boxShadow = 'none';
            });
        });

        // æµ‹è¯•è¾“å‡ºæ‰“å­—æœºæ•ˆæœ
        const testOutputs = document.querySelectorAll('.test-output pre');
        testOutputs.forEach(output => {
            const text = output.textContent;
            output.textContent = '';
            output.style.visibility = 'visible';

            let index = 0;
            const typeInterval = setInterval(() => {
                if (index < text.length) {
                    output.textContent += text[index];
                    index++;
                } else {
                    clearInterval(typeInterval);
                }
            }, 10);
        });
    });
</script>
</body>
</html>