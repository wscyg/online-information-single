<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç¬¬12ç« ï¼šTransformerä¼˜åŒ– - è®©æ¨¡å‹é£èµ·æ¥</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&display=swap');

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
            color: #e2e8f0;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        /* åŠ¨ç”»æ•ˆæœ */
        @keyframes gradient-shift {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-20px); }
        }

        @keyframes pulse-glow {
            0%, 100% {
                box-shadow: 0 0 20px rgba(251, 191, 36, 0.5);
            }
            50% {
                box-shadow: 0 0 40px rgba(251, 191, 36, 0.8);
            }
        }

        @keyframes slide-in {
            0% {
                opacity: 0;
                transform: translateX(-30px);
            }
            100% {
                opacity: 1;
                transform: translateX(0);
            }
        }

        /* å®¹å™¨æ ·å¼ */
        .chapter-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* ç« èŠ‚å¤´éƒ¨ */
        .chapter-header {
            background: linear-gradient(135deg, rgba(251, 191, 36, 0.2), rgba(245, 158, 11, 0.1));
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
            border: 1px solid rgba(251, 191, 36, 0.3);
            text-align: center;
        }

        .chapter-header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(251, 191, 36, 0.1) 0%, transparent 70%);
            animation: pulse-glow 4s ease-in-out infinite;
        }

        .chapter-header h1 {
            font-size: 2.8rem;
            margin-bottom: 1.5rem;
            background: linear-gradient(45deg, #fbbf24, #f59e0b, #f97316);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            position: relative;
            z-index: 2;
        }

        .chapter-header p {
            font-size: 1.3rem;
            color: #cbd5e1;
            margin-bottom: 2rem;
            position: relative;
            z-index: 2;
        }

        .meta-tags {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            position: relative;
            z-index: 2;
        }

        .meta-tag {
            background: rgba(251, 191, 36, 0.2);
            color: #fbbf24;
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            font-weight: bold;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            border: 1px solid rgba(251, 191, 36, 0.4);
        }

        /* ä¸»è¦åŒºå—æ ·å¼ */
        .optimization-section {
            background: linear-gradient(135deg, rgba(15, 23, 42, 0.8), rgba(30, 41, 59, 0.6));
            border-radius: 20px;
            padding: 2.5rem;
            margin: 3rem 0;
            border: 2px solid rgba(251, 191, 36, 0.2);
            position: relative;
            overflow: hidden;
            box-shadow: 0 16px 64px rgba(0, 0, 0, 0.3);
        }

        /* æ€§èƒ½å¯¹æ¯”å¡ç‰‡ */
        .performance-card {
            background: rgba(15, 23, 42, 0.9);
            border: 1px solid rgba(251, 191, 36, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            transition: all 0.3s ease;
        }

        .performance-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(251, 191, 36, 0.2);
        }

        .performance-metric {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 1rem 0;
        }

        .metric-bar {
            flex: 1;
            height: 20px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
            margin: 0 1rem;
            position: relative;
            overflow: hidden;
        }

        .metric-fill {
            height: 100%;
            background: linear-gradient(90deg, #fbbf24, #f59e0b);
            border-radius: 10px;
            transition: width 1s ease-out;
        }

        /* ä»£ç å®ç°æ ·å¼ */
        .code-implementation {
            background: rgba(15, 23, 42, 0.95);
            border: 1px solid rgba(251, 191, 36, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            font-family: 'JetBrains Mono', monospace;
            overflow-x: auto;
            position: relative;
        }

        .code-implementation pre {
            margin: 0;
            color: #e2e8f0;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .code-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 1rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .code-title {
            color: #fbbf24;
            font-weight: bold;
            font-size: 1.1rem;
        }

        .code-filename {
            color: #64748b;
            font-size: 0.9rem;
            background: rgba(255, 255, 255, 0.05);
            padding: 0.3rem 0.8rem;
            border-radius: 6px;
        }

        /* ä»£ç é«˜äº® */
        .code-keyword {
            color: #9333ea;
            font-weight: bold;
        }

        .code-function {
            color: #3b82f6;
            font-weight: bold;
        }

        .code-string {
            color: #fbbf24;
        }

        .code-comment {
            color: #64748b;
            font-style: italic;
        }

        .code-number {
            color: #22c55e;
        }

        .code-class {
            color: #06b6d4;
            font-weight: bold;
        }

        /* ä¼˜åŒ–æŠ€æœ¯å¡ç‰‡ */
        .optimization-card {
            background: rgba(251, 191, 36, 0.1);
            border: 1px solid rgba(251, 191, 36, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            position: relative;
            overflow: hidden;
        }

        .optimization-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, #fbbf24, #f59e0b);
        }

        /* æ­¥éª¤æ ·å¼ */
        .optimization-step {
            background: rgba(15, 23, 42, 0.9);
            border: 1px solid rgba(251, 191, 36, 0.3);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
            position: relative;
        }

        .step-header {
            display: flex;
            align-items: center;
            margin-bottom: 1.5rem;
        }

        .step-number {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
            background: linear-gradient(45deg, #fbbf24, #f59e0b);
            color: #0f172a;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 1rem;
            font-size: 1.2rem;
        }

        .step-title {
            color: #fbbf24;
            font-weight: bold;
            font-size: 1.3rem;
        }

        /* æ¯”è¾ƒè¡¨æ ¼ */
        .comparison-table {
            background: rgba(15, 23, 42, 0.9);
            border-radius: 12px;
            overflow: hidden;
            margin: 2rem 0;
        }

        .comparison-table table {
            width: 100%;
            border-collapse: collapse;
        }

        .comparison-table th {
            background: rgba(251, 191, 36, 0.2);
            color: #fbbf24;
            padding: 1rem;
            text-align: left;
            font-weight: bold;
        }

        .comparison-table td {
            padding: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
            color: #cbd5e1;
        }

        .comparison-table tr:hover {
            background: rgba(255, 255, 255, 0.05);
        }

        /* çŸ¥è¯†ç‚¹å¡ç‰‡ */
        .insight-card {
            background: linear-gradient(135deg, rgba(59, 130, 246, 0.1), rgba(79, 70, 229, 0.05));
            border: 1px solid rgba(59, 130, 246, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .insight-card h4 {
            color: #3b82f6;
            margin-bottom: 0.8rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* è­¦å‘Šå¡ç‰‡ */
        .warning-card {
            background: rgba(239, 68, 68, 0.1);
            border: 1px solid rgba(239, 68, 68, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .warning-card h4 {
            color: #ef4444;
            margin-bottom: 0.8rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* æˆåŠŸå¡ç‰‡ */
        .success-card {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        .success-card h4 {
            color: #22c55e;
            margin-bottom: 0.8rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        /* å“åº”å¼è®¾è®¡ */
        @media (max-width: 768px) {
            .chapter-container {
                padding: 1rem;
            }

            .chapter-header h1 {
                font-size: 2rem;
            }

            .optimization-section {
                padding: 1.5rem;
            }

            .code-implementation {
                padding: 1rem;
                font-size: 0.8rem;
            }

            .comparison-table {
                overflow-x: auto;
            }
        }
    </style>
</head>
<body>

<div class="chapter-container">
    <!-- ç« èŠ‚å¤´éƒ¨ -->
    <div class="chapter-header">
        <h1>ç¬¬12ç« ï¼šTransformerä¼˜åŒ–</h1>
        <p>è®©æ¨¡å‹é£èµ·æ¥ï¼šä»è®­ç»ƒåˆ°æ¨ç†çš„å…¨æ–¹ä½ä¼˜åŒ–</p>
        <div class="meta-tags">
            <span class="meta-tag">
                âš¡ <span>æ€§èƒ½ä¼˜åŒ–</span>
            </span>
            <span class="meta-tag">
                â±ï¸ <span>120åˆ†é’Ÿ</span>
            </span>
            <span class="meta-tag">
                ğŸš€ <span>è¿›é˜¶æŠ€æœ¯</span>
            </span>
            <span class="meta-tag">
                ğŸ’¡ <span>å®æˆ˜æŠ€å·§</span>
            </span>
        </div>
    </div>

    <!-- ğŸŒŸ å¼€ç¯‡ï¼šä¸ºä»€ä¹ˆéœ€è¦ä¼˜åŒ– -->
    <div class="optimization-section" style="border-color: #fbbf24;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(251, 191, 36, 0.2), rgba(245, 158, 11, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(251, 191, 36, 0.4);">
                <span style="font-size: 2rem;">ğŸŒŸ</span>
                <h2 style="color: #fbbf24; margin: 0; font-size: 1.8rem; font-weight: bold;">ä¸€ä¸ªä»¤äººéœ‡æƒŠçš„äº‹å®</h2>
            </div>
        </div>

        <!-- å¼•è¨€æ•…äº‹ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">ğŸ“– ä»GPT-3åˆ°ChatGPTçš„ä¼˜åŒ–ä¹‹è·¯</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                    2020å¹´ï¼ŒGPT-3éœ‡æ’¼å‘å¸ƒï¼Œ1750äº¿å‚æ•°è®©äººæƒŠå¹ã€‚<br>
                    ä½†ä½ çŸ¥é“å—ï¼Ÿå¦‚æœæ²¡æœ‰ä¼˜åŒ–æŠ€æœ¯ï¼š<br><br>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1.5rem 0;">
                        <div style="background: rgba(239, 68, 68, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                            <strong style="color: #ef4444;">è®­ç»ƒæˆæœ¬</strong><br>
                            éœ€è¦3000ä¸‡ç¾å…ƒï¼ˆè€Œé450ä¸‡ï¼‰
                        </div>
                        <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #fbbf24;">
                            <strong style="color: #fbbf24;">æ¨ç†å»¶è¿Ÿ</strong><br>
                            ç”Ÿæˆä¸€ä¸ªè¯éœ€è¦10ç§’ï¼ˆè€Œé0.1ç§’ï¼‰
                        </div>
                        <div style="background: rgba(139, 92, 246, 0.1); padding: 1rem; border-radius: 8px; border-left: 4px solid #8b5cf6;">
                            <strong style="color: #8b5cf6;">å†…å­˜å ç”¨</strong><br>
                            éœ€è¦700GBæ˜¾å­˜ï¼ˆè€Œéå¤šå¡40GBï¼‰
                        </div>
                    </div>

                    <div style="margin-top: 1.5rem; padding: 1rem; background: rgba(34, 197, 94, 0.1); border-radius: 8px;">
                        <strong style="color: #22c55e;">ä¼˜åŒ–çš„é­”åŠ›ï¼š</strong><br>
                        æ­£æ˜¯å„ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œè®©Transformerä»å®éªŒå®¤èµ°å‘äº†æ•°åäº¿ç”¨æˆ·ï¼
                    </div>
                </div>
            </div>
        </div>

        <!-- ä¼˜åŒ–é¢†åŸŸæ¦‚è§ˆ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px;">
            <div style="color: #3b82f6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">ğŸ¯ Transformerä¼˜åŒ–çš„å››å¤§æˆ˜åœº</div>

            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1.5rem;">
                <div class="optimization-card">
                    <h3 style="color: #fbbf24; margin-bottom: 1rem;">âš¡ è®­ç»ƒä¼˜åŒ–</h3>
                    <div style="color: #cbd5e1;">
                        â€¢ å­¦ä¹ ç‡è°ƒåº¦ï¼šæ‰¾åˆ°æœ€ä½³å­¦ä¹ è½¨è¿¹<br>
                        â€¢ æ··åˆç²¾åº¦ï¼šFP16/BF16åŠ é€Ÿ<br>
                        â€¢ æ¢¯åº¦ç´¯ç§¯ï¼šçªç ´æ˜¾å­˜é™åˆ¶<br>
                        â€¢ ä¼˜åŒ–å™¨æ”¹è¿›ï¼šAdam â†’ AdamW â†’ LAMB
                    </div>
                </div>

                <div class="optimization-card">
                    <h3 style="color: #22c55e; margin-bottom: 1rem;">ğŸš€ æ¨ç†ä¼˜åŒ–</h3>
                    <div style="color: #cbd5e1;">
                        â€¢ KV Cacheï¼šé¿å…é‡å¤è®¡ç®—<br>
                        â€¢ é‡åŒ–æŠ€æœ¯ï¼šINT8/INT4å‹ç¼©<br>
                        â€¢ æ¨¡å‹å‰ªæï¼šç§»é™¤å†—ä½™å‚æ•°<br>
                        â€¢ çŸ¥è¯†è’¸é¦ï¼šå°æ¨¡å‹å¤§æ™ºæ…§
                    </div>
                </div>

                <div class="optimization-card">
                    <h3 style="color: #3b82f6; margin-bottom: 1rem;">ğŸ—ï¸ æ¶æ„ä¼˜åŒ–</h3>
                    <div style="color: #cbd5e1;">
                        â€¢ Flash Attentionï¼šå†…å­˜é«˜æ•ˆæ³¨æ„åŠ›<br>
                        â€¢ Sparse Attentionï¼šé™ä½å¤æ‚åº¦<br>
                        â€¢ Multi-Query Attentionï¼šå…±äº«KV<br>
                        â€¢ RoPEï¼šæ›´å¥½çš„ä½ç½®ç¼–ç 
                    </div>
                </div>

                <div class="optimization-card">
                    <h3 style="color: #8b5cf6; margin-bottom: 1rem;">ğŸ’» å·¥ç¨‹ä¼˜åŒ–</h3>
                    <div style="color: #cbd5e1;">
                        â€¢ ç®—å­èåˆï¼šå‡å°‘å†…å­˜è®¿é—®<br>
                        â€¢ å¼ é‡å¹¶è¡Œï¼šå¤šGPUååŒ<br>
                        â€¢ æµæ°´çº¿å¹¶è¡Œï¼šå±‚é—´å¹¶è¡Œ<br>
                        â€¢ ZeROä¼˜åŒ–ï¼šåˆ†å¸ƒå¼è®­ç»ƒ
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- âš¡ Part 1: è®­ç»ƒä¼˜åŒ– -->
    <div class="optimization-section" style="border-color: #ef4444;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(239, 68, 68, 0.2), rgba(220, 38, 38, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(239, 68, 68, 0.4);">
                <span style="font-size: 2rem;">âš¡</span>
                <h2 style="color: #ef4444; margin: 0; font-size: 1.8rem; font-weight: bold;">Part 1: è®­ç»ƒä¼˜åŒ–</h2>
            </div>
        </div>

        <!-- 1.1 å­¦ä¹ ç‡è°ƒåº¦ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">1</div>
                <div class="step-title">å­¦ä¹ ç‡è°ƒåº¦ï¼šè®­ç»ƒçš„èŠ‚å¥å¤§å¸ˆ</div>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ’¡</span> ä¸ºä»€ä¹ˆå­¦ä¹ ç‡è°ƒåº¦å¦‚æ­¤é‡è¦ï¼Ÿ</h4>
                <div style="color: #cbd5e1;">
                    æƒ³è±¡ä½ åœ¨é»‘æš—ä¸­å¯»æ‰¾å®è—ï¼š<br>
                    â€¢ å¼€å§‹æ—¶æ­¥å­è¦å¤§ï¼ˆå¿«é€Ÿæ¢ç´¢ï¼‰<br>
                    â€¢ æ¥è¿‘ç›®æ ‡æ—¶æ­¥å­è¦å°ï¼ˆç²¾ç¡®å®šä½ï¼‰<br>
                    â€¢ Transformerçš„è®­ç»ƒå°±æ˜¯è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹ï¼
                </div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°Transformerçš„å­¦ä¹ ç‡è°ƒåº¦å™¨</div>
                    <div class="code-filename">optimizers/lr_scheduler.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> math
<span class="code-keyword">import</span> torch
<span class="code-keyword">from</span> torch.optim.lr_scheduler <span class="code-keyword">import</span> _LRScheduler

<span class="code-keyword">class</span> <span class="code-class">TransformerLRScheduler</span>(_LRScheduler):
    <span class="code-string">"""Transformerçš„å­¦ä¹ ç‡è°ƒåº¦å™¨

    å®ç°è®ºæ–‡ä¸­çš„"warmup + inverse square root decay"ç­–ç•¥
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 optimizer: torch.optim.Optimizer,
                 d_model: int,
                 warmup_steps: int = <span class="code-number">4000</span>,
                 last_epoch: int = -<span class="code-number">1</span>):
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        super().__init__(optimizer, last_epoch)

    <span class="code-keyword">def</span> <span class="code-function">get_lr</span>(self):
        <span class="code-string">"""è®¡ç®—å½“å‰æ­¥çš„å­¦ä¹ ç‡"""</span>
        step = max(<span class="code-number">1</span>, self.last_epoch)

        <span class="code-comment"># é­”æ³•å…¬å¼ï¼šlr = d_model^(-0.5) * min(step^(-0.5), step * warmup^(-1.5))</span>
        scale = self.d_model ** (-<span class="code-number">0.5</span>)

        <span class="code-keyword">if</span> step < self.warmup_steps:
            <span class="code-comment"># Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿</span>
            lr_scale = scale * step * (self.warmup_steps ** (-<span class="code-number">1.5</span>))
        <span class="code-keyword">else</span>:
            <span class="code-comment"># Decayé˜¶æ®µï¼šå¹³æ–¹æ ¹å€’æ•°è¡°å‡</span>
            lr_scale = scale * (step ** (-<span class="code-number">0.5</span>))

        <span class="code-keyword">return</span> [base_lr * lr_scale <span class="code-keyword">for</span> base_lr <span class="code-keyword">in</span> self.base_lrs]

<span class="code-comment"># å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–</span>
<span class="code-keyword">def</span> <span class="code-function">visualize_lr_schedule</span>(d_model=<span class="code-number">512</span>, warmup_steps=<span class="code-number">4000</span>, total_steps=<span class="code-number">50000</span>):
    <span class="code-string">"""å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦"""</span>
    <span class="code-keyword">import</span> matplotlib.pyplot <span class="code-keyword">as</span> plt

    steps = list(range(<span class="code-number">1</span>, total_steps))
    lrs = []

    <span class="code-keyword">for</span> step <span class="code-keyword">in</span> steps:
        scale = d_model ** (-<span class="code-number">0.5</span>)
        <span class="code-keyword">if</span> step < warmup_steps:
            lr = scale * step * (warmup_steps ** (-<span class="code-number">1.5</span>))
        <span class="code-keyword">else</span>:
            lr = scale * (step ** (-<span class="code-number">0.5</span>))
        lrs.append(lr)

    plt.figure(figsize=(<span class="code-number">10</span>, <span class="code-number">6</span>))
    plt.plot(steps, lrs, <span class="code-string">'b-'</span>, linewidth=<span class="code-number">2</span>)
    plt.axvline(x=warmup_steps, color=<span class="code-string">'r'</span>, linestyle=<span class="code-string">'--'</span>,
                label=<span class="code-string">f'Warmup ends (step {warmup_steps})'</span>)
    plt.xlabel(<span class="code-string">'Training Steps'</span>)
    plt.ylabel(<span class="code-string">'Learning Rate'</span>)
    plt.title(<span class="code-string">'Transformer Learning Rate Schedule'</span>)
    plt.legend()
    plt.grid(True, alpha=<span class="code-number">0.3</span>)
    plt.show()

<span class="code-comment"># é«˜çº§ç‰ˆæœ¬ï¼šä½™å¼¦é€€ç« + é‡å¯</span>
<span class="code-keyword">class</span> <span class="code-class">CosineAnnealingWarmupRestarts</span>(_LRScheduler):
    <span class="code-string">"""å¸¦warmupå’Œé‡å¯çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨

    ç°ä»£å¤§æ¨¡å‹è®­ç»ƒçš„æµè¡Œé€‰æ‹©
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 optimizer: torch.optim.Optimizer,
                 first_cycle_steps: int,
                 cycle_mult: float = <span class="code-number">1.0</span>,
                 max_lr: float = <span class="code-number">0.1</span>,
                 min_lr: float = <span class="code-number">0.001</span>,
                 warmup_steps: int = <span class="code-number">0</span>,
                 gamma: float = <span class="code-number">1.0</span>,
                 last_epoch: int = -<span class="code-number">1</span>):

        self.first_cycle_steps = first_cycle_steps
        self.cycle_mult = cycle_mult
        self.max_lr = max_lr
        self.min_lr = min_lr
        self.warmup_steps = warmup_steps
        self.gamma = gamma

        self.cur_cycle_steps = first_cycle_steps
        self.cycle = <span class="code-number">0</span>
        self.step_in_cycle = last_epoch

        super().__init__(optimizer, last_epoch)

        <span class="code-comment"># åˆå§‹åŒ–å­¦ä¹ ç‡</span>
        self.init_lr()

    <span class="code-keyword">def</span> <span class="code-function">init_lr</span>(self):
        self.base_lrs = []
        <span class="code-keyword">for</span> param_group <span class="code-keyword">in</span> self.optimizer.param_groups:
            param_group[<span class="code-string">'lr'</span>] = self.min_lr
            self.base_lrs.append(self.min_lr)

    <span class="code-keyword">def</span> <span class="code-function">get_lr</span>(self):
        <span class="code-keyword">if</span> self.step_in_cycle == -<span class="code-number">1</span>:
            <span class="code-keyword">return</span> self.base_lrs

        <span class="code-keyword">if</span> self.step_in_cycle < self.warmup_steps:
            <span class="code-comment"># Warmupé˜¶æ®µ</span>
            lr_scale = self.step_in_cycle / self.warmup_steps
            lr = self.min_lr + (self.max_lr - self.min_lr) * lr_scale
        <span class="code-keyword">else</span>:
            <span class="code-comment"># ä½™å¼¦é€€ç«é˜¶æ®µ</span>
            progress = (self.step_in_cycle - self.warmup_steps) / \
                      (self.cur_cycle_steps - self.warmup_steps)
            lr = self.min_lr + (self.max_lr - self.min_lr) * \
                 <span class="code-number">0.5</span> * (<span class="code-number">1.0</span> + math.cos(math.pi * progress))

        <span class="code-keyword">return</span> [lr <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> self.base_lrs]</code></pre>
            </div>

            <div class="performance-card">
                <h4 style="color: #22c55e; margin-bottom: 1rem;">ğŸ“Š ä¸åŒå­¦ä¹ ç‡ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”</h4>
                <div class="performance-metric">
                    <span style="color: #cbd5e1; min-width: 150px;">å›ºå®šå­¦ä¹ ç‡</span>
                    <div class="metric-bar">
                        <div class="metric-fill" style="width: 60%;"></div>
                    </div>
                    <span style="color: #fbbf24; min-width: 80px;">60% å‡†ç¡®ç‡</span>
                </div>
                <div class="performance-metric">
                    <span style="color: #cbd5e1; min-width: 150px;">åŸå§‹Transformer</span>
                    <div class="metric-bar">
                        <div class="metric-fill" style="width: 85%;"></div>
                    </div>
                    <span style="color: #fbbf24; min-width: 80px;">85% å‡†ç¡®ç‡</span>
                </div>
                <div class="performance-metric">
                    <span style="color: #cbd5e1; min-width: 150px;">ä½™å¼¦é€€ç«+é‡å¯</span>
                    <div class="metric-bar">
                        <div class="metric-fill" style="width: 92%;"></div>
                    </div>
                    <span style="color: #fbbf24; min-width: 80px;">92% å‡†ç¡®ç‡</span>
                </div>
            </div>
        </div>

        <!-- 1.2 æ··åˆç²¾åº¦è®­ç»ƒ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">2</div>
                <div class="step-title">æ··åˆç²¾åº¦è®­ç»ƒï¼šé€Ÿåº¦ä¸ç²¾åº¦çš„å®Œç¾å¹³è¡¡</div>
            </div>

            <div class="warning-card">
                <h4><span>âš ï¸</span> ä¼ ç»Ÿå›°å¢ƒ</h4>
                <div style="color: #cbd5e1;">
                    â€¢ FP32ï¼šç²¾åº¦é«˜ä½†é€Ÿåº¦æ…¢ã€æ˜¾å­˜å ç”¨å¤§<br>
                    â€¢ FP16ï¼šé€Ÿåº¦å¿«ä½†å®¹æ˜“æ•°å€¼æº¢å‡ºã€æ¢¯åº¦æ¶ˆå¤±<br>
                    â€¢ è§£å†³æ–¹æ¡ˆï¼šæ··åˆç²¾åº¦è®­ç»ƒï¼
                </div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ</div>
                    <div class="code-filename">training/mixed_precision.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">from</span> torch.cuda.amp <span class="code-keyword">import</span> autocast, GradScaler
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Dict, Optional

<span class="code-keyword">class</span> <span class="code-class">MixedPrecisionTrainer</span>:
    <span class="code-string">"""æ··åˆç²¾åº¦è®­ç»ƒå™¨

    è‡ªåŠ¨å¤„ç†FP16/FP32è½¬æ¢ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 model: torch.nn.Module,
                 optimizer: torch.optim.Optimizer,
                 loss_scale: str = <span class="code-string">'dynamic'</span>,
                 init_scale: float = <span class="code-number">2.**16</span>,
                 growth_factor: float = <span class="code-number">2.0</span>,
                 backoff_factor: float = <span class="code-number">0.5</span>,
                 growth_interval: int = <span class="code-number">2000</span>,
                 enabled: bool = True):

        self.model = model
        self.optimizer = optimizer
        self.enabled = enabled <span class="code-keyword">and</span> torch.cuda.is_available()

        <span class="code-comment"># åˆå§‹åŒ–æ¢¯åº¦ç¼©æ”¾å™¨</span>
        <span class="code-keyword">if</span> self.enabled:
            self.scaler = GradScaler(
                init_scale=init_scale,
                growth_factor=growth_factor,
                backoff_factor=backoff_factor,
                growth_interval=growth_interval,
                enabled=True
            )
        <span class="code-keyword">else</span>:
            self.scaler = None

        <span class="code-comment"># ç»Ÿè®¡ä¿¡æ¯</span>
        self.stats = {
            <span class="code-string">'scale_updates'</span>: <span class="code-number">0</span>,
            <span class="code-string">'overflow_count'</span>: <span class="code-number">0</span>,
            <span class="code-string">'successful_steps'</span>: <span class="code-number">0</span>
        }

    <span class="code-keyword">def</span> <span class="code-function">train_step</span>(self,
                    inputs: Dict[str, torch.Tensor],
                    targets: torch.Tensor,
                    loss_fn: torch.nn.Module) -> Dict[str, float]:
        <span class="code-string">"""æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤"""</span>

        self.optimizer.zero_grad()

        <span class="code-keyword">if</span> self.enabled:
            <span class="code-comment"># ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦</span>
            <span class="code-keyword">with</span> autocast():
                <span class="code-comment"># å‰å‘ä¼ æ’­ï¼ˆFP16ï¼‰</span>
                outputs = self.model(**inputs)
                loss = loss_fn(outputs, targets)

            <span class="code-comment"># åå‘ä¼ æ’­ï¼ˆç¼©æ”¾æ¢¯åº¦ï¼‰</span>
            self.scaler.scale(loss).backward()

            <span class="code-comment"># æ¢¯åº¦è£å‰ªï¼ˆåœ¨FP32ç©ºé—´ï¼‰</span>
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=<span class="code-number">1.0</span>)

            <span class="code-comment"># ä¼˜åŒ–å™¨æ­¥éª¤</span>
            self.scaler.step(self.optimizer)

            <span class="code-comment"># æ›´æ–°ç¼©æ”¾å› å­</span>
            scale_before = self.scaler.get_scale()
            self.scaler.update()
            scale_after = self.scaler.get_scale()

            <span class="code-comment"># æ›´æ–°ç»Ÿè®¡ä¿¡æ¯</span>
            <span class="code-keyword">if</span> scale_before > scale_after:
                self.stats[<span class="code-string">'overflow_count'</span>] += <span class="code-number">1</span>
            <span class="code-keyword">else</span>:
                self.stats[<span class="code-string">'successful_steps'</span>] += <span class="code-number">1</span>

            <span class="code-keyword">if</span> scale_before != scale_after:
                self.stats[<span class="code-string">'scale_updates'</span>] += <span class="code-number">1</span>

        <span class="code-keyword">else</span>:
            <span class="code-comment"># æ ‡å‡†FP32è®­ç»ƒ</span>
            outputs = self.model(**inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=<span class="code-number">1.0</span>)
            self.optimizer.step()

        <span class="code-keyword">return</span> {
            <span class="code-string">'loss'</span>: loss.item(),
            <span class="code-string">'scale'</span>: self.scaler.get_scale() <span class="code-keyword">if</span> self.enabled <span class="code-keyword">else</span> <span class="code-number">1.0</span>,
            <span class="code-string">'overflow_rate'</span>: self.stats[<span class="code-string">'overflow_count'</span>] /
                              max(<span class="code-number">1</span>, self.stats[<span class="code-string">'overflow_count'</span>] +
                                  self.stats[<span class="code-string">'successful_steps'</span>])
        }

<span class="code-comment"># BF16è®­ç»ƒï¼ˆæ›´ç¨³å®šçš„æ›¿ä»£æ–¹æ¡ˆï¼‰</span>
<span class="code-keyword">class</span> <span class="code-class">BFloat16Trainer</span>:
    <span class="code-string">"""BFloat16è®­ç»ƒå™¨

    Googleçš„Brain Float16æ ¼å¼ï¼Œæ›´å¤§çš„æŒ‡æ•°èŒƒå›´ï¼Œæ›´å°‘çš„æº¢å‡ºé—®é¢˜
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, model: torch.nn.Module, optimizer: torch.optim.Optimizer):
        self.model = model
        self.optimizer = optimizer

        <span class="code-comment"># æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ</span>
        <span class="code-keyword">if</span> torch.cuda.is_available() <span class="code-keyword">and</span> torch.cuda.is_bf16_supported():
            self.enabled = True
            <span class="code-comment"># è½¬æ¢æ¨¡å‹åˆ°BF16</span>
            self.model = self.model.to(torch.bfloat16)
        <span class="code-keyword">else</span>:
            self.enabled = False
            print(<span class="code-string">"BFloat16 not supported on this hardware"</span>)

    <span class="code-keyword">def</span> <span class="code-function">train_step</span>(self, inputs, targets, loss_fn):
        <span class="code-string">"""BF16è®­ç»ƒæ­¥éª¤ï¼ˆæ— éœ€æ¢¯åº¦ç¼©æ”¾ï¼‰"""</span>

        self.optimizer.zero_grad()

        <span class="code-keyword">if</span> self.enabled:
            <span class="code-comment"># ç¡®ä¿è¾“å…¥æ˜¯BF16</span>
            inputs = {k: v.to(torch.bfloat16) <span class="code-keyword">for</span> k, v <span class="code-keyword">in</span> inputs.items()}
            targets = targets.to(torch.bfloat16)

        <span class="code-comment"># å‰å‘å’Œåå‘ä¼ æ’­</span>
        outputs = self.model(**inputs)
        loss = loss_fn(outputs, targets)
        loss.backward()

        <span class="code-comment"># æ¢¯åº¦è£å‰ªå’Œä¼˜åŒ–</span>
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=<span class="code-number">1.0</span>)
        self.optimizer.step()

        <span class="code-keyword">return</span> {<span class="code-string">'loss'</span>: loss.item()}</code></pre>
            </div>

            <div class="success-card">
                <h4><span>âœ…</span> æ··åˆç²¾åº¦è®­ç»ƒçš„æ”¶ç›Š</h4>
                <div style="color: #cbd5e1;">
                    â€¢ <strong>é€Ÿåº¦æå‡ï¼š</strong>2-3å€è®­ç»ƒåŠ é€Ÿï¼ˆTensor CoreåŠ æŒï¼‰<br>
                    â€¢ <strong>æ˜¾å­˜èŠ‚çœï¼š</strong>å‡å°‘50%æ˜¾å­˜å ç”¨<br>
                    â€¢ <strong>ç²¾åº¦ä¿æŒï¼š</strong>ä¸FP32å‡ ä¹ç›¸åŒçš„æ¨¡å‹è´¨é‡<br>
                    â€¢ <strong>æ›´å¤§æ‰¹æ¬¡ï¼š</strong>å¯ä»¥ä½¿ç”¨æ›´å¤§çš„batch size
                </div>
            </div>
        </div>

        <!-- 1.3 æ¢¯åº¦ç´¯ç§¯ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">3</div>
                <div class="step-title">æ¢¯åº¦ç´¯ç§¯ï¼šçªç ´æ˜¾å­˜é™åˆ¶</div>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ’¡</span> æ¢¯åº¦ç´¯ç§¯çš„å¦™å¤„</h4>
                <div style="color: #cbd5e1;">
                    å½“ä½ æƒ³ç”¨batch_size=128è®­ç»ƒï¼Œä½†æ˜¾å­˜åªå¤Ÿbatch_size=16æ—¶ï¼š<br>
                    â€¢ ç´¯ç§¯8ä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦<br>
                    â€¢ ç­‰æ•ˆäºä¸€ä¸ªå¤§æ‰¹æ¬¡çš„æ•ˆæœ<br>
                    â€¢ çªç ´ç¡¬ä»¶é™åˆ¶ï¼
                </div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°æ¢¯åº¦ç´¯ç§¯è®­ç»ƒ</div>
                    <div class="code-filename">training/gradient_accumulation.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">GradientAccumulationTrainer</span>:
    <span class="code-string">"""æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨

    æ¨¡æ‹Ÿå¤§batchè®­ç»ƒï¼Œé€‚åˆæ˜¾å­˜å—é™çš„åœºæ™¯
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 model: torch.nn.Module,
                 optimizer: torch.optim.Optimizer,
                 accumulation_steps: int = <span class="code-number">4</span>,
                 mixed_precision: bool = True,
                 max_grad_norm: float = <span class="code-number">1.0</span>):

        self.model = model
        self.optimizer = optimizer
        self.accumulation_steps = accumulation_steps
        self.max_grad_norm = max_grad_norm

        <span class="code-comment"># æ··åˆç²¾åº¦æ”¯æŒ</span>
        self.mixed_precision = mixed_precision <span class="code-keyword">and</span> torch.cuda.is_available()
        <span class="code-keyword">if</span> self.mixed_precision:
            self.scaler = GradScaler()

        <span class="code-comment"># å†…éƒ¨è®¡æ•°å™¨</span>
        self.step_count = <span class="code-number">0</span>
        self.accumulated_loss = <span class="code-number">0.0</span>

    <span class="code-keyword">def</span> <span class="code-function">train_step</span>(self, batch_data, batch_targets, loss_fn):
        <span class="code-string">"""æ‰§è¡Œä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤"""</span>

        <span class="code-comment"># ç¼©æ”¾æŸå¤±ï¼ˆå¹³å‡åˆ°accumulation_stepsï¼‰</span>
        <span class="code-keyword">if</span> self.mixed_precision:
            <span class="code-keyword">with</span> autocast():
                outputs = self.model(batch_data)
                loss = loss_fn(outputs, batch_targets)
                loss = loss / self.accumulation_steps

            <span class="code-comment"># ç¼©æ”¾åå‘ä¼ æ’­</span>
            self.scaler.scale(loss).backward()
        <span class="code-keyword">else</span>:
            outputs = self.model(batch_data)
            loss = loss_fn(outputs, batch_targets)
            loss = loss / self.accumulation_steps
            loss.backward()

        self.accumulated_loss += loss.item()
        self.step_count += <span class="code-number">1</span>

        <span class="code-comment"># æ˜¯å¦åˆ°äº†æ›´æ–°å‚æ•°çš„æ—¶å€™</span>
        <span class="code-keyword">if</span> self.step_count % self.accumulation_steps == <span class="code-number">0</span>:
            <span class="code-keyword">if</span> self.mixed_precision:
                <span class="code-comment"># åç¼©æ”¾æ¢¯åº¦</span>
                self.scaler.unscale_(self.optimizer)

                <span class="code-comment"># æ¢¯åº¦è£å‰ª</span>
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )

                <span class="code-comment"># ä¼˜åŒ–å™¨æ­¥éª¤</span>
                self.scaler.step(self.optimizer)
                self.scaler.update()
            <span class="code-keyword">else</span>:
                <span class="code-comment"># æ¢¯åº¦è£å‰ª</span>
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.max_grad_norm
                )

                <span class="code-comment"># ä¼˜åŒ–å™¨æ­¥éª¤</span>
                self.optimizer.step()

            <span class="code-comment"># æ¸…é›¶æ¢¯åº¦</span>
            self.optimizer.zero_grad()

            <span class="code-comment"># è¿”å›ç´¯ç§¯çš„æŸå¤±</span>
            avg_loss = self.accumulated_loss
            self.accumulated_loss = <span class="code-number">0.0</span>

            <span class="code-keyword">return</span> {
                <span class="code-string">'loss'</span>: avg_loss * self.accumulation_steps,
                <span class="code-string">'updated'</span>: True
            }

        <span class="code-keyword">return</span> {
            <span class="code-string">'loss'</span>: loss.item() * self.accumulation_steps,
            <span class="code-string">'updated'</span>: False
        }

<span class="code-comment"># é«˜çº§ï¼šåŠ¨æ€æ¢¯åº¦ç´¯ç§¯ï¼ˆæ ¹æ®æ˜¾å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´ï¼‰</span>
<span class="code-keyword">class</span> <span class="code-class">DynamicGradientAccumulation</span>:
    <span class="code-string">"""åŠ¨æ€è°ƒæ•´ç´¯ç§¯æ­¥æ•°ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 model: torch.nn.Module,
                 optimizer: torch.optim.Optimizer,
                 target_batch_size: int = <span class="code-number">128</span>,
                 min_accumulation: int = <span class="code-number">1</span>,
                 max_accumulation: int = <span class="code-number">32</span>):

        self.model = model
        self.optimizer = optimizer
        self.target_batch_size = target_batch_size
        self.min_accumulation = min_accumulation
        self.max_accumulation = max_accumulation

        <span class="code-comment"># è‡ªåŠ¨æ¢æµ‹æœ€ä½³ç´¯ç§¯æ­¥æ•°</span>
        self.accumulation_steps = self._find_optimal_accumulation()

    <span class="code-keyword">def</span> <span class="code-function">_find_optimal_accumulation</span>(self):
        <span class="code-string">"""é€šè¿‡äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°æœ€å¤§å¯ç”¨çš„æ‰¹æ¬¡å¤§å°"""</span>

        <span class="code-keyword">if</span> <span class="code-keyword">not</span> torch.cuda.is_available():
            <span class="code-keyword">return</span> self.min_accumulation

        <span class="code-comment"># åˆ›å»ºdummyè¾“å…¥</span>
        dummy_batch_size = <span class="code-number">1</span>
        dummy_input = torch.randn(
            dummy_batch_size, <span class="code-number">512</span>, <span class="code-number">512</span>
        ).cuda()

        low, high = self.min_accumulation, self.max_accumulation
        best_accumulation = self.min_accumulation

        <span class="code-keyword">while</span> low <= high:
            mid = (low + high) // <span class="code-number">2</span>
            effective_batch = self.target_batch_size // mid

            <span class="code-keyword">try</span>:
                <span class="code-comment"># æ¸…ç†ç¼“å­˜</span>
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

                <span class="code-comment"># å°è¯•å‰å‘ä¼ æ’­</span>
                test_input = dummy_input.repeat(effective_batch, <span class="code-number">1</span>, <span class="code-number">1</span>)
                <span class="code-keyword">with</span> torch.no_grad():
                    _ = self.model(test_input)

                <span class="code-comment"># æˆåŠŸï¼Œå°è¯•æ›´å°çš„ç´¯ç§¯æ­¥æ•°ï¼ˆæ›´å¤§çš„æ‰¹æ¬¡ï¼‰</span>
                best_accumulation = mid
                high = mid - <span class="code-number">1</span>

            <span class="code-keyword">except</span> RuntimeError <span class="code-keyword">as</span> e:
                <span class="code-keyword">if</span> <span class="code-string">"out of memory"</span> <span class="code-keyword">in</span> str(e):
                    <span class="code-comment"># OOMï¼Œéœ€è¦æ›´å¤šç´¯ç§¯æ­¥æ•°ï¼ˆæ›´å°çš„æ‰¹æ¬¡ï¼‰</span>
                    low = mid + <span class="code-number">1</span>
                <span class="code-keyword">else</span>:
                    <span class="code-keyword">raise</span> e

        print(<span class="code-string">f"Optimal accumulation steps: {best_accumulation}"</span>)
        print(<span class="code-string">f"Effective batch size per step: {self.target_batch_size // best_accumulation}"</span>)

        <span class="code-keyword">return</span> best_accumulation</code></pre>
            </div>
        </div>

        <!-- 1.4 ä¼˜åŒ–å™¨æ”¹è¿› -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">4</div>
                <div class="step-title">ä¼˜åŒ–å™¨æ¼”è¿›ï¼šä»Adamåˆ°LAMB</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°é«˜çº§ä¼˜åŒ–å™¨</div>
                    <div class="code-filename">optimizers/advanced_optimizers.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">from</span> torch.optim <span class="code-keyword">import</span> Optimizer
<span class="code-keyword">import</span> math

<span class="code-keyword">class</span> <span class="code-class">AdamW</span>(Optimizer):
    <span class="code-string">"""AdamWä¼˜åŒ–å™¨ï¼šè§£è€¦æƒé‡è¡°å‡

    ç›¸æ¯”Adamçš„æ”¹è¿›ï¼šæƒé‡è¡°å‡ç›´æ¥åº”ç”¨äºå‚æ•°ï¼Œè€Œéæ¢¯åº¦
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, params, lr=<span class="code-number">1e-3</span>, betas=(<span class="code-number">0.9</span>, <span class="code-number">0.999</span>),
                 eps=<span class="code-number">1e-8</span>, weight_decay=<span class="code-number">0.01</span>):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        super().__init__(params, defaults)

    <span class="code-keyword">def</span> <span class="code-function">step</span>(self, closure=None):
        loss = None
        <span class="code-keyword">if</span> closure <span class="code-keyword">is not None</span>:
            loss = closure()

        <span class="code-keyword">for</span> group <span class="code-keyword">in</span> self.param_groups:
            <span class="code-keyword">for</span> p <span class="code-keyword">in</span> group[<span class="code-string">'params'</span>]:
                <span class="code-keyword">if</span> p.grad <span class="code-keyword">is None</span>:
                    <span class="code-keyword">continue</span>

                grad = p.grad.data
                <span class="code-keyword">if</span> grad.is_sparse:
                    <span class="code-keyword">raise</span> RuntimeError(<span class="code-string">'AdamWä¸æ”¯æŒç¨€ç–æ¢¯åº¦'</span>)

                state = self.state[p]

                <span class="code-comment"># çŠ¶æ€åˆå§‹åŒ–</span>
                <span class="code-keyword">if</span> len(state) == <span class="code-number">0</span>:
                    state[<span class="code-string">'step'</span>] = <span class="code-number">0</span>
                    state[<span class="code-string">'exp_avg'</span>] = torch.zeros_like(p.data)
                    state[<span class="code-string">'exp_avg_sq'</span>] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = state[<span class="code-string">'exp_avg'</span>], state[<span class="code-string">'exp_avg_sq'</span>]
                beta1, beta2 = group[<span class="code-string">'betas'</span>]

                state[<span class="code-string">'step'</span>] += <span class="code-number">1</span>

                <span class="code-comment"># æŒ‡æ•°ç§»åŠ¨å¹³å‡</span>
                exp_avg.mul_(beta1).add_(grad, alpha=<span class="code-number">1</span> - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="code-number">1</span> - beta2)

                <span class="code-comment"># åå·®æ ¡æ­£</span>
                bias_correction1 = <span class="code-number">1</span> - beta1 ** state[<span class="code-string">'step'</span>]
                bias_correction2 = <span class="code-number">1</span> - beta2 ** state[<span class="code-string">'step'</span>]

                <span class="code-comment"># è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡</span>
                denom = exp_avg_sq.sqrt().add_(group[<span class="code-string">'eps'</span>])
                step_size = group[<span class="code-string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1

                <span class="code-comment"># å‚æ•°æ›´æ–°</span>
                p.data.addcdiv_(exp_avg, denom, value=-step_size)

                <span class="code-comment"># è§£è€¦æƒé‡è¡°å‡ï¼ˆå…³é”®åŒºåˆ«ï¼‰</span>
                <span class="code-keyword">if</span> group[<span class="code-string">'weight_decay'</span>] != <span class="code-number">0</span>:
                    p.data.add_(p.data, alpha=-group[<span class="code-string">'lr'</span>] * group[<span class="code-string">'weight_decay'</span>])

        <span class="code-keyword">return</span> loss

<span class="code-keyword">class</span> <span class="code-class">LAMB</span>(Optimizer):
    <span class="code-string">"""Layer-wise Adaptive Moments optimizer for Batch training

    å¤§æ‰¹é‡è®­ç»ƒçš„æ•‘æ˜Ÿï¼Œè‡ªé€‚åº”è°ƒæ•´æ¯å±‚çš„å­¦ä¹ ç‡
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, params, lr=<span class="code-number">1e-3</span>, betas=(<span class="code-number">0.9</span>, <span class="code-number">0.999</span>),
                 eps=<span class="code-number">1e-6</span>, weight_decay=<span class="code-number">0.01</span>, bias_correction=True):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,
                       bias_correction=bias_correction)
        super().__init__(params, defaults)

    <span class="code-keyword">def</span> <span class="code-function">step</span>(self, closure=None):
        loss = None
        <span class="code-keyword">if</span> closure <span class="code-keyword">is not None</span>:
            loss = closure()

        <span class="code-keyword">for</span> group <span class="code-keyword">in</span> self.param_groups:
            <span class="code-keyword">for</span> p <span class="code-keyword">in</span> group[<span class="code-string">'params'</span>]:
                <span class="code-keyword">if</span> p.grad <span class="code-keyword">is None</span>:
                    <span class="code-keyword">continue</span>

                grad = p.grad.data
                <span class="code-keyword">if</span> grad.is_sparse:
                    <span class="code-keyword">raise</span> RuntimeError(<span class="code-string">'LAMBä¸æ”¯æŒç¨€ç–æ¢¯åº¦'</span>)

                state = self.state[p]

                <span class="code-comment"># çŠ¶æ€åˆå§‹åŒ–</span>
                <span class="code-keyword">if</span> len(state) == <span class="code-number">0</span>:
                    state[<span class="code-string">'step'</span>] = <span class="code-number">0</span>
                    state[<span class="code-string">'exp_avg'</span>] = torch.zeros_like(p.data)
                    state[<span class="code-string">'exp_avg_sq'</span>] = torch.zeros_like(p.data)

                exp_avg, exp_avg_sq = state[<span class="code-string">'exp_avg'</span>], state[<span class="code-string">'exp_avg_sq'</span>]
                beta1, beta2 = group[<span class="code-string">'betas'</span>]

                state[<span class="code-string">'step'</span>] += <span class="code-number">1</span>

                <span class="code-comment"># æŒ‡æ•°ç§»åŠ¨å¹³å‡</span>
                exp_avg.mul_(beta1).add_(grad, alpha=<span class="code-number">1</span> - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=<span class="code-number">1</span> - beta2)

                <span class="code-comment"># è®¡ç®—è‡ªé€‚åº”é¡¹</span>
                <span class="code-keyword">if</span> group[<span class="code-string">'bias_correction'</span>]:
                    bias_correction1 = <span class="code-number">1</span> - beta1 ** state[<span class="code-string">'step'</span>]
                    bias_correction2 = <span class="code-number">1</span> - beta2 ** state[<span class="code-string">'step'</span>]
                    exp_avg_hat = exp_avg / bias_correction1
                    exp_avg_sq_hat = exp_avg_sq / bias_correction2
                <span class="code-keyword">else</span>:
                    exp_avg_hat = exp_avg
                    exp_avg_sq_hat = exp_avg_sq

                <span class="code-comment"># Adamæ›´æ–°é¡¹</span>
                update = exp_avg_hat / (exp_avg_sq_hat.sqrt() + group[<span class="code-string">'eps'</span>])

                <span class="code-comment"># L2æ­£åˆ™åŒ–</span>
                <span class="code-keyword">if</span> group[<span class="code-string">'weight_decay'</span>] != <span class="code-number">0</span>:
                    update.add_(p.data, alpha=group[<span class="code-string">'weight_decay'</span>])

                <span class="code-comment"># LAMBçš„å…³é”®ï¼šå±‚è‡ªé€‚åº”å­¦ä¹ ç‡</span>
                <span class="code-comment"># è®¡ç®—å‚æ•°å’Œæ›´æ–°çš„L2èŒƒæ•°</span>
                p_norm = p.data.norm(<span class="code-number">2</span>)
                update_norm = update.norm(<span class="code-number">2</span>)

                <span class="code-comment"># è®¡ç®—è‡ªé€‚åº”å­¦ä¹ ç‡</span>
                <span class="code-keyword">if</span> p_norm > <span class="code-number">0</span> <span class="code-keyword">and</span> update_norm > <span class="code-number">0</span>:
                    trust_ratio = p_norm / update_norm
                <span class="code-keyword">else</span>:
                    trust_ratio = <span class="code-number">1.0</span>

                <span class="code-comment"># åº”ç”¨æ›´æ–°</span>
                p.data.add_(update, alpha=-group[<span class="code-string">'lr'</span>] * trust_ratio)

        <span class="code-keyword">return</span> loss</code></pre>
            </div>

            <div class="comparison-table">
                <h4 style="color: #fbbf24; margin-bottom: 1rem; padding: 1rem;">ğŸ“Š ä¼˜åŒ–å™¨å¯¹æ¯”</h4>
                <table>
                    <thead>
                    <tr>
                        <th>ä¼˜åŒ–å™¨</th>
                        <th>ç‰¹ç‚¹</th>
                        <th>é€‚ç”¨åœºæ™¯</th>
                        <th>æ”¶æ•›é€Ÿåº¦</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td><strong>SGD</strong></td>
                        <td>ç®€å•ã€ç¨³å®š</td>
                        <td>å‡¸ä¼˜åŒ–é—®é¢˜</td>
                        <td>â­â­</td>
                    </tr>
                    <tr>
                        <td><strong>Adam</strong></td>
                        <td>è‡ªé€‚åº”å­¦ä¹ ç‡</td>
                        <td>å¤§å¤šæ•°æ·±åº¦å­¦ä¹ </td>
                        <td>â­â­â­â­</td>
                    </tr>
                    <tr>
                        <td><strong>AdamW</strong></td>
                        <td>è§£è€¦æƒé‡è¡°å‡</td>
                        <td>Transformerè®­ç»ƒ</td>
                        <td>â­â­â­â­â­</td>
                    </tr>
                    <tr>
                        <td><strong>LAMB</strong></td>
                        <td>å±‚è‡ªé€‚åº”</td>
                        <td>å¤§æ‰¹é‡è®­ç»ƒ</td>
                        <td>â­â­â­â­â­</td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>

    <!-- ğŸš€ Part 2: æ¨ç†ä¼˜åŒ– -->
    <div class="optimization-section" style="border-color: #22c55e;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">ğŸš€</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">Part 2: æ¨ç†ä¼˜åŒ–</h2>
            </div>
        </div>

        <!-- 2.1 KV Cache -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">5</div>
                <div class="step-title">KV Cacheï¼šé¿å…é‡å¤è®¡ç®—</div>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ’¡</span> KV Cacheçš„åŸç†</h4>
                <div style="color: #cbd5e1;">
                    åœ¨è‡ªå›å½’ç”Ÿæˆæ—¶ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°è¯éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰ä¹‹å‰è¯çš„æ³¨æ„åŠ›ã€‚<br>
                    KV Cacheç¼“å­˜äº†ä¹‹å‰çš„Keyå’ŒValueï¼Œåªéœ€è®¡ç®—æ–°è¯çš„éƒ¨åˆ†ï¼<br>
                    <strong>æ•ˆæœï¼š</strong>æ¨ç†é€Ÿåº¦æå‡10å€ä»¥ä¸Šï¼
                </div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°KV Cacheä¼˜åŒ–</div>
                    <div class="code-filename">inference/kv_cache.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Optional, Tuple, Dict

<span class="code-keyword">class</span> <span class="code-class">KVCache</span>:
    <span class="code-string">"""Key-Valueç¼“å­˜ç®¡ç†å™¨"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 n_layers: int,
                 max_batch_size: int,
                 max_seq_length: int,
                 n_heads: int,
                 head_dim: int,
                 device: torch.device = torch.device(<span class="code-string">'cuda'</span>)):

        self.n_layers = n_layers
        self.max_batch_size = max_batch_size
        self.max_seq_length = max_seq_length
        self.n_heads = n_heads
        self.head_dim = head_dim
        self.device = device

        <span class="code-comment"># é¢„åˆ†é…ç¼“å­˜ç©ºé—´</span>
        self.k_cache = torch.zeros(
            n_layers, max_batch_size, max_seq_length,
            n_heads, head_dim, device=device
        )
        self.v_cache = torch.zeros(
            n_layers, max_batch_size, max_seq_length,
            n_heads, head_dim, device=device
        )

        <span class="code-comment"># è®°å½•æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦</span>
        self.seq_lengths = torch.zeros(max_batch_size, dtype=torch.long, device=device)

    <span class="code-keyword">def</span> <span class="code-function">update</span>(self,
                layer_idx: int,
                batch_idx: torch.Tensor,
                k: torch.Tensor,
                v: torch.Tensor,
                seq_pos: int):
        <span class="code-string">"""æ›´æ–°ç¼“å­˜"""</span>
        <span class="code-comment"># k, v: [batch_size, n_heads, seq_len, head_dim]</span>
        batch_size, _, seq_len, _ = k.shape

        <span class="code-comment"># æ›´æ–°å¯¹åº”ä½ç½®çš„ç¼“å­˜</span>
        self.k_cache[layer_idx, batch_idx, seq_pos:seq_pos+seq_len] = k.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
        self.v_cache[layer_idx, batch_idx, seq_pos:seq_pos+seq_len] = v.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

        <span class="code-comment"># æ›´æ–°åºåˆ—é•¿åº¦</span>
        self.seq_lengths[batch_idx] = seq_pos + seq_len

    <span class="code-keyword">def</span> <span class="code-function">get</span>(self,
            layer_idx: int,
            batch_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        <span class="code-string">"""è·å–ç¼“å­˜çš„KV"""</span>
        <span class="code-comment"># è·å–æœ‰æ•ˆé•¿åº¦</span>
        max_len = self.seq_lengths[batch_idx].max().item()

        <span class="code-comment"># è¿”å›æœ‰æ•ˆéƒ¨åˆ†</span>
        k = self.k_cache[layer_idx, batch_idx, :max_len].transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
        v = self.v_cache[layer_idx, batch_idx, :max_len].transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

        <span class="code-keyword">return</span> k, v

    <span class="code-keyword">def</span> <span class="code-function">clear</span>(self, batch_idx: Optional[torch.Tensor] = None):
        <span class="code-string">"""æ¸…ç©ºç¼“å­˜"""</span>
        <span class="code-keyword">if</span> batch_idx <span class="code-keyword">is None</span>:
            self.k_cache.zero_()
            self.v_cache.zero_()
            self.seq_lengths.zero_()
        <span class="code-keyword">else</span>:
            self.k_cache[:, batch_idx].zero_()
            self.v_cache[:, batch_idx].zero_()
            self.seq_lengths[batch_idx] = <span class="code-number">0</span>

<span class="code-keyword">class</span> <span class="code-class">CachedMultiHeadAttention</span>(nn.Module):
    <span class="code-string">"""æ”¯æŒKV Cacheçš„å¤šå¤´æ³¨æ„åŠ›"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, n_heads: int, dropout: float = <span class="code-number">0.1</span>):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.scale = self.head_dim ** -<span class="code-number">0.5</span>

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self,
                x: torch.Tensor,
                kv_cache: Optional[KVCache] = None,
                layer_idx: int = <span class="code-number">0</span>,
                use_cache: bool = False) -> Tuple[torch.Tensor, Optional[KVCache]]:
        <span class="code-string">"""
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒKVç¼“å­˜

        Args:
            x: [batch_size, seq_len, d_model]
            kv_cache: KVç¼“å­˜å¯¹è±¡
            layer_idx: å½“å‰å±‚ç´¢å¼•
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        """</span>
        batch_size, seq_len, _ = x.shape

        <span class="code-comment"># è®¡ç®—Q</span>
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
        Q = Q.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)  <span class="code-comment"># [batch, n_heads, seq_len, head_dim]</span>

        <span class="code-keyword">if</span> use_cache <span class="code-keyword">and</span> kv_cache <span class="code-keyword">is not None</span>:
            <span class="code-comment"># æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ç¼“å­˜</span>
            <span class="code-keyword">if</span> seq_len == <span class="code-number">1</span>:  <span class="code-comment"># ç”Ÿæˆæ–°token</span>
                <span class="code-comment"># åªè®¡ç®—å½“å‰ä½ç½®çš„KV</span>
                K = self.W_k(x).view(batch_size, <span class="code-number">1</span>, self.n_heads, self.head_dim)
                V = self.W_v(x).view(batch_size, <span class="code-number">1</span>, self.n_heads, self.head_dim)
                K = K.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
                V = V.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

                <span class="code-comment"># è·å–å½“å‰ä½ç½®</span>
                batch_idx = torch.arange(batch_size, device=x.device)
                seq_pos = kv_cache.seq_lengths[batch_idx][<span class="code-number">0</span>].item()

                <span class="code-comment"># æ›´æ–°ç¼“å­˜</span>
                kv_cache.update(layer_idx, batch_idx, K, V, seq_pos)

                <span class="code-comment"># è·å–å®Œæ•´çš„KVï¼ˆåŒ…æ‹¬å†å²ï¼‰</span>
                K_full, V_full = kv_cache.get(layer_idx, batch_idx)

                <span class="code-comment"># æ³¨æ„åŠ›è®¡ç®—</span>
                scores = torch.matmul(Q, K_full.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) * self.scale
                attn_weights = torch.softmax(scores, dim=-<span class="code-number">1</span>)
                attn_output = torch.matmul(attn_weights, V_full)

            <span class="code-keyword">else</span>:  <span class="code-comment"># åˆå§‹åŒ–ï¼ˆç¬¬ä¸€æ¬¡å‰å‘ï¼‰</span>
                K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
                V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
                K = K.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
                V = V.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

                <span class="code-comment"># å­˜å…¥ç¼“å­˜</span>
                batch_idx = torch.arange(batch_size, device=x.device)
                kv_cache.update(layer_idx, batch_idx, K, V, <span class="code-number">0</span>)

                <span class="code-comment"># æ­£å¸¸æ³¨æ„åŠ›è®¡ç®—</span>
                scores = torch.matmul(Q, K.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) * self.scale
                attn_weights = torch.softmax(scores, dim=-<span class="code-number">1</span>)
                attn_output = torch.matmul(attn_weights, V)

        <span class="code-keyword">else</span>:
            <span class="code-comment"># è®­ç»ƒæ¨¡å¼ï¼šä¸ä½¿ç”¨ç¼“å­˜</span>
            K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
            V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
            K = K.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
            V = V.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

            scores = torch.matmul(Q, K.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) * self.scale
            attn_weights = torch.softmax(scores, dim=-<span class="code-number">1</span>)
            attn_weights = self.dropout(attn_weights)
            attn_output = torch.matmul(attn_weights, V)

        <span class="code-comment"># é‡å¡‘å¹¶è¾“å‡ºæŠ•å½±</span>
        attn_output = attn_output.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.d_model)
        output = self.W_o(attn_output)

        <span class="code-keyword">return</span> output, kv_cache

<span class="code-comment"># æ¨ç†ç¤ºä¾‹</span>
<span class="code-keyword">def</span> <span class="code-function">cached_inference</span>(model, input_ids, max_length=<span class="code-number">100</span>):
    <span class="code-string">"""ä½¿ç”¨KV Cacheçš„æ¨ç†"""</span>
    device = input_ids.device
    batch_size = input_ids.shape[<span class="code-number">0</span>]

    <span class="code-comment"># åˆå§‹åŒ–KV Cache</span>
    kv_cache = KVCache(
        n_layers=model.n_layers,
        max_batch_size=batch_size,
        max_seq_length=max_length,
        n_heads=model.n_heads,
        head_dim=model.head_dim,
        device=device
    )

    <span class="code-comment"># å¤„ç†è¾“å…¥åºåˆ—</span>
    outputs = model(input_ids, kv_cache=kv_cache, use_cache=True)

    <span class="code-comment"># ç”Ÿæˆå¾ªç¯</span>
    generated = input_ids
    <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(max_length - input_ids.shape[<span class="code-number">1</span>]):
        <span class="code-comment"># åªå¤„ç†æœ€åä¸€ä¸ªtoken</span>
        last_token = generated[:, -<span class="code-number">1</span>:]
        outputs = model(last_token, kv_cache=kv_cache, use_cache=True)

        <span class="code-comment"># è·å–ä¸‹ä¸€ä¸ªtoken</span>
        next_token = outputs.argmax(dim=-<span class="code-number">1</span>)
        generated = torch.cat([generated, next_token], dim=<span class="code-number">1</span>)

        <span class="code-comment"># æ£€æŸ¥æ˜¯å¦ç»“æŸ</span>
        <span class="code-keyword">if</span> (next_token == model.eos_token_id).all():
            <span class="code-keyword">break</span>

    <span class="code-keyword">return</span> generated</code></pre>
            </div>

            <div class="performance-card">
                <h4 style="color: #22c55e; margin-bottom: 1rem;">ğŸ“Š KV Cacheæ€§èƒ½æå‡</h4>
                <div class="performance-metric">
                    <span style="color: #cbd5e1; min-width: 150px;">æ— ç¼“å­˜</span>
                    <div class="metric-bar">
                        <div class="metric-fill" style="width: 100%; background: #ef4444;"></div>
                    </div>
                    <span style="color: #ef4444; min-width: 80px;">100ms/token</span>
                </div>
                <div class="performance-metric">
                    <span style="color: #cbd5e1; min-width: 150px;">ä½¿ç”¨KV Cache</span>
                    <div class="metric-bar">
                        <div class="metric-fill" style="width: 10%;"></div>
                    </div>
                    <span style="color: #22c55e; min-width: 80px;">10ms/token</span>
                </div>
            </div>
        </div>

        <!-- 2.2 é‡åŒ–æŠ€æœ¯ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">6</div>
                <div class="step-title">é‡åŒ–æŠ€æœ¯ï¼šæ¨¡å‹å‹ç¼©çš„è‰ºæœ¯</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°INT8é‡åŒ–</div>
                    <div class="code-filename">quantization/int8_quantization.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Tuple

<span class="code-keyword">class</span> <span class="code-class">INT8Quantizer</span>:
    <span class="code-string">"""INT8é‡åŒ–å™¨

    å°†FP32/FP16æƒé‡é‡åŒ–ä¸ºINT8ï¼Œå‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿæ¨ç†
    """</span>

    @staticmethod
    <span class="code-keyword">def</span> <span class="code-function">quantize</span>(tensor: torch.Tensor,
                  symmetric: bool = True) -> Tuple[torch.Tensor, float, int]:
        <span class="code-string">"""é‡åŒ–å¼ é‡åˆ°INT8

        Returns:
            quantized_tensor: INT8å¼ é‡
            scale: ç¼©æ”¾å› å­
            zero_point: é›¶ç‚¹ï¼ˆéå¯¹ç§°é‡åŒ–æ—¶ä½¿ç”¨ï¼‰
        """</span>
        <span class="code-keyword">if</span> symmetric:
            <span class="code-comment"># å¯¹ç§°é‡åŒ–ï¼š[-127, 127]</span>
            max_val = tensor.abs().max()
            scale = max_val / <span class="code-number">127.0</span>
            zero_point = <span class="code-number">0</span>

            <span class="code-comment"># é‡åŒ–</span>
            quantized = torch.round(tensor / scale)
            quantized = torch.clamp(quantized, -<span class="code-number">127</span>, <span class="code-number">127</span>).to(torch.int8)
        <span class="code-keyword">else</span>:
            <span class="code-comment"># éå¯¹ç§°é‡åŒ–ï¼š[0, 255]</span>
            min_val = tensor.min()
            max_val = tensor.max()
            scale = (max_val - min_val) / <span class="code-number">255.0</span>
            zero_point = int(-min_val / scale)

            <span class="code-comment"># é‡åŒ–</span>
            quantized = torch.round((tensor - min_val) / scale)
            quantized = torch.clamp(quantized, <span class="code-number">0</span>, <span class="code-number">255</span>).to(torch.uint8)

        <span class="code-keyword">return</span> quantized, scale, zero_point

    @staticmethod
    <span class="code-keyword">def</span> <span class="code-function">dequantize</span>(quantized: torch.Tensor,
                    scale: float,
                    zero_point: int = <span class="code-number">0</span>) -> torch.Tensor:
        <span class="code-string">"""åé‡åŒ–INT8åˆ°æµ®ç‚¹æ•°"""</span>
        <span class="code-keyword">return</span> scale * (quantized.float() - zero_point)

<span class="code-keyword">class</span> <span class="code-class">QuantizedLinear</span>(nn.Module):
    <span class="code-string">"""é‡åŒ–çš„çº¿æ€§å±‚"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 in_features: int,
                 out_features: int,
                 bias: bool = True,
                 quantize_weight: bool = True,
                 quantize_activation: bool = False):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.quantize_weight = quantize_weight
        self.quantize_activation = quantize_activation

        <span class="code-comment"># åˆå§‹åŒ–æƒé‡ï¼ˆå…ˆç”¨FP32ï¼‰</span>
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        <span class="code-keyword">if</span> bias:
            self.bias = nn.Parameter(torch.zeros(out_features))
        <span class="code-keyword">else</span>:
            self.register_parameter(<span class="code-string">'bias'</span>, None)

        <span class="code-comment"># é‡åŒ–å‚æ•°</span>
        self.register_buffer(<span class="code-string">'weight_scale'</span>, torch.tensor(<span class="code-number">1.0</span>))
        self.register_buffer(<span class="code-string">'weight_zero_point'</span>, torch.tensor(<span class="code-number">0</span>))
        self.register_buffer(<span class="code-string">'quantized_weight'</span>, None)

    <span class="code-keyword">def</span> <span class="code-function">quantize_weights</span>(self):
        <span class="code-string">"""é‡åŒ–æƒé‡"""</span>
        <span class="code-keyword">if</span> self.quantize_weight:
            quantized, scale, zero_point = INT8Quantizer.quantize(
                self.weight.data, symmetric=True
            )
            self.quantized_weight = quantized
            self.weight_scale = torch.tensor(scale)
            self.weight_zero_point = torch.tensor(zero_point)

            <span class="code-comment"># é‡Šæ”¾åŸå§‹æƒé‡å†…å­˜</span>
            self.weight = None

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-keyword">if</span> self.quantized_weight <span class="code-keyword">is not None</span>:
            <span class="code-comment"># ä½¿ç”¨é‡åŒ–æƒé‡</span>
            weight = INT8Quantizer.dequantize(
                self.quantized_weight,
                self.weight_scale,
                self.weight_zero_point
            )
        <span class="code-keyword">else</span>:
            weight = self.weight

        <span class="code-keyword">if</span> self.quantize_activation <span class="code-keyword">and</span> <span class="code-keyword">not</span> self.training:
            <span class="code-comment"># é‡åŒ–æ¿€æ´»å€¼ï¼ˆæ¨ç†æ—¶ï¼‰</span>
            x_q, x_scale, x_zp = INT8Quantizer.quantize(x, symmetric=True)
            x = INT8Quantizer.dequantize(x_q, x_scale, x_zp)

        <span class="code-keyword">return</span> F.linear(x, weight, self.bias)

<span class="code-comment"># åŠ¨æ€é‡åŒ–ï¼ˆPyTorchåŸç”Ÿæ”¯æŒï¼‰</span>
<span class="code-keyword">def</span> <span class="code-function">dynamic_quantize_model</span>(model: nn.Module) -> nn.Module:
    <span class="code-string">"""å¯¹æ¨¡å‹è¿›è¡ŒåŠ¨æ€é‡åŒ–"""</span>
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {nn.Linear, nn.LSTM, nn.GRU},  <span class="code-comment"># è¦é‡åŒ–çš„å±‚ç±»å‹</span>
        dtype=torch.qint8
    )
    <span class="code-keyword">return</span> quantized_model

<span class="code-comment"># GPTQé£æ ¼çš„é‡åŒ–ï¼ˆæ›´é«˜çº§ï¼‰</span>
<span class="code-keyword">class</span> <span class="code-class">GPTQQuantizer</span>:
    <span class="code-string">"""åŸºäºHessiançš„å±‚çº§é‡åŒ–"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, bits: int = <span class="code-number">4</span>, group_size: int = <span class="code-number">128</span>):
        self.bits = bits
        self.group_size = group_size
        self.maxq = <span class="code-number">2</span> ** bits - <span class="code-number">1</span>

    <span class="code-keyword">def</span> <span class="code-function">quantize_layer</span>(self,
                        layer: nn.Linear,
                        inp: torch.Tensor,
                        quantize_output: bool = True) -> nn.Module:
        <span class="code-string">"""é‡åŒ–å•ä¸ªå±‚"""</span>
        <span class="code-comment"># æ”¶é›†è¾“å…¥ç»Ÿè®¡</span>
        <span class="code-keyword">with</span> torch.no_grad():
            out = layer(inp)

            <span class="code-comment"># è®¡ç®—Hessianè¿‘ä¼¼</span>
            H = inp.t() @ inp
            H = H.float()

            <span class="code-comment"># æ·»åŠ æ­£åˆ™åŒ–</span>
            dead = torch.diag(H) == <span class="code-number">0</span>
            H[dead, dead] = <span class="code-number">1</span>

            <span class="code-comment"># Choleskyåˆ†è§£</span>
            <span class="code-keyword">try</span>:
                L = torch.linalg.cholesky(H)
            <span class="code-keyword">except</span>:
                <span class="code-comment"># å¦‚æœå¤±è´¥ï¼Œæ·»åŠ æ›´å¤šæ­£åˆ™åŒ–</span>
                H += <span class="code-number">1e-6</span> * torch.eye(H.shape[<span class="code-number">0</span>], device=H.device)
                L = torch.linalg.cholesky(H)

            <span class="code-comment"># é€åˆ—é‡åŒ–æƒé‡</span>
            W = layer.weight.data.clone()
            Q = torch.zeros_like(W)

            <span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(W.shape[<span class="code-number">1</span>]):
                w = W[:, i]
                d = H[i, i]

                <span class="code-keyword">if</span> self.group_size > <span class="code-number">0</span> <span class="code-keyword">and</span> i % self.group_size == <span class="code-number">0</span>:
                    <span class="code-comment"># è®¡ç®—ç»„çš„é‡åŒ–å‚æ•°</span>
                    <span class="code-keyword">if</span> i + self.group_size <= W.shape[<span class="code-number">1</span>]:
                        group = W[:, i:i+self.group_size]
                    <span class="code-keyword">else</span>:
                        group = W[:, i:]

                    scale = group.abs().max() / self.maxq

                <span class="code-comment"># é‡åŒ–</span>
                q = torch.round(w / scale).clamp(-self.maxq, self.maxq)
                Q[:, i] = q

                <span class="code-comment"># æ›´æ–°å‰©ä½™åˆ—ä»¥è¡¥å¿é‡åŒ–è¯¯å·®</span>
                <span class="code-keyword">if</span> i < W.shape[<span class="code-number">1</span>] - <span class="code-number">1</span>:
                    err = (w - q * scale) / d
                    W[:, i+<span class="code-number">1</span>:] -= err.unsqueeze(<span class="code-number">1</span>) @ H[i, i+<span class="code-number">1</span>:].unsqueeze(<span class="code-number">0</span>)

        <span class="code-comment"># åˆ›å»ºé‡åŒ–å±‚</span>
        <span class="code-keyword">return</span> self._create_quantized_layer(Q, scale, layer.bias)</code></pre>
            </div>

            <div class="success-card">
                <h4><span>âœ…</span> é‡åŒ–å¸¦æ¥çš„æ”¶ç›Š</h4>
                <div style="color: #cbd5e1;">
                    â€¢ <strong>å†…å­˜å‡å°‘ï¼š</strong>4å€ï¼ˆINT8ï¼‰åˆ°8å€ï¼ˆINT4ï¼‰<br>
                    â€¢ <strong>æ¨ç†åŠ é€Ÿï¼š</strong>2-4å€ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰<br>
                    â€¢ <strong>ç²¾åº¦æŸå¤±ï¼š</strong>< 1%ï¼ˆåˆç†çš„é‡åŒ–ç­–ç•¥ï¼‰<br>
                    â€¢ <strong>éƒ¨ç½²å‹å¥½ï¼š</strong>å¯åœ¨è¾¹ç¼˜è®¾å¤‡è¿è¡Œå¤§æ¨¡å‹
                </div>
            </div>
        </div>
    </div>

    <!-- ğŸ—ï¸ Part 3: æ¶æ„ä¼˜åŒ– -->
    <div class="optimization-section" style="border-color: #3b82f6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(79, 70, 229, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(59, 130, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸ—ï¸</span>
                <h2 style="color: #3b82f6; margin: 0; font-size: 1.8rem; font-weight: bold;">Part 3: æ¶æ„ä¼˜åŒ–</h2>
            </div>
        </div>

        <!-- 3.1 Flash Attention -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">7</div>
                <div class="step-title">Flash Attentionï¼šå†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›</div>
            </div>

            <div class="insight-card">
                <h4><span>ğŸ’¡</span> Flash Attentionçš„æ ¸å¿ƒæ€æƒ³</h4>
                <div style="color: #cbd5e1;">
                    æ ‡å‡†æ³¨æ„åŠ›éœ€è¦å­˜å‚¨å·¨å¤§çš„æ³¨æ„åŠ›çŸ©é˜µï¼ˆNÃ—Nï¼‰ã€‚<br>
                    Flash Attentioné€šè¿‡åˆ†å—è®¡ç®—å’Œé‡è®¡ç®—ï¼Œå°†å†…å­˜å¤æ‚åº¦ä»O(NÂ²)é™åˆ°O(N)ï¼<br>
                    <strong>å…³é”®ï¼š</strong>èåˆCUDA kernelï¼Œå‡å°‘HBMè®¿é—®
                </div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">Flash Attentionçš„Pythonå®ç°ï¼ˆæ¦‚å¿µæ¼”ç¤ºï¼‰</div>
                    <div class="code-filename">attention/flash_attention.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn.functional <span class="code-keyword">as</span> F
<span class="code-keyword">import</span> math
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Optional

<span class="code-keyword">def</span> <span class="code-function">flash_attention_forward</span>(
    Q: torch.Tensor,  <span class="code-comment"># [batch, heads, seq_len, head_dim]</span>
    K: torch.Tensor,
    V: torch.Tensor,
    block_size: int = <span class="code-number">64</span>,
    causal: bool = False
) -> torch.Tensor:
    <span class="code-string">"""Flash Attentionå‰å‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰

    å®é™…å®ç°éœ€è¦CUDA kernelï¼Œè¿™é‡Œå±•ç¤ºç®—æ³•æ€æƒ³
    """</span>
    batch_size, n_heads, seq_len, head_dim = Q.shape
    scale = <span class="code-number">1.0</span> / math.sqrt(head_dim)

    <span class="code-comment"># è¾“å‡ºåˆå§‹åŒ–</span>
    O = torch.zeros_like(Q)
    L = torch.zeros(batch_size, n_heads, seq_len, device=Q.device)

    <span class="code-comment"># åˆ†å—å¤„ç†</span>
    n_blocks = (seq_len + block_size - <span class="code-number">1</span>) // block_size

    <span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(n_blocks):
        <span class="code-comment"># å½“å‰Qå—çš„èŒƒå›´</span>
        q_start = i * block_size
        q_end = min((i + <span class="code-number">1</span>) * block_size, seq_len)
        Q_block = Q[:, :, q_start:q_end, :]

        <span class="code-comment"># åˆå§‹åŒ–å—è¾“å‡º</span>
        O_block = torch.zeros_like(Q_block)
        L_block = torch.zeros(batch_size, n_heads, q_end - q_start, device=Q.device)

        <span class="code-keyword">for</span> j <span class="code-keyword">in</span> range(n_blocks):
            <span class="code-comment"># å½“å‰KVå—çš„èŒƒå›´</span>
            kv_start = j * block_size
            kv_end = min((j + <span class="code-number">1</span>) * block_size, seq_len)

            <span class="code-comment"># å› æœæ©ç æ£€æŸ¥</span>
            <span class="code-keyword">if</span> causal <span class="code-keyword">and</span> kv_start >= q_end:
                <span class="code-keyword">break</span>

            K_block = K[:, :, kv_start:kv_end, :]
            V_block = V[:, :, kv_start:kv_end, :]

            <span class="code-comment"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span>
            S_block = torch.matmul(Q_block, K_block.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) * scale

            <span class="code-comment"># åº”ç”¨å› æœæ©ç </span>
            <span class="code-keyword">if</span> causal:
                mask = torch.triu(
                    torch.ones(q_end - q_start, kv_end - kv_start, device=Q.device),
                    diagonal=kv_start - q_start + <span class="code-number">1</span>
                )
                S_block.masked_fill_(mask.unsqueeze(<span class="code-number">0</span>).unsqueeze(<span class="code-number">0</span>) == <span class="code-number">1</span>, float(<span class="code-string">'-inf'</span>))

            <span class="code-comment"># è®¡ç®—å±€éƒ¨softmaxï¼ˆæ•°å€¼ç¨³å®šï¼‰</span>
            m_block = S_block.max(dim=-<span class="code-number">1</span>, keepdim=True).values
            P_block = torch.exp(S_block - m_block)
            l_block = P_block.sum(dim=-<span class="code-number">1</span>, keepdim=True)

            <span class="code-comment"># æ›´æ–°å…¨å±€ç»Ÿè®¡é‡ï¼ˆåœ¨çº¿ç®—æ³•ï¼‰</span>
            m_new = torch.maximum(L_block.unsqueeze(-<span class="code-number">1</span>), m_block)
            l1 = torch.exp(L_block.unsqueeze(-<span class="code-number">1</span>) - m_new) * L_block.unsqueeze(-<span class="code-number">1</span>)
            l2 = torch.exp(m_block - m_new) * l_block
            L_block_new = l1 + l2

            <span class="code-comment"># æ›´æ–°è¾“å‡º</span>
            O_block = (l1 * O_block + torch.exp(m_block - m_new) * torch.matmul(P_block, V_block)) / L_block_new
            L_block = L_block_new.squeeze(-<span class="code-number">1</span>)

        <span class="code-comment"># å†™å›å…¨å±€è¾“å‡º</span>
        O[:, :, q_start:q_end, :] = O_block
        L[:, :, q_start:q_end] = L_block

    <span class="code-keyword">return</span> O

<span class="code-comment"># å®é™…ä½¿ç”¨ï¼šè°ƒç”¨ä¼˜åŒ–çš„CUDAå®ç°</span>
<span class="code-keyword">try</span>:
    <span class="code-keyword">from</span> flash_attn <span class="code-keyword">import</span> flash_attn_func

    <span class="code-keyword">class</span> <span class="code-class">FlashAttention</span>(nn.Module):
        <span class="code-string">"""ä½¿ç”¨Flash Attentionçš„æ³¨æ„åŠ›å±‚"""</span>

        <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, n_heads: int, dropout: float = <span class="code-number">0.0</span>):
            super().__init__()
            self.d_model = d_model
            self.n_heads = n_heads
            self.head_dim = d_model // n_heads
            self.dropout = dropout

            self.W_q = nn.Linear(d_model, d_model)
            self.W_k = nn.Linear(d_model, d_model)
            self.W_v = nn.Linear(d_model, d_model)
            self.W_o = nn.Linear(d_model, d_model)

        <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):
            batch_size, seq_len, _ = x.shape

            <span class="code-comment"># QKVæŠ•å½±</span>
            Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
            K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
            V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.head_dim)

            <span class="code-comment"># ä½¿ç”¨Flash Attention</span>
            attn_output = flash_attn_func(
                Q, K, V,
                dropout_p=self.dropout <span class="code-keyword">if</span> self.training <span class="code-keyword">else</span> <span class="code-number">0.0</span>,
                causal=mask <span class="code-keyword">is not None</span>
            )

            <span class="code-comment"># è¾“å‡ºæŠ•å½±</span>
            attn_output = attn_output.view(batch_size, seq_len, self.d_model)
            output = self.W_o(attn_output)

            <span class="code-keyword">return</span> output

<span class="code-keyword">except</span> ImportError:
    print(<span class="code-string">"Flash Attention not available, using standard attention"</span>)</code></pre>
            </div>
        </div>

        <!-- 3.2 Multi-Query Attention -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">8</div>
                <div class="step-title">Multi-Query Attentionï¼šå…±äº«KVçš„æ™ºæ…§</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°Multi-Queryå’ŒGrouped-Query Attention</div>
                    <div class="code-filename">attention/multi_query_attention.py</div>
                </div>
                <pre><code><span class="code-keyword">class</span> <span class="code-class">MultiQueryAttention</span>(nn.Module):
    <span class="code-string">"""Multi-Query Attention (MQA)

    æ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«åŒä¸€ç»„Keyå’ŒValueï¼Œå¤§å¹…å‡å°‘KV Cache
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, n_heads: int):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads

        <span class="code-comment"># Qæœ‰n_headsç»„ï¼Œä½†Kå’ŒVåªæœ‰1ç»„</span>
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, self.head_dim)  <span class="code-comment"># æ³¨æ„ï¼šåªè¾“å‡ºhead_dim</span>
        self.W_v = nn.Linear(d_model, self.head_dim)
        self.W_o = nn.Linear(d_model, d_model)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape

        <span class="code-comment"># è®¡ç®—Qï¼ˆå¤šå¤´ï¼‰</span>
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
        Q = Q.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)  <span class="code-comment"># [batch, n_heads, seq_len, head_dim]</span>

        <span class="code-comment"># è®¡ç®—Kå’ŒVï¼ˆå•å¤´ï¼‰</span>
        K = self.W_k(x)  <span class="code-comment"># [batch, seq_len, head_dim]</span>
        V = self.W_v(x)

        <span class="code-comment"># æ‰©å±•Kå’ŒVåˆ°æ‰€æœ‰å¤´</span>
        K = K.unsqueeze(<span class="code-number">1</span>).expand(-<span class="code-number">1</span>, self.n_heads, -<span class="code-number">1</span>, -<span class="code-number">1</span>)
        V = V.unsqueeze(<span class="code-number">1</span>).expand(-<span class="code-number">1</span>, self.n_heads, -<span class="code-number">1</span>, -<span class="code-number">1</span>)

        <span class="code-comment"># æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—</span>
        scores = torch.matmul(Q, K.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(scores, dim=-<span class="code-number">1</span>)
        attn_output = torch.matmul(attn_weights, V)

        <span class="code-comment"># é‡å¡‘å’Œè¾“å‡º</span>
        attn_output = attn_output.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.d_model)

        <span class="code-keyword">return</span> self.W_o(attn_output)


<span class="code-keyword">class</span> <span class="code-class">GroupedQueryAttention</span>(nn.Module):
    <span class="code-string">"""Grouped-Query Attention (GQA)

    MHAå’ŒMQAçš„æŠ˜ä¸­ï¼šå°†æ³¨æ„åŠ›å¤´åˆ†ç»„ï¼Œç»„å†…å…±äº«KV
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, n_heads: int, n_kv_heads: int):
        super().__init__()
        assert n_heads % n_kv_heads == <span class="code-number">0</span>

        self.d_model = d_model
        self.n_heads = n_heads
        self.n_kv_heads = n_kv_heads
        self.n_groups = n_heads // n_kv_heads
        self.head_dim = d_model // n_heads

        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, n_kv_heads * self.head_dim)
        self.W_v = nn.Linear(d_model, n_kv_heads * self.head_dim)
        self.W_o = nn.Linear(d_model, d_model)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape

        <span class="code-comment"># è®¡ç®—Q</span>
        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.head_dim)
        Q = Q.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

        <span class="code-comment"># è®¡ç®—Kå’ŒVï¼ˆn_kv_headsç»„ï¼‰</span>
        K = self.W_k(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        V = self.W_v(x).view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
        K = K.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)
        V = V.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>)

        <span class="code-comment"># é‡å¤Kå’ŒVä»¥åŒ¹é…Qçš„å¤´æ•°</span>
        K = K.repeat_interleave(self.n_groups, dim=<span class="code-number">1</span>)
        V = V.repeat_interleave(self.n_groups, dim=<span class="code-number">1</span>)

        <span class="code-comment"># æ³¨æ„åŠ›è®¡ç®—</span>
        scores = torch.matmul(Q, K.transpose(-<span class="code-number">2</span>, -<span class="code-number">1</span>)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(scores, dim=-<span class="code-number">1</span>)
        attn_output = torch.matmul(attn_weights, V)

        <span class="code-comment"># è¾“å‡º</span>
        attn_output = attn_output.transpose(<span class="code-number">1</span>, <span class="code-number">2</span>).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.d_model)

        <span class="code-keyword">return</span> self.W_o(attn_output)</code></pre>
            </div>

            <div class="comparison-table">
                <h4 style="color: #3b82f6; margin-bottom: 1rem; padding: 1rem;">ğŸ“Š æ³¨æ„åŠ›å˜ä½“å¯¹æ¯”</h4>
                <table>
                    <thead>
                    <tr>
                        <th>æ–¹æ³•</th>
                        <th>KVå‚æ•°é‡</th>
                        <th>KV Cacheå¤§å°</th>
                        <th>è¡¨è¾¾èƒ½åŠ›</th>
                        <th>æ¨ç†é€Ÿåº¦</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td><strong>MHA</strong></td>
                        <td>100%</td>
                        <td>100%</td>
                        <td>â­â­â­â­â­</td>
                        <td>â­â­â­</td>
                    </tr>
                    <tr>
                        <td><strong>MQA</strong></td>
                        <td>1/n_heads</td>
                        <td>1/n_heads</td>
                        <td>â­â­â­</td>
                        <td>â­â­â­â­â­</td>
                    </tr>
                    <tr>
                        <td><strong>GQA</strong></td>
                        <td>n_kv_heads/n_heads</td>
                        <td>n_kv_heads/n_heads</td>
                        <td>â­â­â­â­</td>
                        <td>â­â­â­â­</td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </div>
    </div>

    <!-- ğŸ’» Part 4: å·¥ç¨‹ä¼˜åŒ– -->
    <div class="optimization-section" style="border-color: #8b5cf6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸ’»</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">Part 4: å·¥ç¨‹ä¼˜åŒ–</h2>
            </div>
        </div>

        <!-- 4.1 ç®—å­èåˆ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">9</div>
                <div class="step-title">ç®—å­èåˆï¼šå‡å°‘å†…å­˜å¸¦å®½ç“¶é¢ˆ</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°èåˆçš„LayerNormå’Œæ¿€æ´»å‡½æ•°</div>
                    <div class="code-filename">optimization/operator_fusion.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">from</span> torch.cuda.amp <span class="code-keyword">import</span> custom_fwd, custom_bwd

<span class="code-comment"># JITç¼–è¯‘çš„èåˆç®—å­</span>
@torch.jit.script
<span class="code-keyword">def</span> <span class="code-function">fused_gelu</span>(x: torch.Tensor) -> torch.Tensor:
    <span class="code-string">"""èåˆçš„GELUæ¿€æ´»å‡½æ•°"""</span>
    <span class="code-keyword">return</span> x * <span class="code-number">0.5</span> * (<span class="code-number">1.0</span> + torch.tanh(<span class="code-number">0.79788456</span> * x * (<span class="code-number">1</span> + <span class="code-number">0.044715</span> * x * x)))

@torch.jit.script
<span class="code-keyword">def</span> <span class="code-function">fused_layer_norm</span>(x: torch.Tensor,
                        weight: torch.Tensor,
                        bias: torch.Tensor,
                        eps: float = <span class="code-number">1e-5</span>) -> torch.Tensor:
    <span class="code-string">"""èåˆçš„LayerNorm"""</span>
    mean = x.mean(dim=-<span class="code-number">1</span>, keepdim=True)
    var = ((x - mean) ** <span class="code-number">2</span>).mean(dim=-<span class="code-number">1</span>, keepdim=True)
    x_norm = (x - mean) / torch.sqrt(var + eps)
    <span class="code-keyword">return</span> weight * x_norm + bias

<span class="code-comment"># è‡ªå®šä¹‰CUDAç®—å­ï¼ˆä¼ªä»£ç ï¼‰</span>
<span class="code-keyword">class</span> <span class="code-class">FusedLayerNormGELU</span>(torch.autograd.Function):
    <span class="code-string">"""èåˆLayerNorm + GELUçš„è‡ªå®šä¹‰ç®—å­"""</span>

    @staticmethod
    @custom_fwd
    <span class="code-keyword">def</span> <span class="code-function">forward</span>(ctx, x, weight, bias, eps=<span class="code-number">1e-5</span>):
        <span class="code-comment"># åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™ä¼šè°ƒç”¨è‡ªå®šä¹‰CUDA kernel</span>
        <span class="code-comment"># è¿™é‡Œç”¨Pythonå±•ç¤ºé€»è¾‘</span>

        <span class="code-comment"># LayerNorm</span>
        mean = x.mean(dim=-<span class="code-number">1</span>, keepdim=True)
        var = ((x - mean) ** <span class="code-number">2</span>).mean(dim=-<span class="code-number">1</span>, keepdim=True)
        std = torch.sqrt(var + eps)
        x_norm = (x - mean) / std
        ln_out = weight * x_norm + bias

        <span class="code-comment"># GELU</span>
        gelu_out = ln_out * <span class="code-number">0.5</span> * (<span class="code-number">1.0</span> + torch.tanh(
            <span class="code-number">0.79788456</span> * ln_out * (<span class="code-number">1</span> + <span class="code-number">0.044715</span> * ln_out * ln_out)
        ))

        <span class="code-comment"># ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­</span>
        ctx.save_for_backward(x, weight, mean, std, ln_out)
        ctx.eps = eps

        <span class="code-keyword">return</span> gelu_out

    @staticmethod
    @custom_bwd
    <span class="code-keyword">def</span> <span class="code-function">backward</span>(ctx, grad_output):
        x, weight, mean, std, ln_out = ctx.saved_tensors
        eps = ctx.eps

        <span class="code-comment"># GELUçš„æ¢¯åº¦</span>
        tanh_out = torch.tanh(<span class="code-number">0.79788456</span> * ln_out * (<span class="code-number">1</span> + <span class="code-number">0.044715</span> * ln_out * ln_out))
        ff = <span class="code-number">0.5</span> * ln_out * ((<span class="code-number">1</span> - tanh_out * tanh_out) *
             (<span class="code-number">0.79788456</span> + <span class="code-number">0.1070322243</span> * ln_out * ln_out)) + <span class="code-number">0.5</span> * (<span class="code-number">1</span> + tanh_out)
        grad_ln = grad_output * ff

        <span class="code-comment"># LayerNormçš„æ¢¯åº¦</span>
        x_norm = (x - mean) / std
        grad_weight = (grad_ln * x_norm).sum(dim=(<span class="code-number">0</span>, <span class="code-number">1</span>))
        grad_bias = grad_ln.sum(dim=(<span class="code-number">0</span>, <span class="code-number">1</span>))

        <span class="code-comment"># è¾“å…¥çš„æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰</span>
        grad_x = grad_ln * weight / std

        <span class="code-keyword">return</span> grad_x, grad_weight, grad_bias, None

<span class="code-comment"># ä½¿ç”¨èåˆç®—å­çš„æ¨¡å—</span>
<span class="code-keyword">class</span> <span class="code-class">OptimizedFeedForward</span>(nn.Module):
    <span class="code-string">"""ä¼˜åŒ–çš„å‰é¦ˆç½‘ç»œï¼Œä½¿ç”¨èåˆç®—å­"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, d_ff: int):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)

        <span class="code-comment"># LayerNormå‚æ•°</span>
        self.ln_weight = nn.Parameter(torch.ones(d_ff))
        self.ln_bias = nn.Parameter(torch.zeros(d_ff))

        self.dropout = nn.Dropout(<span class="code-number">0.1</span>)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-comment"># ç¬¬ä¸€å±‚ + èåˆçš„LayerNorm+GELU</span>
        h = self.linear1(x)
        h = FusedLayerNormGELU.apply(h, self.ln_weight, self.ln_bias)
        h = self.dropout(h)

        <span class="code-comment"># ç¬¬äºŒå±‚</span>
        output = self.linear2(h)
        <span class="code-keyword">return</span> self.dropout(output)</code></pre>
            </div>
        </div>

        <!-- 4.2 æ¨¡å‹å¹¶è¡Œ -->
        <div class="optimization-step">
            <div class="step-header">
                <div class="step-number">10</div>
                <div class="step-title">æ¨¡å‹å¹¶è¡Œï¼šçªç ´å•GPUé™åˆ¶</div>
            </div>

            <div class="code-implementation">
                <div class="code-header">
                    <div class="code-title">å®ç°å¼ é‡æ¨¡å‹å¹¶è¡Œ</div>
                    <div class="code-filename">distributed/tensor_parallel.py</div>
                </div>
                <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> torch.distributed <span class="code-keyword">as</span> dist
<span class="code-keyword">from</span> torch.nn <span class="code-keyword">import</span> functional <span class="code-keyword">as</span> F

<span class="code-keyword">class</span> <span class="code-class">ColumnParallelLinear</span>(nn.Module):
    <span class="code-string">"""åˆ—å¹¶è¡Œçš„çº¿æ€§å±‚

    å°†æƒé‡çŸ©é˜µæŒ‰åˆ—åˆ†å‰²åˆ°å¤šä¸ªGPUä¸Š
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 in_features: int,
                 out_features: int,
                 world_size: int,
                 rank: int,
                 bias: bool = True):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.world_size = world_size
        self.rank = rank

        <span class="code-comment"># è®¡ç®—æ¯ä¸ªGPUçš„è¾“å‡ºç»´åº¦</span>
        assert out_features % world_size == <span class="code-number">0</span>
        self.out_features_per_partition = out_features // world_size

        <span class="code-comment"># åˆå§‹åŒ–æœ¬åœ°æƒé‡</span>
        self.weight = nn.Parameter(torch.empty(
            self.out_features_per_partition,
            in_features
        ))

        <span class="code-keyword">if</span> bias:
            self.bias = nn.Parameter(torch.empty(self.out_features_per_partition))
        <span class="code-keyword">else</span>:
            self.register_parameter(<span class="code-string">'bias'</span>, None)

        <span class="code-comment"># åˆå§‹åŒ–</span>
        nn.init.kaiming_uniform_(self.weight)
        <span class="code-keyword">if</span> bias:
            nn.init.zeros_(self.bias)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-comment"># è¾“å…¥åœ¨æ‰€æœ‰GPUä¸Šæ˜¯ç›¸åŒçš„</span>
        <span class="code-comment"># æ¯ä¸ªGPUè®¡ç®—éƒ¨åˆ†è¾“å‡º</span>
        local_output = F.linear(x, self.weight, self.bias)

        <span class="code-comment"># ä¸éœ€è¦é€šä¿¡ï¼Œç›´æ¥è¿”å›å±€éƒ¨ç»“æœ</span>
        <span class="code-keyword">return</span> local_output


<span class="code-keyword">class</span> <span class="code-class">RowParallelLinear</span>(nn.Module):
    <span class="code-string">"""è¡Œå¹¶è¡Œçš„çº¿æ€§å±‚

    å°†æƒé‡çŸ©é˜µæŒ‰è¡Œåˆ†å‰²ï¼Œéœ€è¦åœ¨æœ€åè¿›è¡Œå½’çº¦
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 in_features: int,
                 out_features: int,
                 world_size: int,
                 rank: int,
                 bias: bool = True):
        super().__init__()

        self.in_features = in_features
        self.out_features = out_features
        self.world_size = world_size
        self.rank = rank

        <span class="code-comment"># è®¡ç®—æ¯ä¸ªGPUçš„è¾“å…¥ç»´åº¦</span>
        assert in_features % world_size == <span class="code-number">0</span>
        self.in_features_per_partition = in_features // world_size

        <span class="code-comment"># åˆå§‹åŒ–æœ¬åœ°æƒé‡</span>
        self.weight = nn.Parameter(torch.empty(
            out_features,
            self.in_features_per_partition
        ))

        <span class="code-keyword">if</span> bias <span class="code-keyword">and</span> rank == <span class="code-number">0</span>:
            <span class="code-comment"># åªåœ¨ç¬¬ä¸€ä¸ªGPUä¸Šä¿å­˜bias</span>
            self.bias = nn.Parameter(torch.empty(out_features))
        <span class="code-keyword">else</span>:
            self.register_parameter(<span class="code-string">'bias'</span>, None)

        <span class="code-comment"># åˆå§‹åŒ–</span>
        nn.init.kaiming_uniform_(self.weight)
        <span class="code-keyword">if</span> hasattr(self, <span class="code-string">'bias'</span>) <span class="code-keyword">and</span> self.bias <span class="code-keyword">is not None</span>:
            nn.init.zeros_(self.bias)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-comment"># xæ¥è‡ªåˆ—å¹¶è¡Œå±‚ï¼Œå·²ç»è¢«åˆ†å‰²</span>
        local_output = F.linear(x, self.weight)

        <span class="code-comment"># All-reduceï¼šå°†æ‰€æœ‰GPUçš„ç»“æœç›¸åŠ </span>
        <span class="code-keyword">if</span> self.world_size > <span class="code-number">1</span>:
            dist.all_reduce(local_output)

        <span class="code-comment"># åªåœ¨rank 0ä¸ŠåŠ bias</span>
        <span class="code-keyword">if</span> self.bias <span class="code-keyword">is not None</span>:
            local_output = local_output + self.bias

        <span class="code-keyword">return</span> local_output


<span class="code-keyword">class</span> <span class="code-class">ParallelMLP</span>(nn.Module):
    <span class="code-string">"""å¼ é‡å¹¶è¡Œçš„MLPå±‚"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, d_model: int, d_ff: int, world_size: int, rank: int):
        super().__init__()

        <span class="code-comment"># ç¬¬ä¸€å±‚ï¼šåˆ—å¹¶è¡Œï¼ˆä¸éœ€è¦é€šä¿¡ï¼‰</span>
        self.fc1 = ColumnParallelLinear(d_model, d_ff, world_size, rank)

        <span class="code-comment"># æ¿€æ´»å‡½æ•°</span>
        self.activation = nn.GELU()

        <span class="code-comment"># ç¬¬äºŒå±‚ï¼šè¡Œå¹¶è¡Œï¼ˆéœ€è¦all-reduceï¼‰</span>
        self.fc2 = RowParallelLinear(d_ff // world_size, d_model, world_size, rank)

        self.dropout = nn.Dropout(<span class="code-number">0.1</span>)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-comment"># å‰å‘ä¼ æ’­</span>
        h = self.fc1(x)
        h = self.activation(h)
        h = self.dropout(h)
        output = self.fc2(h)
        <span class="code-keyword">return</span> self.dropout(output)


<span class="code-comment"># Pipelineå¹¶è¡Œç¤ºä¾‹</span>
<span class="code-keyword">class</span> <span class="code-class">PipelineParallelTransformer</span>(nn.Module):
    <span class="code-string">"""æµæ°´çº¿å¹¶è¡Œçš„Transformer

    å°†æ¨¡å‹å±‚åˆ†é…åˆ°ä¸åŒGPUä¸Š
    """</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self,
                 n_layers: int,
                 d_model: int,
                 n_devices: int):
        super().__init__()

        self.n_layers = n_layers
        self.n_devices = n_devices

        <span class="code-comment"># è®¡ç®—æ¯ä¸ªè®¾å¤‡çš„å±‚æ•°</span>
        layers_per_device = n_layers // n_devices

        <span class="code-comment"># åˆ›å»ºå±‚å¹¶åˆ†é…åˆ°è®¾å¤‡</span>
        self.layers = nn.ModuleList()
        <span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(n_layers):
            device_id = i // layers_per_device
            layer = TransformerLayer(d_model).to(<span class="code-string">f'cuda:{device_id}'</span>)
            self.layers.append(layer)

    <span class="code-keyword">def</span> <span class="code-function">forward</span>(self, x: torch.Tensor) -> torch.Tensor:
        <span class="code-comment"># é€å±‚å‰å‘ï¼Œè‡ªåŠ¨å¤„ç†è®¾å¤‡é—´ä¼ è¾“</span>
        <span class="code-keyword">for</span> layer <span class="code-keyword">in</span> self.layers:
            device = next(layer.parameters()).device
            x = x.to(device)
            x = layer(x)

        <span class="code-keyword">return</span> x</code></pre>
            </div>

            <div class="performance-card">
                <h4 style="color: #8b5cf6; margin-bottom: 1rem;">ğŸ“Š å¹¶è¡Œç­–ç•¥å¯¹æ¯”</h4>
                <div class="comparison-table">
                    <table style="width: 100%; background: transparent;">
                        <thead>
                        <tr style="background: rgba(139, 92, 246, 0.2);">
                            <th style="color: #8b5cf6; padding: 0.8rem;">å¹¶è¡Œæ–¹å¼</th>
                            <th style="color: #8b5cf6; padding: 0.8rem;">é€šä¿¡å¼€é”€</th>
                            <th style="color: #8b5cf6; padding: 0.8rem;">å†…å­˜æ•ˆç‡</th>
                            <th style="color: #8b5cf6; padding: 0.8rem;">é€‚ç”¨åœºæ™¯</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td style="padding: 0.8rem;"><strong>æ•°æ®å¹¶è¡Œ</strong></td>
                            <td style="padding: 0.8rem;">é«˜ï¼ˆæ¢¯åº¦åŒæ­¥ï¼‰</td>
                            <td style="padding: 0.8rem;">ä½ï¼ˆå¤åˆ¶æ¨¡å‹ï¼‰</td>
                            <td style="padding: 0.8rem;">å°æ¨¡å‹</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.8rem;"><strong>å¼ é‡å¹¶è¡Œ</strong></td>
                            <td style="padding: 0.8rem;">ä¸­ï¼ˆæ¿€æ´»åŒæ­¥ï¼‰</td>
                            <td style="padding: 0.8rem;">é«˜ï¼ˆåˆ†å‰²æ¨¡å‹ï¼‰</td>
                            <td style="padding: 0.8rem;">å•å±‚å¾ˆå¤§</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.8rem;"><strong>æµæ°´çº¿å¹¶è¡Œ</strong></td>
                            <td style="padding: 0.8rem;">ä½ï¼ˆå±‚é—´ä¼ è¾“ï¼‰</td>
                            <td style="padding: 0.8rem;">é«˜ï¼ˆåˆ†å‰²æ¨¡å‹ï¼‰</td>
                            <td style="padding: 0.8rem;">å±‚æ•°å¾ˆå¤š</td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>

    <!-- ğŸ¯ å®æˆ˜ä¼˜åŒ–æŠ€å·§ -->
    <div class="optimization-section" style="border-color: #22c55e; background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(16, 185, 129, 0.05));">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">ğŸ¯</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">å®æˆ˜ä¼˜åŒ–æ¸…å•</h2>
            </div>
        </div>

        <!-- ä¼˜åŒ–å†³ç­–æ ‘ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2.5rem; border-radius: 16px; margin-bottom: 2rem;">
            <h3 style="color: #fbbf24; margin-bottom: 1.5rem; text-align: center;">ğŸŒ³ ä¼˜åŒ–å†³ç­–æ ‘</h3>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 12px;">
                <div style="color: #cbd5e1; line-height: 2;">
                    <strong style="color: #3b82f6;">1. é¦–å…ˆç¡®å®šç“¶é¢ˆï¼š</strong><br>
                    ã€€ã€€â”œâ”€ è®­ç»ƒå¤ªæ…¢ â†’ æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯<br>
                    ã€€ã€€â”œâ”€ æ˜¾å­˜ä¸è¶³ â†’ æ¢¯åº¦æ£€æŸ¥ç‚¹ + æ¨¡å‹å¹¶è¡Œ<br>
                    ã€€ã€€â”œâ”€ æ¨ç†å»¶è¿Ÿé«˜ â†’ KV Cache + é‡åŒ–<br>
                    ã€€ã€€â””â”€ ååé‡ä½ â†’ æ‰¹å¤„ç†ä¼˜åŒ– + ç®—å­èåˆ<br><br>

                    <strong style="color: #22c55e;">2. æ ¹æ®æ¨¡å‹è§„æ¨¡é€‰æ‹©ï¼š</strong><br>
                    ã€€ã€€â”œâ”€ < 1Bå‚æ•°ï¼šæ•°æ®å¹¶è¡Œ + æ··åˆç²¾åº¦<br>
                    ã€€ã€€â”œâ”€ 1B-10Bï¼šå¼ é‡å¹¶è¡Œ + ZeRO-2<br>
                    ã€€ã€€â”œâ”€ 10B-100Bï¼š3Då¹¶è¡Œ + ZeRO-3<br>
                    ã€€ã€€â””â”€ > 100Bï¼šä¸“å®¶å¹¶è¡Œ + æ‰€æœ‰æŠ€æœ¯<br><br>

                    <strong style="color: #fbbf24;">3. æ¨ç†ä¼˜åŒ–ä¼˜å…ˆçº§ï¼š</strong><br>
                    ã€€ã€€1ï¸âƒ£ KV Cacheï¼ˆå¿…é¡»ï¼‰<br>
                    ã€€ã€€2ï¸âƒ£ é‡åŒ–ï¼ˆINT8/INT4ï¼‰<br>
                    ã€€ã€€3ï¸âƒ£ Flash Attention<br>
                    ã€€ã€€4ï¸âƒ£ æ¨¡å‹å‰ªæ<br>
                    ã€€ã€€5ï¸âƒ£ çŸ¥è¯†è’¸é¦
                </div>
            </div>
        </div>

        <!-- æœ€ä½³å®è·µ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 16px; margin-bottom: 2rem;">
            <h3 style="color: #3b82f6; margin-bottom: 1.5rem; text-align: center;">ğŸ’¡ ä¼˜åŒ–æœ€ä½³å®è·µ</h3>

            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem;">
                <div class="optimization-card">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem;">âš¡ è®­ç»ƒåŠ é€ŸæŠ€å·§</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        âœ“ ä½¿ç”¨torch.compile()åŠ é€Ÿ20-30%<br>
                        âœ“ å¼€å¯cudnn.benchmarkè‡ªåŠ¨è°ƒä¼˜<br>
                        âœ“ é¢„å…ˆåˆ†é…æ˜¾å­˜é¿å…ç¢ç‰‡åŒ–<br>
                        âœ“ ä½¿ç”¨NCCL_P2P_DISABLE=1å‡å°‘P2På¼€é”€<br>
                        âœ“ è®¾ç½®åˆé€‚çš„num_workersåŠ è½½æ•°æ®
                    </div>
                </div>

                <div class="optimization-card">
                    <h4 style="color: #22c55e; margin-bottom: 1rem;">ğŸš€ æ¨ç†ä¼˜åŒ–æŠ€å·§</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        âœ“ ä½¿ç”¨torch.inference_mode()æ›¿ä»£no_grad<br>
                        âœ“ å¯ç”¨CUDA Graphså‡å°‘kernelå¯åŠ¨å¼€é”€<br>
                        âœ“ æ‰¹å¤„ç†ç›¸ä¼¼é•¿åº¦çš„åºåˆ—<br>
                        âœ“ ä½¿ç”¨torch.jit.scriptèåˆå°ç®—å­<br>
                        âœ“ è€ƒè™‘ONNX/TensorRTéƒ¨ç½²
                    </div>
                </div>

                <div class="optimization-card">
                    <h4 style="color: #3b82f6; margin-bottom: 1rem;">ğŸ’¾ å†…å­˜ä¼˜åŒ–æŠ€å·§</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        âœ“ åŠæ—¶delä¸ç”¨çš„ä¸­é—´å˜é‡<br>
                        âœ“ ä½¿ç”¨inplaceæ“ä½œå‡å°‘å†…å­˜åˆ†é…<br>
                        âœ“ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆactivation checkpointingï¼‰<br>
                        âœ“ ä½¿ç”¨CPU offloadå¤„ç†å¤§æ¨¡å‹<br>
                        âœ“ ç›‘æ§torch.cuda.memory_summary()
                    </div>
                </div>

                <div class="optimization-card">
                    <h4 style="color: #8b5cf6; margin-bottom: 1rem;">ğŸ”§ è°ƒè¯•ä¼˜åŒ–æŠ€å·§</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        âœ“ ä½¿ç”¨PyTorch Profileræ‰¾ç“¶é¢ˆ<br>
                        âœ“ å¼€å¯anomaly detectionæ’æŸ¥NaN<br>
                        âœ“ ä½¿ç”¨wandb/tensorboardç›‘æ§æŒ‡æ ‡<br>
                        âœ“ è®°å½•æ¯å±‚çš„æ¿€æ´»å€¼ç»Ÿè®¡<br>
                        âœ“ å®šæœŸä¿å­˜checkpointé˜²æ­¢å´©æºƒ
                    </div>
                </div>
            </div>
        </div>

        <!-- æ€§èƒ½åŸºå‡†æµ‹è¯•ä»£ç  -->
        <div class="code-implementation">
            <div class="code-header">
                <div class="code-title">å®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•</div>
                <div class="code-filename">benchmark/performance_test.py</div>
            </div>
            <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> time
<span class="code-keyword">from</span> contextlib <span class="code-keyword">import</span> contextmanager
<span class="code-keyword">from</span> dataclasses <span class="code-keyword">import</span> dataclass
<span class="code-keyword">from</span> typing <span class="code-keyword">import</span> Dict, List

@dataclass
<span class="code-keyword">class</span> <span class="code-class">BenchmarkResult</span>:
    <span class="code-string">"""åŸºå‡†æµ‹è¯•ç»“æœ"""</span>
    name: str
    avg_time: float
    memory_used: float
    throughput: float

<span class="code-keyword">class</span> <span class="code-class">TransformerBenchmark</span>:
    <span class="code-string">"""Transformeræ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""</span>

    <span class="code-keyword">def</span> <span class="code-function">__init__</span>(self, model, device=<span class="code-string">'cuda'</span>):
        self.model = model.to(device)
        self.device = device
        self.results = []

    @contextmanager
    <span class="code-keyword">def</span> <span class="code-function">timer</span>(self, name: str):
        <span class="code-string">"""è®¡æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""</span>
        torch.cuda.synchronize()
        start_time = time.time()
        start_memory = torch.cuda.memory_allocated()

        <span class="code-keyword">yield</span>

        torch.cuda.synchronize()
        end_time = time.time()
        end_memory = torch.cuda.memory_allocated()

        elapsed = end_time - start_time
        memory_used = (end_memory - start_memory) / <span class="code-number">1024</span>**<span class="code-number">3</span>  <span class="code-comment"># GB</span>

        self.results.append({
            <span class="code-string">'name'</span>: name,
            <span class="code-string">'time'</span>: elapsed,
            <span class="code-string">'memory'</span>: memory_used
        })

    <span class="code-keyword">def</span> <span class="code-function">benchmark_training</span>(self,
                            batch_size: int = <span class="code-number">32</span>,
                            seq_length: int = <span class="code-number">512</span>,
                            num_iterations: int = <span class="code-number">100</span>):
        <span class="code-string">"""è®­ç»ƒæ€§èƒ½åŸºå‡†æµ‹è¯•"""</span>

        <span class="code-comment"># å‡†å¤‡æ•°æ®</span>
        src = torch.randint(<span class="code-number">0</span>, <span class="code-number">10000</span>, (batch_size, seq_length)).to(self.device)
        tgt = torch.randint(<span class="code-number">0</span>, <span class="code-number">10000</span>, (batch_size, seq_length)).to(self.device)

        optimizer = torch.optim.AdamW(self.model.parameters(), lr=<span class="code-number">1e-4</span>)
        criterion = nn.CrossEntropyLoss()

        <span class="code-comment"># é¢„çƒ­</span>
        <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(<span class="code-number">10</span>):
            output = self.model(src, tgt[:, :-<span class="code-number">1</span>])
            loss = criterion(output.reshape(-<span class="code-number">1</span>, output.size(-<span class="code-number">1</span>)),
                           tgt[:, <span class="code-number">1</span>:].reshape(-<span class="code-number">1</span>))
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        <span class="code-comment"># å®é™…æµ‹è¯•</span>
        times = []
        <span class="code-keyword">with</span> self.timer(<span class="code-string">"Training"</span>):
            <span class="code-keyword">for</span> _ <span class="code-keyword">in</span> range(num_iterations):
                start = time.time()

                output = self.model(src, tgt[:, :-<span class="code-number">1</span>])
                loss = criterion(output.reshape(-<span class="code-number">1</span>, output.size(-<span class="code-number">1</span>)),
                               tgt[:, <span class="code-number">1</span>:].reshape(-<span class="code-number">1</span>))
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()

                torch.cuda.synchronize()
                times.append(time.time() - start)

        <span class="code-keyword">return</span> {
            <span class="code-string">'avg_time_per_iter'</span>: sum(times) / len(times),
            <span class="code-string">'throughput'</span>: batch_size * seq_length / (sum(times) / len(times)),
            <span class="code-string">'memory_gb'</span>: torch.cuda.max_memory_allocated() / <span class="code-number">1024</span>**<span class="code-number">3</span>
        }

    <span class="code-keyword">def</span> <span class="code-function">benchmark_inference</span>(self,
                             batch_size: int = <span class="code-number">1</span>,
                             prompt_length: int = <span class="code-number">128</span>,
                             generation_length: int = <span class="code-number">128</span>):
        <span class="code-string">"""æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""</span>

        <span class="code-comment"># æµ‹è¯•å¸¦KV Cacheçš„æ¨ç†</span>
        prompt = torch.randint(<span class="code-number">0</span>, <span class="code-number">10000</span>, (batch_size, prompt_length)).to(self.device)

        <span class="code-keyword">with</span> torch.inference_mode():
            <span class="code-comment"># å¤„ç†prompt</span>
            <span class="code-keyword">with</span> self.timer(<span class="code-string">"Prompt Processing"</span>):
                _ = self.model(prompt, prompt)

            <span class="code-comment"># ç”Ÿæˆtokens</span>
            generation_times = []
            <span class="code-keyword">for</span> i <span class="code-keyword">in</span> range(generation_length):
                start = time.time()

                <span class="code-comment"># æ¨¡æ‹Ÿç”Ÿæˆï¼ˆå®é™…åº”è¯¥ä½¿ç”¨KV Cacheï¼‰</span>
                new_token = torch.randint(<span class="code-number">0</span>, <span class="code-number">10000</span>, (batch_size, <span class="code-number">1</span>)).to(self.device)
                _ = self.model(new_token, new_token)

                torch.cuda.synchronize()
                generation_times.append(time.time() - start)

        <span class="code-keyword">return</span> {
            <span class="code-string">'prompt_processing_time'</span>: self.results[-<span class="code-number">2</span>][<span class="code-string">'time'</span>],
            <span class="code-string">'avg_generation_time'</span>: sum(generation_times) / len(generation_times),
            <span class="code-string">'tokens_per_second'</span>: <span class="code-number">1.0</span> / (sum(generation_times) / len(generation_times))
        }

    <span class="code-keyword">def</span> <span class="code-function">compare_optimizations</span>(self, configs: List[Dict]):
        <span class="code-string">"""æ¯”è¾ƒä¸åŒä¼˜åŒ–é…ç½®"""</span>
        results = []

        <span class="code-keyword">for</span> config <span class="code-keyword">in</span> configs:
            print(<span class="code-string">f"\næµ‹è¯•é…ç½®: {config['name']}"</span>)

            <span class="code-comment"># åº”ç”¨é…ç½®</span>
            <span class="code-keyword">if</span> config.get(<span class="code-string">'mixed_precision'</span>):
                <span class="code-keyword">with</span> torch.cuda.amp.autocast():
                    result = self.benchmark_training()
            <span class="code-keyword">elif</span> config.get(<span class="code-string">'compile'</span>):
                compiled_model = torch.compile(self.model)
                self.model = compiled_model
                result = self.benchmark_training()
            <span class="code-keyword">else</span>:
                result = self.benchmark_training()

            results.append({
                <span class="code-string">'config'</span>: config[<span class="code-string">'name'</span>],
                **result
            })

        <span class="code-keyword">return</span> results

<span class="code-comment"># ä½¿ç”¨ç¤ºä¾‹</span>
<span class="code-keyword">if</span> __name__ == <span class="code-string">"__main__"</span>:
    <span class="code-comment"># åˆ›å»ºæ¨¡å‹</span>
    model = Transformer(config)

    <span class="code-comment"># åˆ›å»ºåŸºå‡†æµ‹è¯•</span>
    benchmark = TransformerBenchmark(model)

    <span class="code-comment"># æµ‹è¯•ä¸åŒé…ç½®</span>
    configs = [
        {<span class="code-string">'name'</span>: <span class="code-string">'Baseline'</span>},
        {<span class="code-string">'name'</span>: <span class="code-string">'Mixed Precision'</span>, <span class="code-string">'mixed_precision'</span>: True},
        {<span class="code-string">'name'</span>: <span class="code-string">'Compiled'</span>, <span class="code-string">'compile'</span>: True},
    ]

    results = benchmark.compare_optimizations(configs)

    <span class="code-comment"># æ‰“å°ç»“æœ</span>
    print(<span class="code-string">"\næ€§èƒ½å¯¹æ¯”ç»“æœ:"</span>)
    print(<span class="code-string">"-"</span> * <span class="code-number">60</span>)
    <span class="code-keyword">for</span> r <span class="code-keyword">in</span> results:
        print(<span class="code-string">f"{r['config']:20s} | "</span>
              <span class="code-string">f"é€Ÿåº¦: {r['throughput']:.0f} tokens/s | "</span>
              <span class="code-string">f"æ˜¾å­˜: {r['memory_gb']:.2f} GB"</span>)</code></pre>
        </div>
    </div>

    <!-- ğŸ“ æ€»ç»“ -->
    <div class="optimization-section" style="border-color: #8b5cf6; background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(124, 58, 237, 0.05));">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">ğŸ“</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">æ€»ç»“ï¼šä¼˜åŒ–çš„è‰ºæœ¯ä¸ç§‘å­¦</h2>
            </div>
        </div>

        <!-- æ ¸å¿ƒè¦ç‚¹ -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2.5rem; border-radius: 16px; margin-bottom: 2rem;">
            <h3 style="color: #22c55e; margin-bottom: 1.5rem; text-align: center;">âœ¨ ä¼˜åŒ–çš„æ ¸å¿ƒè¦ç‚¹</h3>

            <div style="display: grid; gap: 1.5rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #3b82f6; margin-bottom: 0.8rem;">ğŸ¯ æ²¡æœ‰é“¶å¼¹</h4>
                    <div style="color: #cbd5e1;">
                        æ¯ç§ä¼˜åŒ–æŠ€æœ¯éƒ½æœ‰å…¶é€‚ç”¨åœºæ™¯å’Œæƒè¡¡ã€‚æ··åˆç²¾åº¦å¯èƒ½åŠ é€Ÿè®­ç»ƒä½†éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ï¼›
                        é‡åŒ–å‡å°‘å†…å­˜ä½†å¯èƒ½æŸå¤±ç²¾åº¦ï¼›å¹¶è¡Œæé«˜ååä½†å¢åŠ é€šä¿¡å¼€é”€ã€‚
                        <strong>å…³é”®æ˜¯æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„ç»„åˆã€‚</strong>
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 0.8rem;">ğŸ“Š åº¦é‡é©±åŠ¨</h4>
                    <div style="color: #cbd5e1;">
                        ä¼˜åŒ–å¿…é¡»åŸºäºå‡†ç¡®çš„æ€§èƒ½åº¦é‡ã€‚ä½¿ç”¨Profileræ‰¾å‡ºçœŸæ­£çš„ç“¶é¢ˆï¼Œ
                        è€Œä¸æ˜¯å‡­ç»éªŒçŒœæµ‹ã€‚è®°ä½ï¼š<strong>è¿‡æ—©ä¼˜åŒ–æ˜¯ä¸‡æ¶ä¹‹æºï¼Œ
                        ä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼ŒæŸäº›ä¼˜åŒ–ï¼ˆå¦‚æ··åˆç²¾åº¦ï¼‰åº”è¯¥ä»ä¸€å¼€å§‹å°±å¯ç”¨ã€‚</strong>
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 0.8rem;">ğŸ”„ æŒç»­è¿­ä»£</h4>
                    <div style="color: #cbd5e1;">
                        ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ã€‚æ–°çš„ç¡¬ä»¶ï¼ˆH100ã€TPU v5ï¼‰å¸¦æ¥æ–°çš„ä¼˜åŒ–æœºä¼šï¼›
                        æ–°çš„ç®—æ³•ï¼ˆFlash Attention 2ã€Ring Attentionï¼‰ä¸æ–­æ¶Œç°ã€‚
                        <strong>ä¿æŒå­¦ä¹ ï¼Œè·Ÿè¸ªæœ€æ–°è¿›å±•ï¼Œä½†ä¹Ÿè¦éªŒè¯åœ¨è‡ªå·±åœºæ™¯ä¸‹çš„å®é™…æ•ˆæœã€‚</strong>
                    </div>
                </div>
            </div>
        </div>

        <!-- æœªæ¥å±•æœ› -->
        <div style="background: linear-gradient(135deg, rgba(251, 191, 36, 0.15), rgba(245, 158, 11, 0.1)); padding: 2rem; border-radius: 16px; border: 1px solid rgba(251, 191, 36, 0.3); margin-bottom: 2rem;">
            <h3 style="color: #fbbf24; margin-bottom: 1.5rem; text-align: center;">ğŸš€ ä¼˜åŒ–æŠ€æœ¯çš„æœªæ¥</h3>

            <div style="color: #f1f5f9; font-size: 1rem; line-height: 1.8;">
                <strong>1. ç¡¬ä»¶ååŒè®¾è®¡ï¼š</strong>æœªæ¥çš„ä¼˜åŒ–å°†æ›´å¤šè€ƒè™‘ç‰¹å®šç¡¬ä»¶ç‰¹æ€§<br>
                <strong>2. è‡ªåŠ¨ä¼˜åŒ–ï¼š</strong>ç¼–è¯‘å™¨å’Œæ¡†æ¶å°†è‡ªåŠ¨é€‰æ‹©æœ€ä½³ä¼˜åŒ–ç­–ç•¥<br>
                <strong>3. ç¨€ç–åŒ–è®¡ç®—ï¼š</strong>åˆ©ç”¨æ¨¡å‹çš„ç¨€ç–æ€§å¤§å¹…æå‡æ•ˆç‡<br>
                <strong>4. è¿‘ä¼¼è®¡ç®—ï¼š</strong>åœ¨å¯æ¥å—çš„ç²¾åº¦æŸå¤±ä¸‹è·å¾—æ•°é‡çº§åŠ é€Ÿ<br>
                <strong>5. ç«¯åˆ°ç«¯ä¼˜åŒ–ï¼š</strong>ä»ç®—æ³•åˆ°ç¡¬ä»¶çš„å…¨æ ˆä¼˜åŒ–<br><br>

                <div style="text-align: center; font-size: 1.2rem; color: #3b82f6; margin-top: 1.5rem;">
                    <strong>è®°ä½ï¼šä¼˜åŒ–çš„ç›®æ ‡ä¸æ˜¯è¿½æ±‚æè‡´æ€§èƒ½ï¼Œè€Œæ˜¯åœ¨çº¦æŸæ¡ä»¶ä¸‹æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ã€‚</strong>
                </div>
            </div>
        </div>

        <!-- ç»“è¯­ -->
        <div style="background: linear-gradient(135deg, rgba(59, 130, 246, 0.2), rgba(147, 51, 234, 0.1)); padding: 3rem; border-radius: 20px; text-align: center; border: 1px solid rgba(59, 130, 246, 0.3);">
            <h2 style="color: #8b5cf6; margin-bottom: 1.5rem;">ğŸ‰ æ­å–œä½ æŒæ¡äº†Transformerä¼˜åŒ–ï¼</h2>
            <div style="color: #f1f5f9; font-size: 1.1rem; line-height: 1.8;">
                é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»äº†è§£äº†ï¼š<br><br>

                <strong style="color: #3b82f6;">
                    ä»è®­ç»ƒåˆ°æ¨ç†çš„å…¨æ–¹ä½ä¼˜åŒ–æŠ€æœ¯<br>
                    ä»ç®—æ³•åˆ°å·¥ç¨‹çš„æ·±åº¦ä¼˜åŒ–ç­–ç•¥<br>
                    ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´ä¼˜åŒ–è·¯å¾„<br>
                </strong><br>

                è¿™äº›ä¼˜åŒ–æŠ€æœ¯è®©Transformerä»å®éªŒå®¤èµ°å‘äº†ç”Ÿäº§ç¯å¢ƒï¼Œ<br>
                è®©å¤§æ¨¡å‹ä»å°‘æ•°äººçš„ç©å…·å˜æˆäº†æ”¹å˜ä¸–ç•Œçš„å·¥å…·ã€‚<br><br>

                <span style="font-size: 1.5rem;">âš¡</span><br>
                <strong style="color: #fbbf24;">
                    ç°åœ¨ï¼Œä½ å·²ç»å‡†å¤‡å¥½è®©ä½ çš„Transformeré£èµ·æ¥äº†ï¼
                </strong>
            </div>
        </div>
    </div>

    <!-- å¯¼èˆª -->
    <div style="display: flex; justify-content: space-between; align-items: center; margin-top: 3rem; padding: 2rem; background: rgba(255, 255, 255, 0.05); border-radius: 12px; backdrop-filter: blur(10px);">
        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #3b82f6, #2563eb); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="window.history.back()">
            â† ç¬¬11ç« ï¼šå®Œæ•´å®ç°
        </button>

        <div style="text-align: center; color: #cbd5e1;">
            <strong>ç¬¬12ç« ï¼šTransformerä¼˜åŒ–</strong><br>
            <span>è®©æ¨¡å‹é£èµ·æ¥</span>
        </div>

        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #22c55e, #16a34a); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="alert('ä¸‹ä¸€ç« ï¼šTransformerçš„åº”ç”¨ä¸å˜ä½“')">
            ç¬¬13ç« ï¼šåº”ç”¨ä¸å˜ä½“ â†’
        </button>
    </div>
</div>

<script>
    // æ·»åŠ äº¤äº’æ•ˆæœ
    document.addEventListener('DOMContentLoaded', function() {
        // æ€§èƒ½æ¡åŠ¨ç”»
        const metricBars = document.querySelectorAll('.metric-fill');
        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    const bar = entry.target;
                    const width = bar.style.width;
                    bar.style.width = '0%';
                    setTimeout(() => {
                        bar.style.width = width;
                    }, 100);
                }
            });
        }, { threshold: 0.5 });

        metricBars.forEach(bar => observer.observe(bar));

        // ä»£ç å—åŠ¨ç”»
        const codeBlocks = document.querySelectorAll('.code-implementation');
        codeBlocks.forEach((block, index) => {
            block.style.opacity = '0';
            block.style.transform = 'translateY(20px)';

            setTimeout(() => {
                block.style.transition = 'all 0.6s ease-out';
                block.style.opacity = '1';
                block.style.transform = 'translateY(0)';
            }, index * 100);
        });

        // ä¼˜åŒ–å¡ç‰‡æ‚¬åœæ•ˆæœ
        const optimizationCards = document.querySelectorAll('.optimization-card');
        optimizationCards.forEach(card => {
            card.addEventListener('mouseenter', function() {
                this.style.transform = 'translateY(-5px) scale(1.02)';
                this.style.boxShadow = '0 15px 40px rgba(251, 191, 36, 0.3)';
            });

            card.addEventListener('mouseleave', function() {
                this.style.transform = 'translateY(0) scale(1)';
                this.style.boxShadow = 'none';
            });
        });

        // æ­¥éª¤ç¼–å·åŠ¨ç”»
        const stepNumbers = document.querySelectorAll('.step-number');
        stepNumbers.forEach((num, index) => {
            num.style.opacity = '0';
            num.style.transform = 'scale(0)';

            setTimeout(() => {
                num.style.transition = 'all 0.5s ease-out';
                num.style.opacity = '1';
                num.style.transform = 'scale(1)';
            }, index * 150);
        });

        // æ·»åŠ ä»£ç å¤åˆ¶åŠŸèƒ½
        const codeHeaders = document.querySelectorAll('.code-header');
        codeHeaders.forEach(header => {
            const copyBtn = document.createElement('button');
            copyBtn.textContent = 'ğŸ“‹ å¤åˆ¶';
            copyBtn.style.cssText = `
                background: rgba(251, 191, 36, 0.2);
                color: #fbbf24;
                border: 1px solid rgba(251, 191, 36, 0.3);
                padding: 0.3rem 0.8rem;
                border-radius: 6px;
                cursor: pointer;
                font-size: 0.85rem;
                transition: all 0.3s ease;
            `;

            copyBtn.addEventListener('click', function() {
                const codeBlock = header.nextElementSibling;
                const code = codeBlock.textContent;

                navigator.clipboard.writeText(code).then(() => {
                    copyBtn.textContent = 'âœ… å·²å¤åˆ¶';
                    setTimeout(() => {
                        copyBtn.textContent = 'ğŸ“‹ å¤åˆ¶';
                    }, 2000);
                });
            });

            copyBtn.addEventListener('mouseenter', function() {
                this.style.background = 'rgba(251, 191, 36, 0.3)';
            });

            copyBtn.addEventListener('mouseleave', function() {
                this.style.background = 'rgba(251, 191, 36, 0.2)';
            });

            header.appendChild(copyBtn);
        });

        // è¡¨æ ¼è¡Œæ‚¬åœæ•ˆæœ
        const tableRows = document.querySelectorAll('.comparison-table tr');
        tableRows.forEach(row => {
            row.addEventListener('mouseenter', function() {
                this.style.background = 'rgba(255, 255, 255, 0.08)';
            });

            row.addEventListener('mouseleave', function() {
                this.style.background = '';
            });
        });
    });
</script>
</body>
</html>