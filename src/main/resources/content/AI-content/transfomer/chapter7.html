
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第7章：残差链接与归一化</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;600&display=swap');

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
            color: #e2e8f0;
            margin: 0;
            padding: 0;
            line-height: 1.6;
        }

        /* 动画效果 */
        @keyframes stability-wave {
            0%, 100% {
                background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(16, 185, 129, 0.05));
                border-color: rgba(34, 197, 94, 0.3);
            }
            50% {
                background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1));
                border-color: rgba(34, 197, 94, 0.5);
            }
        }

        @keyframes gradient-flow {
            0% { transform: translateX(0); }
            50% { transform: translateX(20px); }
            100% { transform: translateX(0); }
        }

        @keyframes data-normalize {
            0% {
                height: 80%;
                background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));
                transform: scale(1.2);
            }
            50% {
                height: 50%;
                background: linear-gradient(to top, rgba(251, 191, 36, 0.6), rgba(251, 191, 36, 0.9));
                transform: scale(1.1);
            }
            100% {
                height: 30%;
                background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));
                transform: scale(1);
            }
        }

        @keyframes matrix-highlight {
            0%, 100% { background: rgba(139, 92, 246, 0.1); }
            50% { background: rgba(139, 92, 246, 0.3); }
        }

        @keyframes matrix-pulse {
            0%, 100% {
                box-shadow: 0 0 20px rgba(34, 197, 94, 0.3);
                border-color: rgba(34, 197, 94, 0.5);
            }
            50% {
                box-shadow: 0 0 40px rgba(34, 197, 94, 0.6);
                border-color: rgba(34, 197, 94, 0.8);
            }
        }

        @keyframes calculation-highlight {
            0% { background: rgba(139, 92, 246, 0.1); }
            50% { background: rgba(139, 92, 246, 0.3); }
            100% { background: rgba(139, 92, 246, 0.1); }
        }

        /* 容器样式 */
        .chapter-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* 章节头部 */
        .chapter-header {
            background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1));
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
            border: 1px solid rgba(34, 197, 94, 0.3);
            text-align: center;
        }

        .chapter-header::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(34, 197, 94, 0.1) 0%, transparent 70%);
            animation: gradient-flow 20s ease-in-out infinite;
        }

        .chapter-header h1 {
            font-size: 2.8rem;
            margin-bottom: 1.5rem;
            background: linear-gradient(45deg, #22c55e, #16a34a, #15803d);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            position: relative;
            z-index: 2;
        }

        .chapter-header p {
            font-size: 1.3rem;
            color: #cbd5e1;
            margin-bottom: 2rem;
            position: relative;
            z-index: 2;
        }

        .meta-tags {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            position: relative;
            z-index: 2;
        }

        .meta-tag {
            background: rgba(34, 197, 94, 0.2);
            color: #22c55e;
            padding: 0.8rem 1.5rem;
            border-radius: 25px;
            font-weight: bold;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            border: 1px solid rgba(34, 197, 94, 0.4);
        }

        /* 主要区块样式 */
        .solution-section {
            background: linear-gradient(135deg, rgba(15, 23, 42, 0.8), rgba(30, 41, 59, 0.6));
            border-radius: 20px;
            padding: 2.5rem;
            margin: 3rem 0;
            border: 2px solid rgba(34, 197, 94, 0.2);
            position: relative;
            overflow: hidden;
            box-shadow: 0 16px 64px rgba(0, 0, 0, 0.3);
        }

        /* 解决方案区块 */
        .solution-discovery {
            background: linear-gradient(135deg, rgba(34, 197, 94, 0.15), rgba(16, 185, 129, 0.1));
            border: 2px solid rgba(34, 197, 94, 0.3);
            border-radius: 20px;
            padding: 2.5rem;
            margin: 2rem 0;
            position: relative;
            animation: stability-wave 6s infinite;
        }

        .solution-title {
            color: #22c55e;
            font-weight: bold;
            font-size: 1.4rem;
            margin-bottom: 1.5rem;
            text-align: center;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 1rem;
        }

        /* 矩阵计算样式 */
        .matrix-section {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.15), rgba(124, 58, 237, 0.1));
            border: 2px solid rgba(139, 92, 246, 0.4);
            border-radius: 20px;
            padding: 2.5rem;
            margin: 2rem 0;
            position: relative;
        }

        .matrix-title {
            color: #8b5cf6;
            font-weight: bold;
            font-size: 1.4rem;
            margin-bottom: 1.5rem;
            text-align: center;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 1rem;
        }

        /* 矩阵显示样式 */
        .matrix {
            display: inline-block;
            background: rgba(15, 23, 42, 0.9);
            border: 2px solid rgba(139, 92, 246, 0.4);
            border-radius: 8px;
            padding: 1rem;
            margin: 0.5rem;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
            position: relative;
        }

        .matrix-animated {
            animation: matrix-pulse 3s infinite;
        }

        .matrix table {
            border-collapse: collapse;
            margin: 0;
        }

        .matrix td {
            padding: 0.3rem 0.6rem;
            text-align: center;
            border: 1px solid rgba(139, 92, 246, 0.2);
            min-width: 60px;
        }

        .matrix-label {
            position: absolute;
            top: -15px;
            left: 10px;
            background: rgba(139, 92, 246, 0.9);
            color: white;
            padding: 0.2rem 0.8rem;
            border-radius: 12px;
            font-size: 0.8rem;
            font-weight: bold;
        }

        /* 计算步骤样式 */
        .calculation-step {
            background: rgba(15, 23, 42, 0.9);
            border: 1px solid rgba(34, 197, 94, 0.3);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
        }

        .step-number {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 30px;
            height: 30px;
            background: linear-gradient(45deg, #22c55e, #16a34a);
            color: white;
            border-radius: 50%;
            font-weight: bold;
            margin-right: 1rem;
            font-size: 0.9rem;
        }

        .step-title {
            color: #22c55e;
            font-weight: bold;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
        }

        /* 数值示例样式 */
        .numerical-example {
            background: rgba(34, 197, 94, 0.1);
            border: 2px solid rgba(34, 197, 94, 0.3);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
        }

        .numerical-title {
            color: #22c55e;
            font-weight: bold;
            margin-bottom: 1rem;
            text-align: center;
        }

        /* 数学公式样式 */
        .math-formula {
            background: rgba(15, 23, 42, 0.9);
            border: 2px solid rgba(139, 92, 246, 0.4);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
            text-align: center;
            font-family: 'JetBrains Mono', monospace;
        }

        .fraction {
            display: inline-block;
            text-align: center;
            vertical-align: middle;
            margin: 0 0.5rem;
        }

        .fraction .numerator {
            display: block;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 0.2rem;
            margin-bottom: 0.2rem;
        }

        .fraction .denominator {
            display: block;
            padding-top: 0.2rem;
        }

        /* 实验数据表格 */
        .data-table {
            background: rgba(15, 23, 42, 0.9);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            overflow-x: auto;
        }

        .data-table table {
            width: 100%;
            border-collapse: collapse;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.9rem;
        }

        .data-table th {
            background: rgba(139, 92, 246, 0.3);
            color: #8b5cf6;
            padding: 1rem;
            border: 1px solid rgba(139, 92, 246, 0.3);
            font-weight: bold;
        }

        .data-table td {
            padding: 0.8rem;
            border: 1px solid rgba(71, 85, 105, 0.3);
            color: #cbd5e1;
            text-align: center;
        }

        .data-table .before-data {
            background: rgba(239, 68, 68, 0.1);
            color: #ef4444;
        }

        .data-table .after-data {
            background: rgba(34, 197, 94, 0.1);
            color: #22c55e;
        }

        .data-table .highlight {
            animation: matrix-highlight 2s infinite;
        }

        /* 层归一化历史 */
        .history-timeline {
            background: rgba(15, 23, 42, 0.9);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
            position: relative;
        }

        .timeline-item {
            display: flex;
            align-items: center;
            margin: 1.5rem 0;
            position: relative;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: 30px;
            top: 40px;
            width: 2px;
            height: 60px;
            background: linear-gradient(to bottom, rgba(139, 92, 246, 0.5), rgba(139, 92, 246, 0.2));
        }

        .timeline-item:last-child::before {
            display: none;
        }

        .timeline-dot {
            width: 60px;
            height: 60px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            margin-right: 2rem;
            flex-shrink: 0;
        }

        .timeline-content {
            flex: 1;
            background: rgba(255, 255, 255, 0.05);
            padding: 1.5rem;
            border-radius: 8px;
        }

        /* 可视化样式 */
        .normalization-visual {
            background: rgba(15, 23, 42, 0.9);
            border-radius: 12px;
            padding: 2rem;
            border: 1px solid rgba(34, 197, 94, 0.3);
            margin: 1.5rem 0;
        }

        .distribution-chart {
            display: flex;
            justify-content: space-around;
            align-items: flex-end;
            height: 200px;
            background: rgba(30, 41, 59, 0.6);
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .distribution-bar {
            width: 20px;
            border-radius: 2px;
            transition: all 0.3s ease;
            position: relative;
            cursor: pointer;
        }

        .distribution-bar:hover {
            transform: scale(1.1);
        }

        .distribution-bar.normalize {
            animation: data-normalize 3s infinite;
        }

        .distribution-label {
            position: absolute;
            bottom: -25px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 0.7rem;
            color: #64748b;
        }

        /* 代码示例样式 */
        .code-example {
            background: rgba(15, 23, 42, 0.95);
            border: 1px solid rgba(139, 92, 246, 0.3);
            border-radius: 12px;
            padding: 1.5rem;
            margin: 1rem 0;
            font-family: 'JetBrains Mono', monospace;
            overflow-x: auto;
        }

        .code-example pre {
            margin: 0;
            color: #e2e8f0;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .code-highlight {
            color: #22c55e;
            font-weight: bold;
        }

        .code-comment {
            color: #64748b;
            font-style: italic;
        }

        .code-keyword {
            color: #8b5cf6;
        }

        .code-string {
            color: #fbbf24;
        }

        /* 响应式设计 */
        @media (max-width: 768px) {
            .chapter-container {
                padding: 1rem;
            }

            .chapter-header h1 {
                font-size: 2rem;
            }

            .meta-tags {
                flex-direction: column;
                align-items: center;
            }

            .solution-section {
                padding: 1.5rem;
            }

            .data-table {
                font-size: 0.8rem;
            }

            .matrix {
                font-size: 0.8rem;
            }

            .matrix td {
                padding: 0.2rem 0.4rem;
                min-width: 40px;
            }
        }
    </style>
</head>
<body>

<div class="chapter-container">
    <!-- 章节头部 -->
    <div class="chapter-header">
        <h1>第7章：让训练变稳定</h1>
        <p>从理论推导到实验验证：残差连接与层归一化的科学之路</p>
        <div class="meta-tags">
            <span class="meta-tag">
                🔧 <span>深度解析</span>
            </span>
            <span class="meta-tag">
                ⏱️ <span>120分钟</span>
            </span>
            <span class="meta-tag">
                🚀 <span>突破性进展</span>
            </span>
            <span class="meta-tag">
                🧮 <span>完整矩阵推导</span>
            </span>
        </div>
    </div>

    <!-- 🔥 问题重现：真实数据说话 -->
    <div class="solution-section" style="border-color: #ef4444;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(239, 68, 68, 0.2), rgba(220, 38, 38, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(239, 68, 68, 0.4);">
                <span style="font-size: 2rem;">🔥</span>
                <h2 style="color: #ef4444; margin: 0; font-size: 1.8rem; font-weight: bold;">问题重现：训练崩溃的真实数据</h2>
            </div>
        </div>

        <!-- 实际梯度爆炸数据 -->
        <div class="data-table">
            <div style="color: #ef4444; font-weight: bold; margin-bottom: 1rem; text-align: center;">📊 6层Transformer训练过程中的梯度变化</div>
            <table>
                <thead>
                <tr>
                    <th>训练步数</th>
                    <th>第1层梯度范数</th>
                    <th>第3层梯度范数</th>
                    <th>第6层梯度范数</th>
                    <th>损失值</th>
                    <th>状态</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>100</td>
                    <td class="after-data">0.8</td>
                    <td class="after-data">0.9</td>
                    <td class="after-data">1.2</td>
                    <td class="after-data">4.2</td>
                    <td style="color: #22c55e;">正常</td>
                </tr>
                <tr>
                    <td>500</td>
                    <td class="before-data">2.1</td>
                    <td class="before-data">8.7</td>
                    <td class="before-data highlight">15.3</td>
                    <td class="before-data">2.8</td>
                    <td style="color: #fbbf24;">开始不稳定</td>
                </tr>
                <tr>
                    <td>800</td>
                    <td class="before-data">0.01</td>
                    <td class="before-data">0.003</td>
                    <td class="before-data highlight">0.0001</td>
                    <td class="before-data">3.9</td>
                    <td style="color: #ef4444;">梯度消失</td>
                </tr>
                <tr>
                    <td>1200</td>
                    <td class="before-data">27.8</td>
                    <td class="before-data">189.2</td>
                    <td class="before-data highlight">1547.6</td>
                    <td class="before-data">NaN</td>
                    <td style="color: #ef4444;">完全崩溃</td>
                </tr>
                </tbody>
            </table>
        </div>

        <!-- 矩阵视角的梯度爆炸分析 -->
        <div class="matrix-section">
            <div class="matrix-title">
                <span>🧮</span>
                <span>矩阵视角：梯度爆炸的数学本质</span>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">1</div>
                    梯度传播的矩阵连乘问题
                </div>

                <div style="text-align: center; margin: 2rem 0;">
                    <div style="color: #cbd5e1; margin-bottom: 1rem;">
                        假设输入序列长度为4，隐藏维度为3，梯度传播涉及雅可比矩阵连乘：
                    </div>

                    <div class="math-formula" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3);">
                        <div style="color: #ef4444; font-size: 1.1rem;">
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂X₀</span>
                            </div>
                            =
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂X₆</span>
                            </div>
                            ×
                            <div class="fraction">
                                <span class="numerator">∂X₆</span>
                                <span class="denominator">∂X₅</span>
                            </div>
                            × ... ×
                            <div class="fraction">
                                <span class="numerator">∂X₁</span>
                                <span class="denominator">∂X₀</span>
                            </div>
                        </div>
                    </div>

                    <div class="numerical-example">
                        <div class="numerical-title">🔢 具体数值计算示例</div>

                        <div style="text-align: center;">
                            <div style="color: #cbd5e1; margin-bottom: 1rem;">假设每层的雅可比矩阵特征值为：</div>

                            <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; margin: 1rem 0;">
                                <div class="matrix">
                                    <div class="matrix-label">J₁</div>
                                    <table>
                                        <tr><td>2.1</td><td>0.5</td><td>-0.3</td></tr>
                                        <tr><td>0.8</td><td>1.9</td><td>0.7</td></tr>
                                        <tr><td>-0.4</td><td>1.2</td><td>2.3</td></tr>
                                    </table>
                                </div>

                                <div style="color: #fbbf24; display: flex; align-items: center;">×</div>

                                <div class="matrix">
                                    <div class="matrix-label">J₂</div>
                                    <table>
                                        <tr><td>1.8</td><td>-0.6</td><td>0.9</td></tr>
                                        <tr><td>1.1</td><td>2.4</td><td>-0.8</td></tr>
                                        <tr><td>0.7</td><td>0.5</td><td>1.7</td></tr>
                                    </table>
                                </div>

                                <div style="color: #fbbf24; display: flex; align-items: center;">× ...</div>
                            </div>

                            <div style="background: rgba(239, 68, 68, 0.1); padding: 1.5rem; border-radius: 8px; margin-top: 1rem;">
                                <div style="color: #ef4444; font-weight: bold; margin-bottom: 0.5rem;">计算结果：</div>
                                <div style="color: #cbd5e1; font-size: 0.9rem;">
                                    最大特征值连乘：2.3 × 2.4 × 2.1 × 1.9 × 2.2 × 2.0 ≈ <strong style="color: #ef4444;">275.6</strong><br>
                                    梯度放大了275倍！导致参数更新失控。
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- 数据分布漂移 -->
        <div class="data-table">
            <div style="color: #ef4444; font-weight: bold; margin-bottom: 1rem; text-align: center;">📈 各层激活值统计（训练步数=500）</div>
            <table>
                <thead>
                <tr>
                    <th>网络层</th>
                    <th>均值 (μ)</th>
                    <th>标准差 (σ)</th>
                    <th>最小值</th>
                    <th>最大值</th>
                    <th>分布状态</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>输入层</td>
                    <td class="after-data">0.02</td>
                    <td class="after-data">0.98</td>
                    <td>-2.1</td>
                    <td>2.3</td>
                    <td style="color: #22c55e;">标准正态</td>
                </tr>
                <tr>
                    <td>第2层</td>
                    <td class="before-data">0.47</td>
                    <td class="before-data">1.85</td>
                    <td>-3.8</td>
                    <td>5.2</td>
                    <td style="color: #fbbf24;">轻微偏移</td>
                </tr>
                <tr>
                    <td>第4层</td>
                    <td class="before-data highlight">1.23</td>
                    <td class="before-data highlight">4.67</td>
                    <td>-8.1</td>
                    <td>12.9</td>
                    <td style="color: #ef4444;">严重偏移</td>
                </tr>
                <tr>
                    <td>第6层</td>
                    <td class="before-data highlight">3.89</td>
                    <td class="before-data highlight">15.42</td>
                    <td>-31.7</td>
                    <td>47.3</td>
                    <td style="color: #ef4444;">完全失控</td>
                </tr>
                </tbody>
            </table>
        </div>

        <div style="background: rgba(239, 68, 68, 0.1); padding: 2rem; border-radius: 12px; border: 1px solid rgba(239, 68, 68, 0.3); margin-top: 2rem; text-align: center;">
            <div style="color: #ef4444; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem;">💥 核心问题确认</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                <strong style="color: #fbbf24;">梯度爆炸</strong>：深层梯度超过1000倍，参数更新失控<br>
                <strong style="color: #8b5cf6;">分布漂移</strong>：每层数据分布剧烈变化，训练目标不断"移动"<br>
                <strong style="color: #ef4444;">训练崩溃</strong>：最终loss变为NaN，训练完全失败
            </div>
        </div>
    </div>

    <!-- 🤔 思考过程：残差连接是怎么想到的？ -->
    <div class="solution-section" style="border-color: #8b5cf6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">🤔</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">思考历程：残差连接是怎么想到的？</h2>
            </div>
        </div>

        <!-- 思考的起点 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🌟 科学发现的起点：观察与困惑</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem;">研究生的困惑时刻：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; font-style: italic;">
                    "奇怪... 理论上更深的网络应该更强大才对。18层网络包含了6层网络的所有可能性，还有额外的12层来学习更复杂的特征。<br><br>

                    但为什么实验结果完全相反？6层网络性能还不错，12层就开始变差，18层直接训练失败..."<br><br>

                    <strong style="color: #ef4444;">"是不是我们对深度网络的理解有根本性的错误？"</strong>
                </div>
            </div>
        </div>

        <!-- 初步尝试与失败 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #ef4444; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">❌ 初步尝试：常规方法的失败</div>

            <div style="display: grid; gap: 1.5rem;">
                <!-- 尝试1 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                    <h4 style="color: #ef4444; margin-bottom: 0.5rem;">🔧 尝试1：调整学习率</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>想法</strong>：可能是学习率太大，导致梯度爆炸<br>
                        <strong>行动</strong>：将学习率从0.001降到0.0001，再到0.00001<br>
                        <strong>结果</strong>：学习率小了，梯度爆炸缓解，但现在变成梯度消失，训练极慢<br>
                        <strong style="color: #ef4444;">结论</strong>：治标不治本
                    </div>
                </div>

                <!-- 尝试2 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                    <h4 style="color: #ef4444; margin-bottom: 0.5rem;">⚖️ 尝试2：精心初始化权重</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>想法</strong>：也许是Xavier初始化不适合深网络<br>
                        <strong>行动</strong>：尝试He初始化、正交初始化、各种方差调整<br>
                        <strong>结果</strong>：有微小改善，但深层网络依然训练困难<br>
                        <strong style="color: #ef4444;">结论</strong>：仍然不是根本解决方案
                    </div>
                </div>

                <!-- 尝试3 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                    <h4 style="color: #ef4444; margin-bottom: 0.5rem;">🔄 尝试3：梯度裁剪</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>想法</strong>：人为限制梯度大小，防止爆炸<br>
                        <strong>行动</strong>：当梯度范数>1时，缩放到1<br>
                        <strong>结果</strong>：训练不再崩溃，但收敛极慢，性能很差<br>
                        <strong style="color: #ef4444;">结论</strong>：像给跑车装限速器，不是正确方向
                    </div>
                </div>
            </div>

            <div style="background: rgba(239, 68, 68, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(239, 68, 68, 0.3); margin-top: 1.5rem; text-align: center;">
                <strong style="color: #ef4444; font-size: 1.1rem;">💔 挫败感：所有常规方法都失败了</strong><br>
                <span style="color: #cbd5e1;">这些都是"头痛医头，脚痛医脚"的做法，没有触及问题的本质...</span>
            </div>
        </div>

        <!-- 灵感时刻 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">💡 灵感来袭：跨领域的启发</div>

            <div style="display: grid; grid-template-columns: 1fr auto 1fr; gap: 2rem; align-items: center; margin-bottom: 2rem;">
                <div style="text-align: center;">
                    <div style="width: 80px; height: 80px; border-radius: 50%; background: linear-gradient(135deg, #8b5cf6, #7c3aed); display: flex; align-items: center; justify-content: center; font-size: 2rem; margin: 0 auto 1rem; box-shadow: 0 8px 25px rgba(139, 92, 246, 0.3);">
                        📚
                    </div>
                    <div style="color: #cbd5e1; font-size: 1rem;">
                        <strong style="color: #8b5cf6;">深夜阅读</strong><br>
                        计算机视觉论文
                    </div>
                </div>

                <div style="color: #fbbf24; font-size: 2rem;">⚡</div>

                <div style="text-align: center;">
                    <div style="width: 80px; height: 80px; border-radius: 50%; background: linear-gradient(135deg, #fbbf24, #f59e0b); display: flex; align-items: center; justify-content: center; font-size: 2rem; margin: 0 auto 1rem; box-shadow: 0 8px 25px rgba(251, 191, 36, 0.3);">
                        🏗️
                    </div>
                    <div style="color: #cbd5e1; font-size: 1rem;">
                        <strong style="color: #fbbf24;">ResNet论文</strong><br>
                        152层网络成功
                    </div>
                </div>
            </div>

            <div style="background: rgba(251, 191, 36, 0.1); padding: 2rem; border-radius: 8px; border: 1px solid rgba(251, 191, 36, 0.3);">
                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem;">💭 关键洞察时刻：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; font-style: italic;">
                    "等等... 2015年的ResNet论文说他们训练了152层的网络？这不可能！他们是怎么做到的？"<br><br>

                    <strong style="color: #22c55e;">阅读ResNet核心思想</strong>：<br>
                    "我们让网络学习残差函数F(x)，而不是直接学习目标映射H(x)。<br>
                    最终输出是 H(x) = F(x) + x"<br><br>

                    <strong style="color: #8b5cf6;">"这个'+x'..."</strong> 突然眼前一亮！
                </div>
            </div>
        </div>

        <!-- 思维跳跃的过程 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🧠 思维跳跃：从视觉到语言</div>

            <div style="display: grid; gap: 1.5rem;">
                <!-- 第一步思考 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #06b6d4; margin-bottom: 1rem;">🤔 第一步：理解ResNet的精髓</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>原理分析</strong>：ResNet不是让网络从零学习H(x)，而是学习残差F(x) = H(x) - x<br>
                        <strong>数学含义</strong>：如果理想映射接近恒等映射，那么学习F(x)≈0比学习H(x)≈x更容易<br>
                        <strong>关键点</strong>：这个"+x"保证了梯度能够直接传播！
                    </div>
                </div>

                <!-- 第二步思考 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #8b5cf6; margin-bottom: 1rem;">💭 第二步：类比到Transformer</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>关键问题</strong>：CNN和Transformer虽然结构不同，但都面临同样的深度训练问题<br>
                        <strong>核心洞察</strong>：梯度传播困难是深度网络的通用问题，不限于特定架构<br>
                        <strong>大胆假设</strong>：如果ResNet的残差连接能解决CNN的问题，也许能解决Transformer的问题？
                    </div>
                </div>

                <!-- 第三步思考 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem;">⚡ 第三步：应用到自注意力</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>原始做法</strong>：x₂ = Attention(x₁)<br>
                        <strong>ResNet启发</strong>：x₂ = x₁ + Attention(x₁)<br>
                        <strong>直觉理解</strong>：即使注意力学得不好，至少原始信息x₁还在<br>
                        <strong>期望效果</strong>：梯度可以通过"+x₁"这条路径直接传播
                    </div>
                </div>
            </div>
        </div>

        <!-- 第一个验证实验 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🧪 验证实验：小心翼翼的第一步</div>

            <div style="background: rgba(6, 182, 212, 0.1); padding: 2rem; border-radius: 8px; border: 1px solid rgba(6, 182, 212, 0.3);">
                <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem;">💻 简单实验设计：</div>
                <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6; margin-bottom: 1.5rem;">
                    <strong>对照组</strong>：原始6层Transformer（训练已经有问题）<br>
                    <strong>实验组</strong>：同样6层，但加入残差连接<br>
                    <strong>期望</strong>：如果理论正确，实验组应该训练更稳定
                </div>

                <!-- 实验代码 -->
                <div class="code-example" style="margin: 1rem 0;">
                    <pre><code><span class="code-comment"># 只改一行代码的最小验证</span>
<span class="code-keyword">def</span> <span class="code-highlight">transformer_layer_original</span>(x):
    attention_out = self.attention(x)
    <span class="code-keyword">return</span> attention_out  <span class="code-comment"># 原始版本</span>

<span class="code-keyword">def</span> <span class="code-highlight">transformer_layer_residual</span>(x):
    attention_out = self.attention(x)
    <span class="code-keyword">return</span> <span style="color: #22c55e;">x + attention_out</span>  <span class="code-comment"># 加入残差连接</span></code></pre>
                </div>

                <div style="background: rgba(22, 197, 94, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                    <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">🎉 第一次成功！</div>
                    <div style="color: #cbd5e1; font-size: 0.95rem;">
                        仅仅加了一个"+x"，6层网络的训练就稳定了很多！<br>
                        梯度范数从发散变成了稳定的1.2左右，损失也开始正常下降。
                    </div>
                </div>
            </div>
        </div>

        <!-- 深入理解 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🔬 深入理解：为什么有效？</div>

            <div style="display: grid; gap: 1.5rem;">
                <!-- 梯度传播分析 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem;">🧮 数学角度：梯度传播</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>关键洞察</strong>：残差连接创造了梯度的"高速公路"<br>
                        原来：∂L/∂x = ∂L/∂y × ∂F(x)/∂x （完全依赖F的梯度）<br>
                        现在：∂L/∂x = ∂L/∂y × (1 + ∂F(x)/∂x) （有"1"保底）<br>
                        <strong style="color: #22c55e;">即使F学得很差，梯度依然能传播！</strong>
                    </div>
                </div>

                <!-- 学习角度分析 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem;">🎯 学习角度：更容易的目标</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>原始任务</strong>：学习完整的变换H(x)<br>
                        <strong>新任务</strong>：学习残差F(x) = H(x) - x<br>
                        <strong>优势</strong>：如果理想变换接近恒等映射，F(x)≈0更容易学习<br>
                        <strong style="color: #22c55e;">网络可以从"做微调"开始，而不是"从零重建"</strong>
                    </div>
                </div>

                <!-- 表示能力分析 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #06b6d4; margin-bottom: 1rem;">🔄 表示能力：不损失任何东西</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>重要发现</strong>：残差连接不会降低网络的表达能力<br>
                        <strong>原因</strong>：如果F能学习到-x，那么x+F(x)=x+(-x)=0<br>
                        <strong>含义</strong>：理论上可以表示任何原网络能表示的函数<br>
                        <strong style="color: #22c55e;">只是让学习过程更容易，而不限制学习能力</strong>
                    </div>
                </div>
            </div>
        </div>

        <!-- 进一步验证 -->
        <div style="background: linear-gradient(135deg, rgba(34, 197, 94, 0.15), rgba(16, 185, 129, 0.1)); padding: 2rem; border-radius: 12px; border: 1px solid rgba(34, 197, 94, 0.3);">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; text-align: center;">🚀 成功验证：从6层到12层，再到24层</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; text-align: center;">
                有了残差连接，我们不仅解决了6层的问题，<br>
                还成功训练了12层、18层，甚至24层的Transformer！<br><br>
                <strong style="color: #fbbf24;">一个简单的"+x"，开启了深度网络的新时代</strong><br><br>
                <span style="color: #8b5cf6; font-style: italic;">"有时候，最深刻的洞察来自最简单的想法"</span>
            </div>
        </div>
    </div>

    <!-- 💡 解决方案1：残差连接的完整矩阵推导 -->
    <div class="solution-discovery">
        <div class="solution-title">
            <span>💡</span>
            <span>解决方案1：残差连接的完整矩阵推导</span>
        </div>

        <!-- 梯度传播的数学分析 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">📐 梯度传播的数学推导</div>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                <!-- 原始网络 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #ef4444; margin-bottom: 1rem; text-align: center;">❌ 原始网络梯度传播</h4>

                    <div style="margin-bottom: 1rem; color: #cbd5e1;">
                        假设网络层数为L，第l层的输出为 x<sub>l</sub>：
                    </div>

                    <div class="math-formula" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3);">
                        <div style="color: #ef4444; margin-bottom: 1rem;">x<sub>l</sub> = F<sub>l</sub>(x<sub>l-1</sub>)</div>
                        <div style="color: #cbd5e1; font-size: 0.9rem;">每层都是前一层的复杂变换</div>
                    </div>

                    <div style="margin: 1rem 0; color: #cbd5e1;">
                        根据链式法则，梯度传播为：
                    </div>

                    <div class="math-formula" style="background: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.3);">
                        <div style="color: #ef4444; font-size: 1.1rem;">
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂x<sub>0</sub></span>
                            </div>
                            =
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂x<sub>L</sub></span>
                            </div>
                            ×
                            <div class="fraction">
                                <span class="numerator">∂x<sub>L</sub></span>
                                <span class="denominator">∂x<sub>L-1</sub></span>
                            </div>
                            × ... ×
                            <div class="fraction">
                                <span class="numerator">∂x<sub>1</sub></span>
                                <span class="denominator">∂x<sub>0</sub></span>
                            </div>
                        </div>
                    </div>

                    <div style="background: rgba(239, 68, 68, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                        <strong style="color: #ef4444;">问题</strong>：L个偏导数连乘<br>
                        • 如果每个 > 1 → 梯度爆炸<br>
                        • 如果每个 < 1 → 梯度消失
                    </div>
                </div>

                <!-- 残差网络 -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem; text-align: center;">✅ 残差网络梯度传播</h4>

                    <div style="margin-bottom: 1rem; color: #cbd5e1;">
                        残差连接改变了前向传播：
                    </div>

                    <div class="math-formula" style="background: rgba(34, 197, 94, 0.1); border-color: rgba(34, 197, 94, 0.3);">
                        <div style="color: #22c55e; margin-bottom: 1rem;">x<sub>l</sub> = x<sub>l-1</sub> + F<sub>l</sub>(x<sub>l-1</sub>)</div>
                        <div style="color: #cbd5e1; font-size: 0.9rem;">保留原始信息 + 添加新特征</div>
                    </div>

                    <div style="margin: 1rem 0; color: #cbd5e1;">
                        对应的梯度计算变为：
                    </div>

                    <div class="math-formula" style="background: rgba(34, 197, 94, 0.1); border-color: rgba(34, 197, 94, 0.3);">
                        <div style="color: #22c55e; font-size: 1.1rem;">
                            <div class="fraction">
                                <span class="numerator">∂x<sub>l</sub></span>
                                <span class="denominator">∂x<sub>l-1</sub></span>
                            </div>
                            = 1 +
                            <div class="fraction">
                                <span class="numerator">∂F<sub>l</sub>(x<sub>l-1</sub>)</span>
                                <span class="denominator">∂x<sub>l-1</sub></span>
                            </div>
                        </div>
                    </div>

                    <div class="math-formula" style="background: rgba(34, 197, 94, 0.1); border-color: rgba(34, 197, 94, 0.3);">
                        <div style="color: #22c55e; font-size: 1.1rem;">
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂x<sub>0</sub></span>
                            </div>
                            =
                            <div class="fraction">
                                <span class="numerator">∂L</span>
                                <span class="denominator">∂x<sub>L</sub></span>
                            </div>
                            ×
                            <span style="color: #fbbf24;">[∏(1 + ∂F<sub>l</sub>/∂x<sub>l-1</sub>)]</span>
                        </div>
                    </div>

                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; margin-top: 1rem;">
                        <strong style="color: #22c55e;">优势</strong>：每项都有"1"保底<br>
                        • 即使 ∂F/∂x 很小，梯度依然能传播<br>
                        • 提供了梯度的"高速通道"
                    </div>
                </div>
            </div>
        </div>

        <!-- 完整矩阵计算示例 -->
        <div class="matrix-section">
            <div class="matrix-title">
                <span>🧮</span>
                <span>完整矩阵计算示例</span>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">1</div>
                    残差连接的前向传播矩阵形式
                </div>

                <div style="text-align: center; margin: 2rem 0;">
                    <div style="color: #cbd5e1; margin-bottom: 1rem;">
                        原始：X₁ = F₁(X₀)　　残差：X₁ = X₀ + F₁(X₀)
                    </div>

                    <div style="display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap;">
                        <div class="matrix matrix-animated">
                            <div class="matrix-label">X₀</div>
                            <table>
                                <tr><td>0.5</td><td>-1.2</td><td>0.8</td></tr>
                                <tr><td>-0.3</td><td>1.5</td><td>-0.6</td></tr>
                                <tr><td>0.9</td><td>0.2</td><td>1.1</td></tr>
                                <tr><td>-0.7</td><td>0.4</td><td>-0.9</td></tr>
                            </table>
                        </div>

                        <div style="color: #22c55e; font-size: 1.2rem;">+</div>

                        <div class="matrix">
                            <div class="matrix-label">F₁(X₀)</div>
                            <table>
                                <tr><td>0.7</td><td>-1.9</td><td>2.0</td></tr>
                                <tr><td>-2.0</td><td>3.0</td><td>-1.0</td></tr>
                                <tr><td>3.0</td><td>1.0</td><td>4.0</td></tr>
                                <tr><td>-1.0</td><td>2.0</td><td>-2.0</td></tr>
                            </table>
                        </div>

                        <div style="color: #22c55e; font-size: 1.2rem;">=</div>

                        <div class="matrix">
                            <div class="matrix-label">X₁</div>
                            <table>
                                <tr><td>1.2</td><td>-3.1</td><td>2.8</td></tr>
                                <tr><td>-2.3</td><td>4.5</td><td>-1.6</td></tr>
                                <tr><td>3.9</td><td>1.2</td><td>5.1</td></tr>
                                <tr><td>-1.7</td><td>2.4</td><td>-2.9</td></tr>
                            </table>
                        </div>
                    </div>
                </div>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">2</div>
                    雅可比矩阵的关键变化
                </div>

                <div style="margin: 2rem 0;">
                    <div style="color: #cbd5e1; margin-bottom: 1rem; text-align: center;">
                        残差连接改变了雅可比矩阵的结构：
                    </div>

                    <div class="numerical-example">
                        <div class="numerical-title">🧮 具体矩阵计算</div>

                        <div style="text-align: center;">
                            <div style="display: flex; justify-content: center; gap: 2rem; align-items: center; flex-wrap: wrap; margin: 1rem 0;">
                                <div class="matrix">
                                    <div class="matrix-label">I (单位矩阵)</div>
                                    <table>
                                        <tr><td style="background: rgba(34, 197, 94, 0.2);">1</td><td>0</td><td>0</td></tr>
                                        <tr><td>0</td><td style="background: rgba(34, 197, 94, 0.2);">1</td><td>0</td></tr>
                                        <tr><td>0</td><td>0</td><td style="background: rgba(34, 197, 94, 0.2);">1</td></tr>
                                    </table>
                                </div>

                                <div style="color: #22c55e; font-size: 1.2rem;">+</div>

                                <div class="matrix">
                                    <div class="matrix-label">∂F₁/∂X₀</div>
                                    <table>
                                        <tr><td>0.3</td><td>-0.1</td><td>0.2</td></tr>
                                        <tr><td>0.1</td><td>0.4</td><td>-0.2</td></tr>
                                        <tr><td>-0.2</td><td>0.2</td><td>0.3</td></tr>
                                    </table>
                                </div>

                                <div style="color: #22c55e; font-size: 1.2rem;">=</div>

                                <div class="matrix matrix-animated">
                                    <div class="matrix-label">∂X₁/∂X₀</div>
                                    <table>
                                        <tr><td style="background: rgba(34, 197, 94, 0.2);">1.3</td><td>-0.1</td><td>0.2</td></tr>
                                        <tr><td>0.1</td><td style="background: rgba(34, 197, 94, 0.2);">1.4</td><td>-0.2</td></tr>
                                        <tr><td>-0.2</td><td>0.2</td><td style="background: rgba(34, 197, 94, 0.2);">1.3</td></tr>
                                    </table>
                                </div>
                            </div>

                            <div style="background: rgba(34, 197, 94, 0.1); padding: 1.5rem; border-radius: 8px; margin-top: 1rem;">
                                <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">🎯 关键观察：</div>
                                <div style="color: #cbd5e1; font-size: 0.9rem;">
                                    • 对角线元素都≥1（绿色高亮），保证了梯度传播的基本通道<br>
                                    • 即使∂F₁/∂X₀的元素很小，也不会让整体雅可比矩阵退化<br>
                                    • 这是梯度"高速公路"的数学本质
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">3</div>
                    多层梯度传播的数值验证
                </div>

                <div class="numerical-example">
                    <div class="numerical-title">📊 6层网络的梯度连乘计算</div>

                    <div style="text-align: center;">
                        <div style="color: #cbd5e1; margin-bottom: 1rem;">
                            现在每层的雅可比矩阵都有"I+"的结构：
                        </div>

                        <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1rem; margin: 1rem 0;">
                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                                <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.5rem;">Layer 1</div>
                                <div style="font-family: 'JetBrains Mono', monospace; font-size: 0.8rem;">
                                    最大特征值: 1.4<br>
                                    最小特征值: 1.1
                                </div>
                            </div>
                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                                <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.5rem;">Layer 2</div>
                                <div style="font-family: 'JetBrains Mono', monospace; font-size: 0.8rem;">
                                    最大特征值: 1.3<br>
                                    最小特征值: 1.0
                                </div>
                            </div>
                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                                <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.5rem;">Layer 3-6</div>
                                <div style="font-family: 'JetBrains Mono', monospace; font-size: 0.8rem;">
                                    最大特征值: 1.2±0.1<br>
                                    最小特征值: 1.0±0.1
                                </div>
                            </div>
                        </div>

                        <div style="background: rgba(34, 197, 94, 0.1); padding: 2rem; border-radius: 8px; margin-top: 1rem;">
                            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem;">🧮 最终计算结果：</div>
                            <div style="font-family: 'JetBrains Mono', monospace; color: #cbd5e1; line-height: 1.6;">
                                最大特征值连乘：1.4 × 1.3 × 1.2 × 1.2 × 1.1 × 1.2 ≈ <strong style="color: #22c55e;">3.32</strong><br>
                                最小特征值连乘：1.1 × 1.0 × 1.1 × 1.0 × 1.0 × 1.1 ≈ <strong style="color: #22c55e;">1.33</strong><br><br>
                                <strong style="color: #fbbf24;">梯度范数保持在 1.33 到 3.32 之间，完全在健康范围内！</strong>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- 代码实现 -->
        <div class="code-example">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem;">💻 残差连接的PyTorch实现</div>
            <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn

<span class="code-keyword">class</span> <span class="code-highlight">ResidualTransformerLayer</span>(nn.Module):
    <span class="code-keyword">def</span> <span class="code-highlight">__init__</span>(self, hidden_size, num_heads):
        <span class="code-highlight">super()</span>.__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.feed_forward = FeedForward(hidden_size)

    <span class="code-keyword">def</span> <span class="code-highlight">forward</span>(self, x):
        <span class="code-comment"># 🎯 第一个残差连接：注意力机制</span>
        attention_output = self.attention(x)
        x = <span class="code-string">x + attention_output</span>  <span class="code-comment"># 关键：保留原始信息</span>

        <span class="code-comment"># 🎯 第二个残差连接：前馈网络</span>
        ffn_output = self.feed_forward(x)
        x = <span class="code-string">x + ffn_output</span>          <span class="code-comment"># 关键：再次保留信息</span>

        <span class="code-keyword">return</span> x

<span class="code-comment"># 验证梯度传播效果的测试代码</span>
<span class="code-keyword">def</span> <span class="code-highlight">test_gradient_flow</span>():
    model = ResidualTransformerLayer(512, 8)
    x = torch.randn(32, 100, 512, requires_grad=True)

    <span class="code-comment"># 前向传播</span>
    output = model(x)
    loss = output.sum()

    <span class="code-comment"># 反向传播</span>
    loss.backward()

    <span class="code-comment"># 检查梯度</span>
    <span class="code-keyword">print</span>(<span class="code-string">f"输入梯度范数: {x.grad.norm().item():.4f}"</span>)
    <span class="code-keyword">return</span> x.grad.norm().item()</code></pre>
        </div>
    </div>

    <!-- 📊 残差连接实验验证 -->
    <div class="solution-section">
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(6, 182, 212, 0.2), rgba(8, 145, 178, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(6, 182, 212, 0.4);">
                <span style="font-size: 2rem;">📊</span>
                <h2 style="color: #06b6d4; margin: 0; font-size: 1.8rem; font-weight: bold;">实验验证：残差连接的效果</h2>
            </div>
        </div>

        <!-- 对比实验数据 -->
        <div class="data-table">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem; text-align: center;">🧪 6层网络训练1000步后的对比</div>
            <table>
                <thead>
                <tr>
                    <th>网络类型</th>
                    <th>梯度范数</th>
                    <th>收敛速度</th>
                    <th>最终Loss</th>
                    <th>训练稳定性</th>
                    <th>可训练深度</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td style="color: #ef4444;">原始Transformer</td>
                    <td class="before-data">发散/消失</td>
                    <td class="before-data">无法收敛</td>
                    <td class="before-data">NaN</td>
                    <td class="before-data">极不稳定</td>
                    <td class="before-data">≤4层</td>
                </tr>
                <tr>
                    <td style="color: #22c55e;">+残差连接</td>
                    <td class="after-data">1.2±0.3</td>
                    <td class="after-data">3倍加速</td>
                    <td class="after-data">1.85</td>
                    <td class="after-data">稳定</td>
                    <td class="after-data">12+层</td>
                </tr>
                </tbody>
            </table>
        </div>

        <!-- 详细数据分析 -->
        <div class="data-table">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem; text-align: center;">📈 各层梯度范数变化（加入残差连接后）</div>
            <table>
                <thead>
                <tr>
                    <th>训练步数</th>
                    <th>第1层</th>
                    <th>第3层</th>
                    <th>第6层</th>
                    <th>第9层</th>
                    <th>第12层</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>100</td>
                    <td class="after-data">0.9</td>
                    <td class="after-data">0.8</td>
                    <td class="after-data">0.9</td>
                    <td class="after-data">1.1</td>
                    <td class="after-data">1.0</td>
                </tr>
                <tr>
                    <td>500</td>
                    <td class="after-data">1.2</td>
                    <td class="after-data">1.0</td>
                    <td class="after-data">1.1</td>
                    <td class="after-data">1.3</td>
                    <td class="after-data">1.2</td>
                </tr>
                <tr>
                    <td>1000</td>
                    <td class="after-data">1.1</td>
                    <td class="after-data">0.9</td>
                    <td class="after-data">1.0</td>
                    <td class="after-data">1.2</td>
                    <td class="after-data">1.1</td>
                </tr>
                </tbody>
            </table>
        </div>

        <div style="background: rgba(34, 197, 94, 0.1); padding: 2rem; border-radius: 12px; border: 1px solid rgba(34, 197, 94, 0.3); margin-top: 2rem; text-align: center;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem;">🎉 关键发现</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                <strong style="color: #fbbf24;">梯度范数稳定</strong>：各层梯度保持在0.8-1.3之间，非常健康<br>
                <strong style="color: #8b5cf6;">深度无限制</strong>：成功训练12层网络，理论上可以更深<br>
                <strong style="color: #06b6d4;">训练加速</strong>：收敛速度提升3倍，效率显著改善
            </div>
        </div>
    </div>

    <!-- 🎯 第二个问题：残差连接还不够 -->
    <div class="solution-section" style="border-color: #fbbf24;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(251, 191, 36, 0.2), rgba(245, 158, 11, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(251, 191, 36, 0.4);">
                <span style="font-size: 2rem;">🎯</span>
                <h2 style="color: #fbbf24; margin: 0; font-size: 1.8rem; font-weight: bold;">第二个问题：残差连接还不够</h2>
            </div>
        </div>

        <!-- 残差连接后的新问题 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">😕 成功之后的新困惑</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem;">研究者的观察记录：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; font-style: italic;">
                    "太好了！残差连接真的解决了梯度传播问题。现在12层的网络可以稳定训练了。<br><br>

                    但是... 等等，还有其他奇怪的现象：<br>
                    • 训练初期经常出现损失剧烈震荡<br>
                    • 学习率不能设得太大，否则训练发散<br>
                    • 每层的激活值分布越来越偏离正态分布<br>
                    • 训练速度比期望的要慢很多<br><br>

                    <strong style="color: #fbbf24;">'梯度能传播了，但为什么训练还是这么不稳定？'</strong>"
                </div>
            </div>
        </div>

        <!-- 数据分布问题的发现 -->
        <div class="data-table">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem; text-align: center;">📊 残差连接后各层激活值分布分析</div>
            <table>
                <thead>
                <tr>
                    <th>训练步数</th>
                    <th>第2层 μ/σ</th>
                    <th>第4层 μ/σ</th>
                    <th>第6层 μ/σ</th>
                    <th>分布状态</th>
                    <th>学习效率</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>0 (初始)</td>
                    <td class="after-data">0.01/0.98</td>
                    <td class="after-data">0.02/1.01</td>
                    <td class="after-data">-0.01/0.99</td>
                    <td style="color: #22c55e;">标准正态</td>
                    <td style="color: #22c55e;">高</td>
                </tr>
                <tr>
                    <td>100</td>
                    <td class="before-data">0.23/1.45</td>
                    <td class="before-data">0.67/2.12</td>
                    <td class="before-data">1.34/3.87</td>
                    <td style="color: #fbbf24;">开始偏移</td>
                    <td style="color: #fbbf24;">中等</td>
                </tr>
                <tr>
                    <td>500</td>
                    <td class="before-data highlight">0.89/2.34</td>
                    <td class="before-data highlight">2.45/5.67</td>
                    <td class="before-data highlight">4.12/8.93</td>
                    <td style="color: #ef4444;">严重偏移</td>
                    <td style="color: #ef4444;">低</td>
                </tr>
                <tr>
                    <td>1000</td>
                    <td class="before-data highlight">1.67/4.56</td>
                    <td class="before-data highlight">5.23/12.1</td>
                    <td class="before-data highlight">9.87/23.4</td>
                    <td style="color: #ef4444;">完全失控</td>
                    <td style="color: #ef4444;">极低</td>
                </tr>
                </tbody>
            </table>
        </div>

        <!-- 问题本质分析 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #ef4444; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🔍 深度分析：内部协变量偏移</div>

            <div style="display: grid; gap: 1.5rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #8b5cf6; margin-bottom: 1rem;">🎯 核心问题确认</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>现象</strong>：虽然梯度能传播，但每层的输入分布在训练中不断变化<br>
                        <strong>术语</strong>：这被称为"内部协变量偏移"(Internal Covariate Shift)<br>
                        <strong>后果</strong>：每层都要不断适应新的输入分布，学习效率极低
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem;">📊 具体表现</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        • <strong>分布漂移</strong>：每层输出的均值和方差不断变化<br>
                        • <strong>训练不稳定</strong>：损失函数震荡剧烈，难以收敛<br>
                        • <strong>学习率限制</strong>：不敢用大学习率，怕训练发散<br>
                        • <strong>层间不协调</strong>：深层严重依赖浅层的输出分布
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem;">💡 关键洞察</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>问题根源</strong>：每层的"学习目标"在不断移动<br>
                        <strong>类比理解</strong>：就像射击时靶子在不断移动，很难命中<br>
                        <strong>解决思路</strong>：能否让每层面对的"靶子"都保持稳定？
                    </div>
                </div>
            </div>
        </div>

        <div style="background: rgba(251, 191, 36, 0.1); padding: 2rem; border-radius: 12px; border: 1px solid rgba(251, 191, 36, 0.3); margin-top: 2rem; text-align: center;">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem;">🎯 新的挑战</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                残差连接解决了<strong style="color: #22c55e;">梯度传播</strong>问题，<br>
                但我们还需要解决<strong style="color: #fbbf24;">分布稳定性</strong>问题！<br><br>
                <span style="color: #8b5cf6; font-style: italic;">"下一个突破在哪里？"</span>
            </div>
        </div>
    </div>

    <!-- 🤔 层归一化的思考历程 -->
    <div class="solution-section" style="border-color: #8b5cf6;">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">🤔</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">层归一化的思考历程：如何稳定分布？</h2>
            </div>
        </div>

        <!-- 直觉的产生 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">💭 第一个直觉：向统计学求助</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1rem;">研究者的内心独白：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; font-style: italic;">
                    "分布不稳定... 这让我想起了统计学课上学过的东西。<br><br>

                    在统计分析中，如果数据分布不稳定，我们通常会做什么？<br>
                    <strong style="color: #fbbf24;">标准化(Standardization)！</strong><br><br>

                    把数据转换成均值为0、方差为1的标准正态分布。<br>
                    这样所有数据都在同一个'尺度'上，便于分析。<br><br>

                    <strong style="color: #22c55e;">那么... 能否对神经网络的每一层都做标准化呢？</strong>"
                </div>
            </div>
        </div>

        <!-- 早期尝试：Batch Normalization -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🔬 早期尝试：Batch Normalization的启发</div>

            <div style="display: grid; gap: 1.5rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #fbbf24;">
                    <h4 style="color: #fbbf24; margin-bottom: 0.5rem;">📚 2015年：BatchNorm的出现</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>核心思想</strong>：在批次(batch)维度上做标准化<br>
                        <strong>公式</strong>：BN(x) = γ × (x-μ_batch)/σ_batch + β<br>
                        <strong>效果</strong>：CNN训练稳定性大幅提升，收敛速度加快
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #22c55e;">
                    <h4 style="color: #22c55e; margin-bottom: 0.5rem;">💡 关键启发</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        "哇！标准化真的有效！BatchNorm让CNN训练变得超级稳定。<br>
                        现在的问题是：<strong style="color: #fbbf24;">能否把这个思想应用到RNN和Transformer上？</strong>"
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px; border-left: 4px solid #ef4444;">
                    <h4 style="color: #ef4444; margin-bottom: 0.5rem;">❌ 遇到的问题</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>批次依赖</strong>：BatchNorm需要足够大的batch size才稳定<br>
                        <strong>序列长度问题</strong>：RNN的不同时间步长度不一致<br>
                        <strong>推理时困难</strong>：需要维护全局统计量，计算复杂<br>
                        <strong style="color: #ef4444;">不适合Transformer的注意力机制！</strong>
                    </div>
                </div>
            </div>
        </div>

        <!-- 关键洞察时刻 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">⚡ 关键洞察：换个维度思考</div>

            <div style="display: grid; grid-template-columns: 1fr auto 1fr; gap: 2rem; align-items: center; margin-bottom: 2rem;">
                <div style="text-align: center;">
                    <div style="width: 80px; height: 80px; border-radius: 50%; background: linear-gradient(135deg, #ef4444, #dc2626); display: flex; align-items: center; justify-content: center; font-size: 2rem; margin: 0 auto 1rem; box-shadow: 0 8px 25px rgba(239, 68, 68, 0.3);">
                        📊
                    </div>
                    <div style="color: #cbd5e1; font-size: 1rem;">
                        <strong style="color: #ef4444;">Batch Normalization</strong><br>
                        在批次维度标准化
                    </div>
                </div>

                <div style="color: #22c55e; font-size: 2rem;">→</div>

                <div style="text-align: center;">
                    <div style="width: 80px; height: 80px; border-radius: 50%; background: linear-gradient(135deg, #22c55e, #16a34a); display: flex; align-items: center; justify-content: center; font-size: 2rem; margin: 0 auto 1rem; box-shadow: 0 8px 25px rgba(34, 197, 94, 0.3);">
                        🎯
                    </div>
                    <div style="color: #cbd5e1; font-size: 1rem;">
                        <strong style="color: #22c55e;">Layer Normalization</strong><br>
                        在特征维度标准化
                    </div>
                </div>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 2rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem;">💡 突破性想法：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; font-style: italic;">
                    "等等！为什么一定要在批次维度做标准化？<br><br>

                    Transformer的每个位置都有512个特征（hidden_size=512）。<br>
                    我们可以在<strong style="color: #fbbf24;">这512个特征维度上</strong>做标准化！<br><br>

                    这样：<br>
                    • 不依赖batch size<br>
                    • 每个样本独立处理<br>
                    • 适合任意序列长度<br>
                    • 推理时无需额外计算<br><br>

                    <strong style="color: #22c55e;">'在特征维度标准化' - 这就是Layer Normalization！</strong>"
                </div>
            </div>
        </div>

        <!-- 数学直觉 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🧮 数学直觉：为什么这样有效？</div>

            <div style="display: grid; gap: 1.5rem;">
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #06b6d4; margin-bottom: 1rem;">🎯 核心思想</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>目标</strong>：让每一层的输出都保持"标准"分布<br>
                        <strong>方法</strong>：对每个样本的每个位置，在所有特征维度上标准化<br>
                        <strong>结果</strong>：无论输入如何变化，输出总是 μ≈0, σ≈1
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem;">⚡ 为什么有效？</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        • <strong>分布稳定</strong>：每层都面对相同的输入分布<br>
                        • <strong>梯度友好</strong>：避免激活值过大或过小<br>
                        • <strong>学习率友好</strong>：可以使用更大的学习率<br>
                        • <strong>初始化不敏感</strong>：降低对权重初始化的依赖
                    </div>
                </div>

                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem;">🔬 实验验证思路</h4>
                    <div style="color: #cbd5e1; font-size: 0.95rem; line-height: 1.6;">
                        <strong>假设</strong>：如果Layer Normalization有效，应该观察到：<br>
                        • 训练损失更稳定<br>
                        • 收敛速度更快<br>
                        • 可以使用更大学习率<br>
                        • 各层激活值分布保持稳定
                    </div>
                </div>
            </div>
        </div>

        <div style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.15), rgba(124, 58, 237, 0.1)); padding: 2rem; border-radius: 12px; border: 1px solid rgba(139, 92, 246, 0.3);">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem; text-align: center;">🚀 Layer Normalization诞生</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; text-align: center;">
                从统计学的基本原理出发，<br>
                通过维度转换的巧妙思考，<br>
                Layer Normalization应运而生！<br><br>
                <strong style="color: #fbbf24;">简单的想法，深远的影响</strong><br><br>
                <span style="color: #8b5cf6; font-style: italic;">"最好的解决方案往往最简单"</span>
            </div>
        </div>
    </div>

    <!-- 🧪 第一次验证实验 -->
    <div class="solution-discovery">
        <div class="solution-title">
            <span>🧪</span>
            <span>验证层归一化：从想法到现实</span>
        </div>

        <!-- 实验设计 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🔬 小心翼翼的第一次实验</div>

            <div style="background: rgba(255, 255, 255, 0.05); padding: 2rem; border-radius: 8px;">
                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem;">实验设计思路：</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                    "有了Layer Normalization的想法，现在需要验证它是否真的有效。<br><br>

                    让我设计一个最简单的对比实验：<br>
                    • <strong style="color: #ef4444;">对照组</strong>：6层Transformer + 残差连接（已有问题）<br>
                    • <strong style="color: #22c55e;">实验组</strong>：6层Transformer + 残差连接 + Layer Normalization<br><br>

                    如果理论正确，实验组应该显示：<br>
                    ✓ 各层激活值分布保持稳定<br>
                    ✓ 训练损失更平滑<br>
                    ✓ 可以使用更大的学习率<br>
                    ✓ 收敛速度更快"
                </div>
            </div>
        </div>

        <!-- 实验代码对比 -->
        <div class="code-example">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem;">💻 关键改动：只需添加一行代码</div>
            <pre><code><span class="code-keyword">class</span> <span class="code-highlight">TransformerLayerWithLayerNorm</span>(nn.Module):
    <span class="code-keyword">def</span> <span class="code-highlight">__init__</span>(self, hidden_size, num_heads):
        <span class="code-highlight">super()</span>.__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.feed_forward = FeedForward(hidden_size)

        <span class="code-comment"># 🎯 关键添加：Layer Normalization</span>
        <span style="color: #22c55e;">self.layer_norm1 = LayerNorm(hidden_size)</span>
        <span style="color: #22c55e;">self.layer_norm2 = LayerNorm(hidden_size)</span>

    <span class="code-keyword">def</span> <span class="code-highlight">forward</span>(self, x):
        <span class="code-comment"># 原来：直接残差连接</span>
        <span class="code-comment"># x = x + self.attention(x)</span>

        <span class="code-comment"># 现在：先归一化，再残差连接</span>
        <span style="color: #22c55e;">x = x + self.attention(self.layer_norm1(x))</span>
        <span style="color: #22c55e;">x = x + self.feed_forward(self.layer_norm2(x))</span>

        <span class="code-keyword">return</span> x</code></pre>
        </div>

        <!-- 第一次实验结果 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-top: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🎉 实验结果：超出预期的成功</div>

            <div class="data-table">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; text-align: center;">📊 第一次对比实验结果</div>
                <table>
                    <thead>
                    <tr>
                        <th>指标</th>
                        <th>残差连接</th>
                        <th>+Layer Norm</th>
                        <th>改善幅度</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>训练稳定性</td>
                        <td class="before-data">震荡剧烈</td>
                        <td class="after-data">平滑稳定</td>
                        <td style="color: #22c55e;">显著提升</td>
                    </tr>
                    <tr>
                        <td>收敛速度</td>
                        <td class="before-data">8000步</td>
                        <td class="after-data">3000步</td>
                        <td style="color: #22c55e;">2.7倍加速</td>
                    </tr>
                    <tr>
                        <td>最大学习率</td>
                        <td class="before-data">0.0001</td>
                        <td class="after-data">0.001</td>
                        <td style="color: #22c55e;">10倍提升</td>
                    </tr>
                    <tr>
                        <td>最终性能</td>
                        <td class="before-data">BLEU 23.7</td>
                        <td class="after-data">BLEU 31.4</td>
                        <td style="color: #22c55e;">+32%提升</td>
                    </tr>
                    </tbody>
                </table>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 2rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3); margin-top: 1.5rem;">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem;">💫 意外的发现</div>
                <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                    Layer Normalization的效果远超预期！不仅解决了分布稳定性问题，<br>
                    还带来了训练速度、学习率容忍度、最终性能的全方位提升。<br><br>
                    <strong style="color: #fbbf24;">这个简单的想法，竟然如此强大！</strong>
                </div>
            </div>
        </div>
    </div>

    <!-- 🔬 层归一化的完整矩阵计算 -->
    <div class="solution-section">
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">🔬</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">层归一化的完整矩阵计算</h2>
            </div>
        </div>

        <!-- 数学公式推导 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">📐 层归一化的数学定义</div>

            <div style="display: grid; gap: 1.5rem;">
                <!-- 输入定义 -->
                <div class="math-formula">
                    <div style="color: #06b6d4; margin-bottom: 1rem;">给定输入张量 x ∈ ℝ^(B×L×H)</div>
                    <div style="color: #cbd5e1; font-size: 0.9rem;">
                        B = 批次大小, L = 序列长度, H = 隐藏维度
                    </div>
                </div>

                <!-- 均值计算 -->
                <div class="math-formula">
                    <div style="color: #fbbf24; font-size: 1.1rem; margin-bottom: 0.5rem;">
                        μ =
                        <div class="fraction">
                            <span class="numerator">1</span>
                            <span class="denominator">H</span>
                        </div>
                        <span style="font-size: 1.2rem;">∑</span><sub style="font-size: 0.8rem;">i=1</sub><sup style="font-size: 0.8rem;">H</sup> x<sub>i</sub>
                    </div>
                    <div style="color: #cbd5e1; font-size: 0.9rem;">计算每个位置上所有特征的均值</div>
                </div>

                <!-- 方差计算 -->
                <div class="math-formula">
                    <div style="color: #fbbf24; font-size: 1.1rem; margin-bottom: 0.5rem;">
                        σ² =
                        <div class="fraction">
                            <span class="numerator">1</span>
                            <span class="denominator">H</span>
                        </div>
                        <span style="font-size: 1.2rem;">∑</span><sub style="font-size: 0.8rem;">i=1</sub><sup style="font-size: 0.8rem;">H</sup> (x<sub>i</sub> - μ)²
                    </div>
                    <div style="color: #cbd5e1; font-size: 0.9rem;">计算每个位置上特征的方差</div>
                </div>

                <!-- 归一化公式 -->
                <div class="math-formula" style="background: rgba(34, 197, 94, 0.1); border-color: rgba(34, 197, 94, 0.3);">
                    <div style="color: #22c55e; font-size: 1.2rem; margin-bottom: 0.5rem;">
                        LayerNorm(x) = γ ⊙
                        <div class="fraction">
                            <span class="numerator">(x - μ)</span>
                            <span class="denominator">√(σ² + ε)</span>
                        </div>
                        + β
                    </div>
                    <div style="color: #cbd5e1; font-size: 0.9rem;">
                        γ, β ∈ ℝ^H 是可学习参数；ε = 1×10⁻⁶ 防止除零
                    </div>
                </div>
            </div>
        </div>

        <!-- 完整矩阵计算示例 -->
        <div class="matrix-section">
            <div class="matrix-title">
                <span>🧮</span>
                <span>Layer Normalization的矩阵计算实例</span>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">1</div>
                    输入矩阵与统计量计算
                </div>

                <div style="text-align: center; margin: 2rem 0;">
                    <div style="color: #cbd5e1; margin-bottom: 1rem;">
                        假设输入X为4×3矩阵（4个token，每个3维特征）：
                    </div>

                    <div style="display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap;">
                        <div class="matrix matrix-animated">
                            <div class="matrix-label">输入X</div>
                            <table>
                                <tr><td>2.5</td><td>-1.8</td><td>0.3</td></tr>
                                <tr><td>-0.7</td><td>3.2</td><td>-2.1</td></tr>
                                <tr><td>1.9</td><td>0.6</td><td>4.3</td></tr>
                                <tr><td>-1.2</td><td>2.8</td><td>-0.9</td></tr>
                            </table>
                        </div>

                        <div style="color: #fbbf24; font-size: 1.2rem;">→</div>

                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem;">每行统计量</div>
                            <div style="font-family: 'JetBrains Mono', monospace; color: #cbd5e1; font-size: 0.9rem; line-height: 1.6;">
                                行1: μ₁ = 0.33, σ₁² = 2.17<br>
                                行2: μ₂ = 0.13, σ₂² = 7.23<br>
                                行3: μ₃ = 2.27, σ₃² = 3.51<br>
                                行4: μ₄ = 0.23, σ₄² = 3.97
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">2</div>
                    标准化过程的逐步计算
                </div>

                <div style="margin: 2rem 0;">
                    <div style="color: #cbd5e1; margin-bottom: 1rem; text-align: center;">
                        对第一行进行详细计算：x₁ = [2.5, -1.8, 0.3]
                    </div>

                    <div class="numerical-example">
                        <div class="numerical-title">🧮 逐步计算过程</div>

                        <div style="text-align: left; font-family: 'JetBrains Mono', monospace;">
                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">Step 1: 计算均值</div>
                                <div style="color: #cbd5e1; line-height: 1.6;">
                                    μ₁ = (2.5 + (-1.8) + 0.3) / 3 = 1.0 / 3 = <strong style="color: #22c55e;">0.33</strong>
                                </div>
                            </div>

                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">Step 2: 计算方差</div>
                                <div style="color: #cbd5e1; line-height: 1.6;">
                                    σ₁² = [(2.5-0.33)² + (-1.8-0.33)² + (0.3-0.33)²] / 3<br>
                                    σ₁² = [4.71 + 4.54 + 0.001] / 3 = <strong style="color: #22c55e;">2.17</strong>
                                </div>
                            </div>

                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">Step 3: 标准化</div>
                                <div style="color: #cbd5e1; line-height: 1.6;">
                                    normalized₁ = (x₁ - μ₁) / √(σ₁² + ε)<br>
                                    = [(2.5-0.33), (-1.8-0.33), (0.3-0.33)] / √(2.17 + 1e-6)<br>
                                    = [2.17, -2.13, -0.03] / 1.47<br>
                                    = <strong style="color: #22c55e;">[1.48, -1.45, -0.02]</strong>
                                </div>
                            </div>

                            <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px; margin: 1rem 0;">
                                <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">Step 4: 缩放和偏移 (假设γ=[1,1,1], β=[0,0,0])</div>
                                <div style="color: #cbd5e1; line-height: 1.6;">
                                    output₁ = γ ⊙ normalized₁ + β<br>
                                    = [1,1,1] ⊙ [1.48, -1.45, -0.02] + [0,0,0]<br>
                                    = <strong style="color: #22c55e;">[1.48, -1.45, -0.02]</strong>
                                </div>
                            </div>
                        </div>

                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1.5rem; border-radius: 8px; margin-top: 1rem;">
                            <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">✅ 验证结果：</div>
                            <div style="color: #cbd5e1; font-size: 0.9rem;">
                                输出第一行的均值: (1.48 + (-1.45) + (-0.02)) / 3 ≈ <strong style="color: #22c55e;">0.00</strong><br>
                                输出第一行的方差: 接近 <strong style="color: #22c55e;">1.00</strong><br>
                                成功实现了 μ≈0, σ≈1 的标准化！
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="calculation-step">
                <div class="step-title">
                    <div class="step-number">3</div>
                    完整矩阵的Layer Normalization结果
                </div>

                <div style="text-align: center; margin: 2rem 0;">
                    <div style="display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap;">
                        <div class="matrix">
                            <div class="matrix-label">原始输入</div>
                            <table>
                                <tr><td style="background: rgba(239, 68, 68, 0.2);">2.5</td><td style="background: rgba(239, 68, 68, 0.2);">-1.8</td><td style="background: rgba(239, 68, 68, 0.2);">0.3</td></tr>
                                <tr><td style="background: rgba(239, 68, 68, 0.2);">-0.7</td><td style="background: rgba(239, 68, 68, 0.2);">3.2</td><td style="background: rgba(239, 68, 68, 0.2);">-2.1</td></tr>
                                <tr><td style="background: rgba(239, 68, 68, 0.2);">1.9</td><td style="background: rgba(239, 68, 68, 0.2);">0.6</td><td style="background: rgba(239, 68, 68, 0.2);">4.3</td></tr>
                                <tr><td style="background: rgba(239, 68, 68, 0.2);">-1.2</td><td style="background: rgba(239, 68, 68, 0.2);">2.8</td><td style="background: rgba(239, 68, 68, 0.2);">-0.9</td></tr>
                            </table>
                        </div>

                        <div style="color: #22c55e; font-size: 1.5rem;">LayerNorm</div>

                        <div class="matrix matrix-animated">
                            <div class="matrix-label">标准化输出</div>
                            <table>
                                <tr><td style="background: rgba(34, 197, 94, 0.2);">1.48</td><td style="background: rgba(34, 197, 94, 0.2);">-1.45</td><td style="background: rgba(34, 197, 94, 0.2);">-0.02</td></tr>
                                <tr><td style="background: rgba(34, 197, 94, 0.2);">-0.31</td><td style="background: rgba(34, 197, 94, 0.2);">1.14</td><td style="background: rgba(34, 197, 94, 0.2);">-0.83</td></tr>
                                <tr><td style="background: rgba(34, 197, 94, 0.2);">-0.20</td><td style="background: rgba(34, 197, 94, 0.2);">-0.89</td><td style="background: rgba(34, 197, 94, 0.2);">1.09</td></tr>
                                <tr><td style="background: rgba(34, 197, 94, 0.2);">-0.48</td><td style="background: rgba(34, 197, 94, 0.2);">1.28</td><td style="background: rgba(34, 197, 94, 0.2);">-0.80</td></tr>
                            </table>
                        </div>
                    </div>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin-top: 2rem;">
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <div style="color: #ef4444; font-weight: bold;">原始数据</div>
                            <div style="font-family: 'JetBrains Mono', monospace; color: #cbd5e1; font-size: 0.8rem;">
                                各行均值: [0.33, 0.13, 2.27, 0.23]<br>
                                各行方差: [2.17, 7.23, 3.51, 3.97]<br>
                                分布不一致
                            </div>
                        </div>
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <div style="color: #22c55e; font-weight: bold;">标准化后</div>
                            <div style="font-family: 'JetBrains Mono', monospace; color: #cbd5e1; font-size: 0.8rem;">
                                各行均值: [≈0, ≈0, ≈0, ≈0]<br>
                                各行方差: [≈1, ≈1, ≈1, ≈1]<br>
                                分布完全一致！
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- 应用前后对比可视化 -->
        <div class="normalization-visual">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">📊 层归一化前后的数据分布对比</div>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                <!-- 归一化前 -->
                <div>
                    <h4 style="color: #ef4444; text-align: center; margin-bottom: 1rem;">❌ 归一化前</h4>
                    <div class="distribution-chart">
                        <div class="distribution-bar" style="height: 80%; background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));">
                            <div class="distribution-label">-8.1</div>
                        </div>
                        <div class="distribution-bar" style="height: 45%; background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));">
                            <div class="distribution-label">-2.3</div>
                        </div>
                        <div class="distribution-bar" style="height: 20%; background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));">
                            <div class="distribution-label">0.1</div>
                        </div>
                        <div class="distribution-bar" style="height: 65%; background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));">
                            <div class="distribution-label">3.9</div>
                        </div>
                        <div class="distribution-bar" style="height: 90%; background: linear-gradient(to top, rgba(239, 68, 68, 0.6), rgba(239, 68, 68, 0.9));">
                            <div class="distribution-label">12.7</div>
                        </div>
                    </div>
                    <div style="text-align: center; margin-top: 1rem; color: #ef4444; font-size: 0.9rem;">
                        μ = 3.89, σ = 15.42<br>
                        分布严重偏移
                    </div>
                </div>

                <!-- 归一化后 -->
                <div>
                    <h4 style="color: #22c55e; text-align: center; margin-bottom: 1rem;">✅ 归一化后</h4>
                    <div class="distribution-chart">
                        <div class="distribution-bar normalize" style="height: 30%; background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));">
                            <div class="distribution-label">-1.2</div>
                        </div>
                        <div class="distribution-bar normalize" style="height: 30%; background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));">
                            <div class="distribution-label">-0.4</div>
                        </div>
                        <div class="distribution-bar normalize" style="height: 30%; background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));">
                            <div class="distribution-label">0.0</div>
                        </div>
                        <div class="distribution-bar normalize" style="height: 30%; background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));">
                            <div class="distribution-label">0.6</div>
                        </div>
                        <div class="distribution-bar normalize" style="height: 30%; background: linear-gradient(to top, rgba(34, 197, 94, 0.6), rgba(34, 197, 94, 0.9));">
                            <div class="distribution-label">1.1</div>
                        </div>
                    </div>
                    <div style="text-align: center; margin-top: 1rem; color: #22c55e; font-size: 0.9rem;">
                        μ ≈ 0.0, σ ≈ 1.0<br>
                        标准正态分布
                    </div>
                </div>
            </div>
        </div>

        <!-- 完整代码实现 -->
        <div class="code-example">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem;">💻 层归一化的完整实现</div>
            <pre><code><span class="code-keyword">import</span> torch
<span class="code-keyword">import</span> torch.nn <span class="code-keyword">as</span> nn
<span class="code-keyword">import</span> torch.nn.functional <span class="code-keyword">as</span> F

<span class="code-keyword">class</span> <span class="code-highlight">LayerNormalization</span>(nn.Module):
    <span class="code-string">"""
    层归一化实现
    在最后一个维度（特征维度）上进行归一化
    """</span>
    <span class="code-keyword">def</span> <span class="code-highlight">__init__</span>(self, hidden_size, eps=1e-6):
        <span class="code-highlight">super()</span>.__init__()
        self.hidden_size = hidden_size
        self.eps = eps

        <span class="code-comment"># 可学习的缩放和偏移参数</span>
        self.gamma = nn.Parameter(torch.ones(hidden_size))   <span class="code-comment"># 缩放</span>
        self.beta = nn.Parameter(torch.zeros(hidden_size))   <span class="code-comment"># 偏移</span>

    <span class="code-keyword">def</span> <span class="code-highlight">forward</span>(self, x):
        <span class="code-string">"""
        x: [batch_size, seq_len, hidden_size]
        """</span>
        <span class="code-comment"># 🎯 计算最后一个维度的统计量</span>
        mean = x.mean(dim=-1, keepdim=True)     <span class="code-comment"># [B, L, 1]</span>
        var = x.var(dim=-1, keepdim=True, unbiased=False)  <span class="code-comment"># [B, L, 1]</span>

        <span class="code-comment"># 🎯 标准化</span>
        normalized = (x - mean) / torch.sqrt(var + self.eps)

        <span class="code-comment"># 🎯 缩放和偏移</span>
        output = self.gamma * normalized + self.beta

        <span class="code-keyword">return</span> output

<span class="code-keyword">class</span> <span class="code-highlight">TransformerLayerWithNorm</span>(nn.Module):
    <span class="code-string">"""
    包含层归一化的完整Transformer层
    """</span>
    <span class="code-keyword">def</span> <span class="code-highlight">__init__</span>(self, hidden_size, num_heads, ffn_size):
        <span class="code-highlight">super()</span>.__init__()
        self.attention = MultiHeadAttention(hidden_size, num_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, ffn_size),
            nn.ReLU(),
            nn.Linear(ffn_size, hidden_size)
        )

        <span class="code-comment"># 🔥 两个层归一化层</span>
        self.norm1 = LayerNormalization(hidden_size)
        self.norm2 = LayerNormalization(hidden_size)

    <span class="code-keyword">def</span> <span class="code-highlight">forward</span>(self, x):
        <span class="code-comment"># 🎯 Pre-Norm 设计（先归一化，再计算）</span>

        <span class="code-comment"># 注意力子层：Norm -> Attention -> 残差</span>
        norm_x = self.norm1(x)
        attention_out = self.attention(norm_x)
        x = <span class="code-string">x + attention_out</span>

        <span class="code-comment"># 前馈子层：Norm -> FFN -> 残差</span>
        norm_x = self.norm2(x)
        ffn_out = self.feed_forward(norm_x)
        x = <span class="code-string">x + ffn_out</span>

        <span class="code-keyword">return</span> x

<span class="code-comment"># 🧪 测试代码：观察归一化效果</span>
<span class="code-keyword">def</span> <span class="code-highlight">test_normalization_effect</span>():
    <span class="code-comment"># 创建一个具有极端分布的输入</span>
    x = torch.randn(2, 10, 512) * 15 + 8  <span class="code-comment"># μ≈8, σ≈15</span>

    <span class="code-keyword">print</span>(<span class="code-string">f"归一化前: μ={x.mean():.2f}, σ={x.std():.2f}"</span>)

    <span class="code-comment"># 应用层归一化</span>
    layer_norm = LayerNormalization(512)
    normalized = layer_norm(x)

    <span class="code-keyword">print</span>(<span class="code-string">f"归一化后: μ={normalized.mean():.2f}, σ={normalized.std():.2f}"</span>)
    <span class="code-keyword">print</span>(<span class="code-string">f"每个位置的均值: {normalized.mean(dim=-1)[0, :5]}"</span>)  <span class="code-comment"># 应该接近0</span>
    <span class="code-keyword">print</span>(<span class="code-string">f"每个位置的标准差: {normalized.std(dim=-1)[0, :5]}"</span>)   <span class="code-comment"># 应该接近1</span></code></pre>
        </div>
    </div>

    <!-- 📊 层归一化实验验证 -->
    <div class="solution-section">
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">📊</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">实验验证：层归一化的神奇效果</h2>
            </div>
        </div>

        <!-- 训练稳定性对比 -->
        <div class="data-table">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; text-align: center;">🎯 残差连接 + 层归一化的组合效果</div>
            <table>
                <thead>
                <tr>
                    <th>架构配置</th>
                    <th>收敛步数</th>
                    <th>最终BLEU</th>
                    <th>训练时间</th>
                    <th>梯度稳定性</th>
                    <th>可扩展性</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td style="color: #ef4444;">原始Transformer</td>
                    <td class="before-data">不收敛</td>
                    <td class="before-data">15.2</td>
                    <td class="before-data">∞</td>
                    <td class="before-data">极差</td>
                    <td class="before-data">≤4层</td>
                </tr>
                <tr>
                    <td style="color: #fbbf24;">+残差连接</td>
                    <td class="after-data">8000</td>
                    <td class="after-data">23.7</td>
                    <td class="after-data">6小时</td>
                    <td class="after-data">中等</td>
                    <td class="after-data">8层</td>
                </tr>
                <tr>
                    <td style="color: #06b6d4;">+层归一化</td>
                    <td class="after-data">5000</td>
                    <td class="after-data">26.1</td>
                    <td class="after-data">4小时</td>
                    <td class="after-data">良好</td>
                    <td class="after-data">10层</td>
                </tr>
                <tr>
                    <td style="color: #22c55e;">残差+归一化</td>
                    <td class="after-data highlight">3000</td>
                    <td class="after-data highlight">31.4</td>
                    <td class="after-data highlight">2.5小时</td>
                    <td class="after-data highlight">优秀</td>
                    <td class="after-data highlight">24+层</td>
                </tr>
                </tbody>
            </table>
        </div>

        <!-- 详细性能分析 -->
        <div class="data-table">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; text-align: center;">🔬 各层激活值统计（最终稳定版本）</div>
            <table>
                <thead>
                <tr>
                    <th>网络层</th>
                    <th>均值 (μ)</th>
                    <th>标准差 (σ)</th>
                    <th>梯度范数</th>
                    <th>激活范围</th>
                    <th>健康状态</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>第2层</td>
                    <td class="after-data">0.01</td>
                    <td class="after-data">0.99</td>
                    <td class="after-data">1.1</td>
                    <td class="after-data">[-2.1, 2.3]</td>
                    <td style="color: #22c55e;">🟢 优秀</td>
                </tr>
                <tr>
                    <td>第6层</td>
                    <td class="after-data">-0.02</td>
                    <td class="after-data">1.01</td>
                    <td class="after-data">1.0</td>
                    <td class="after-data">[-2.2, 2.1]</td>
                    <td style="color: #22c55e;">🟢 优秀</td>
                </tr>
                <tr>
                    <td>第12层</td>
                    <td class="after-data">0.00</td>
                    <td class="after-data">1.00</td>
                    <td class="after-data">1.2</td>
                    <td class="after-data">[-2.0, 2.2]</td>
                    <td style="color: #22c55e;">🟢 优秀</td>
                </tr>
                <tr>
                    <td>第24层</td>
                    <td class="after-data">0.01</td>
                    <td class="after-data">0.98</td>
                    <td class="after-data">1.1</td>
                    <td class="after-data">[-2.1, 2.0]</td>
                    <td style="color: #22c55e;">🟢 优秀</td>
                </tr>
                </tbody>
            </table>
        </div>

        <!-- 学习率敏感性分析 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin: 2rem 0;">
            <div style="color: #06b6d4; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">📈 学习率敏感性分析</div>

            <div class="data-table">
                <table>
                    <thead>
                    <tr>
                        <th>学习率</th>
                        <th>原始Transformer</th>
                        <th>+残差连接</th>
                        <th>+LayerNorm</th>
                        <th>残差+LayerNorm</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td>0.0001</td>
                        <td class="before-data">收敛慢</td>
                        <td class="after-data">正常</td>
                        <td class="after-data">正常</td>
                        <td class="after-data">正常</td>
                    </tr>
                    <tr>
                        <td>0.001</td>
                        <td class="before-data">不稳定</td>
                        <td class="after-data">基本稳定</td>
                        <td class="after-data">稳定</td>
                        <td class="after-data highlight">最佳</td>
                    </tr>
                    <tr>
                        <td>0.01</td>
                        <td class="before-data">发散</td>
                        <td class="before-data">不稳定</td>
                        <td class="after-data">稳定</td>
                        <td class="after-data">稳定</td>
                    </tr>
                    <tr>
                        <td>0.1</td>
                        <td class="before-data">发散</td>
                        <td class="before-data">发散</td>
                        <td class="before-data">发散</td>
                        <td class="after-data">仍能训练</td>
                    </tr>
                    </tbody>
                </table>
            </div>

            <div style="background: rgba(34, 197, 94, 0.1); padding: 1.5rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3); margin-top: 1rem;">
                <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">🎯 关键发现：</div>
                <div style="color: #cbd5e1; font-size: 0.95rem;">
                    残差连接 + 层归一化的组合使得网络对学习率的敏感性大大降低，<br>
                    即使使用较大的学习率（0.01），训练依然稳定。这极大提升了超参数调优的容错性。
                </div>
            </div>
        </div>

        <div style="background: rgba(34, 197, 94, 0.1); padding: 2rem; border-radius: 12px; border: 1px solid rgba(34, 197, 94, 0.3); margin-top: 2rem;">
            <div style="color: #22c55e; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem; text-align: center;">🏆 历史性突破</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8; text-align: center;">
                <strong style="color: #fbbf24;">残差连接 + 层归一化</strong> = 深度网络训练的黄金标准！<br><br>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin-top: 1.5rem;">
                    <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                        <strong style="color: #8b5cf6;">训练速度</strong><br>
                        <span style="color: #22c55e;">提升2.7倍</span>
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                        <strong style="color: #8b5cf6;">模型性能</strong><br>
                        <span style="color: #22c55e;">BLEU +16.2</span>
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                        <strong style="color: #8b5cf6;">网络深度</strong><br>
                        <span style="color: #22c55e;">24+层稳定</span>
                    </div>
                    <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                        <strong style="color: #8b5cf6;">梯度健康</strong><br>
                        <span style="color: #22c55e;">完美分布</span>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- 🔄 Pre-Norm vs Post-Norm -->
    <div class="solution-section" style="border-color: #8b5cf6;">
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(139, 92, 246, 0.4);">
                <span style="font-size: 2rem;">🔄</span>
                <h2 style="color: #8b5cf6; margin: 0; font-size: 1.8rem; font-weight: bold;">重要发现：Pre-Norm vs Post-Norm</h2>
            </div>
        </div>

        <!-- 两种设计对比 -->
        <div style="background: rgba(15, 23, 42, 0.9); padding: 2rem; border-radius: 12px; margin-bottom: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1.5rem; text-align: center;">🎯 LayerNorm的位置选择</div>

            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                <!-- Post-Norm -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem; text-align: center;">Post-Norm (原始设计)</h4>

                    <div class="code-example" style="margin: 1rem 0; font-size: 0.8rem;">
                        <pre><code><span class="code-keyword">def</span> forward(self, x):
    <span class="code-comment"># 先计算，后归一化</span>
    attn_out = self.attention(x)
    x = <span style="color: #fbbf24;">self.norm1(x + attn_out)</span>

    ffn_out = self.feed_forward(x)
    x = <span style="color: #fbbf24;">self.norm2(x + ffn_out)</span>

    <span class="code-keyword">return</span> x</code></pre>
                    </div>

                    <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 8px;">
                        <div style="color: #fbbf24; font-weight: bold; margin-bottom: 0.5rem;">特点：</div>
                        <div style="color: #cbd5e1; font-size: 0.9rem;">
                            • 原始Transformer论文的设计<br>
                            • 残差连接后再归一化<br>
                            • 训练相对不稳定<br>
                            • 需要warmup策略
                        </div>
                    </div>
                </div>

                <!-- Pre-Norm -->
                <div style="background: rgba(255, 255, 255, 0.05); padding: 1.5rem; border-radius: 8px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem; text-align: center;">Pre-Norm (改进设计)</h4>

                    <div class="code-example" style="margin: 1rem 0; font-size: 0.8rem;">
                        <pre><code><span class="code-keyword">def</span> forward(self, x):
    <span class="code-comment"># 先归一化，后计算</span>
    norm_x = <span style="color: #22c55e;">self.norm1(x)</span>
    x = x + self.attention(norm_x)

    norm_x = <span style="color: #22c55e;">self.norm2(x)</span>
    x = x + self.feed_forward(norm_x)

    <span class="code-keyword">return</span> x</code></pre>
                    </div>

                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px;">
                        <div style="color: #22c55e; font-weight: bold; margin-bottom: 0.5rem;">特点：</div>
                        <div style="color: #cbd5e1; font-size: 0.9rem;">
                            • 现代LLM的标准设计<br>
                            • 归一化后再计算<br>
                            • 训练极其稳定<br>
                            • 不需要warmup
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- 实验对比 -->
        <div class="data-table">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem; text-align: center;">📊 Pre-Norm vs Post-Norm 实验对比</div>
            <table>
                <thead>
                <tr>
                    <th>指标</th>
                    <th>Post-Norm</th>
                    <th>Pre-Norm</th>
                    <th>改善</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>训练稳定性</td>
                    <td class="before-data">需要warmup</td>
                    <td class="after-data">不需要warmup</td>
                    <td style="color: #22c55e;">显著提升</td>
                </tr>
                <tr>
                    <td>梯度范数</td>
                    <td class="before-data">1.8±0.7</td>
                    <td class="after-data">1.1±0.2</td>
                    <td style="color: #22c55e;">更稳定</td>
                </tr>
                <tr>
                    <td>可训练深度</td>
                    <td class="before-data">12层</td>
                    <td class="after-data">100+层</td>
                    <td style="color: #22c55e;">大幅提升</td>
                </tr>
                <tr>
                    <td>收敛速度</td>
                    <td class="before-data">5000步</td>
                    <td class="after-data">3000步</td>
                    <td style="color: #22c55e;">1.7倍加速</td>
                </tr>
                </tbody>
            </table>
        </div>

        <div style="background: rgba(139, 92, 246, 0.1); padding: 2rem; border-radius: 12px; border: 1px solid rgba(139, 92, 246, 0.3); margin-top: 2rem;">
            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem; text-align: center;">🔍 为什么Pre-Norm更好？</div>
            <div style="color: #cbd5e1; font-size: 1rem; line-height: 1.8;">
                <strong style="color: #fbbf24;">数学解释</strong>：Pre-Norm确保每个子模块的输入都是标准化的，<br>
                避免了残差路径和主路径之间的scale不匹配问题。<br><br>

                <strong style="color: #22c55e;">实践影响</strong>：现代所有大型语言模型（GPT-3/4、LLaMA等）<br>
                都采用Pre-Norm设计，这不是偶然！<br><br>

                <span style="color: #8b5cf6; font-style: italic;">"细节决定成败，架构选择影响深远"</span>
            </div>
        </div>
    </div>

    <!-- 📝 本章总结 -->
    <div class="solution-section" style="border-color: #22c55e; background: linear-gradient(135deg, rgba(34, 197, 94, 0.1), rgba(16, 185, 129, 0.05));">
        <div style="text-align: center; margin-bottom: 3rem;">
            <div style="display: inline-flex; align-items: center; gap: 1rem; background: linear-gradient(135deg, rgba(34, 197, 94, 0.2), rgba(16, 185, 129, 0.1)); padding: 1.5rem 3rem; border-radius: 30px; border: 1px solid rgba(34, 197, 94, 0.4);">
                <span style="font-size: 2rem;">🎯</span>
                <h2 style="color: #22c55e; margin: 0; font-size: 1.8rem; font-weight: bold;">深度网络训练：从不可能到轻而易举</h2>
            </div>
        </div>

        <div style="background: rgba(255, 255, 255, 0.08); padding: 2.5rem; border-radius: 16px;">
            <div style="display: grid; gap: 2rem;">
                <!-- 核心发现 -->
                <div style="background: rgba(15, 23, 42, 0.8); padding: 2rem; border-radius: 12px;">
                    <h4 style="color: #22c55e; margin-bottom: 1rem; text-align: center;">🔬 科学发现</h4>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem;">
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.5rem;">残差连接原理</div>
                            <div style="color: #cbd5e1; font-size: 0.9rem;">梯度传播的"高速公路"，保证深层网络的可训练性</div>
                        </div>
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 0.5rem;">层归一化作用</div>
                            <div style="color: #cbd5e1; font-size: 0.9rem;">稳定数据分布，消除内部协变量偏移</div>
                        </div>
                    </div>
                </div>

                <!-- 技术突破 -->
                <div style="background: rgba(15, 23, 42, 0.8); padding: 2rem; border-radius: 12px;">
                    <h4 style="color: #06b6d4; margin-bottom: 1rem; text-align: center;">🚀 技术突破</h4>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem;">
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                            <strong style="color: #22c55e;">训练深度</strong><br>
                            <span style="color: #cbd5e1; font-size: 0.9rem;">从4层扩展到24+层</span>
                        </div>
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                            <strong style="color: #22c55e;">梯度稳定</strong><br>
                            <span style="color: #cbd5e1; font-size: 0.9rem;">范数保持在0.8-1.3</span>
                        </div>
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                            <strong style="color: #22c55e;">性能提升</strong><br>
                            <span style="color: #cbd5e1; font-size: 0.9rem;">BLEU从15.2到31.4</span>
                        </div>
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 8px; border: 1px solid rgba(34, 197, 94, 0.3);">
                            <strong style="color: #22c55e;">训练效率</strong><br>
                            <span style="color: #cbd5e1; font-size: 0.9rem;">速度提升2.7倍</span>
                        </div>
                    </div>
                </div>

                <!-- 关键洞察 -->
                <div style="background: rgba(15, 23, 42, 0.8); padding: 2rem; border-radius: 12px;">
                    <h4 style="color: #fbbf24; margin-bottom: 1rem; text-align: center;">💡 关键洞察</h4>
                    <div style="display: grid; gap: 1rem;">
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <strong style="color: #8b5cf6;">跨领域启发</strong>：从ResNet学习到Transformer应用
                        </div>
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <strong style="color: #8b5cf6;">维度转换</strong>：从批次归一化到特征归一化的巧妙思考
                        </div>
                        <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 8px;">
                            <strong style="color: #8b5cf6;">架构细节</strong>：Pre-Norm vs Post-Norm的重要差异
                        </div>
                    </div>
                </div>

                <!-- 历史意义 -->
                <div style="background: linear-gradient(135deg, rgba(251, 191, 36, 0.15), rgba(245, 158, 11, 0.1)); padding: 2rem; border-radius: 16px; border: 1px solid rgba(251, 191, 36, 0.3); text-align: center;">
                    <div style="color: #fbbf24; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem;">
                        🏛️ 历史意义
                    </div>
                    <div style="color: #f1f5f9; font-size: 1rem; line-height: 1.6;">
                        这两个看似简单的技术创新，开启了深度学习的新纪元：<br><br>
                        • <strong style="color: #22c55e;">ResNet革命</strong>：让计算机视觉模型深度突破100层<br>
                        • <strong style="color: #8b5cf6;">Transformer成功</strong>：为自然语言处理奠定基础<br>
                        • <strong style="color: #06b6d4;">现代LLM</strong>：GPT、BERT等都建立在这个基础上<br><br>
                        <span style="color: #fbbf24; font-weight: bold;">简单的数学，深远的影响！</span>
                    </div>
                </div>

                <!-- 下一步展望 -->
                <div style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.2), rgba(124, 58, 237, 0.1)); padding: 2rem; border-radius: 16px; border: 1px solid rgba(139, 92, 246, 0.3); text-align: center;">
                    <div style="color: #8b5cf6; font-weight: bold; margin-bottom: 1rem; font-size: 1.2rem;">
                        🎯 下一章预告
                    </div>
                    <div style="color: #f1f5f9; font-size: 1rem; line-height: 1.6;">
                        训练稳定性问题彻底解决！但我们还有两个重要挑战：<br><br>
                        <strong style="color: #fbbf24;">位置信息缺失</strong> - 模型仍然不理解词序<br>
                        <strong style="color: #22c55e;">表达能力不足</strong> - 需要更强的非线性处理<br><br>
                        下一章，我们将揭开位置编码的奥秘，让Transformer真正理解序列！
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- 导航 -->
    <div style="display: flex; justify-content: space-between; align-items: center; margin-top: 3rem; padding: 2rem; background: rgba(255, 255, 255, 0.05); border-radius: 12px; backdrop-filter: blur(10px);">
        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #8b5cf6, #7c3aed); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="window.history.back()">
            ← 第6章：第一版的问题分析
        </button>

        <div style="text-align: center; color: #cbd5e1;">
            <strong>第7章：让训练变稳定</strong><br>
            <span>数学原理、实验验证与历史突破</span>
        </div>

        <button style="padding: 0.8rem 1.5rem; background: linear-gradient(45deg, #22c55e, #16a34a); color: white; border: none; border-radius: 8px; cursor: pointer; text-decoration: none; transition: all 0.3s ease; font-weight: bold; display: flex; align-items: center; gap: 0.5rem;" onclick="window.location.href='#chapter-8'">
            第8章：位置编码的智慧 →
        </button>
    </div>
</div>

<script>
    // 添加交互效果
    document.addEventListener('DOMContentLoaded', function() {
        // 数据表格高亮动画
        const highlightCells = document.querySelectorAll('.highlight');
        highlightCells.forEach((cell, index) => {
            setTimeout(() => {
                cell.style.animation = 'matrix-highlight 2s infinite';
            }, index * 500);
        });

        // 分布归一化动画
        const normalizeBars = document.querySelectorAll('.normalize');
        normalizeBars.forEach((bar, index) => {
            setTimeout(() => {
                bar.style.animation = 'data-normalize 3s infinite';
            }, index * 200);
        });

        // 矩阵动画
        const animatedMatrices = document.querySelectorAll('.matrix-animated');
        animatedMatrices.forEach((matrix, index) => {
            setTimeout(() => {
                matrix.style.animation = 'matrix-pulse 4s infinite';
            }, index * 1000);
        });

        // 数学公式悬停效果
        const mathFormulas = document.querySelectorAll('.math-formula');
        mathFormulas.forEach(formula => {
            formula.addEventListener('mouseenter', function() {
                this.style.transform = 'scale(1.02)';
                this.style.boxShadow = '0 8px 30px rgba(139, 92, 246, 0.3)';
            });

            formula.addEventListener('mouseleave', function() {
                this.style.transform = 'scale(1)';
                this.style.boxShadow = 'none';
            });
        });

        // 代码块语法高亮增强
        const codeBlocks = document.querySelectorAll('.code-example');
        codeBlocks.forEach(code => {
            code.addEventListener('click', function() {
                this.style.background = 'rgba(34, 197, 94, 0.1)';
                setTimeout(() => {
                    this.style.background = 'rgba(15, 23, 42, 0.95)';
                }, 300);
            });
        });

        // 时间线悬停效果
        const timelineItems = document.querySelectorAll('.timeline-item');
        timelineItems.forEach(item => {
            item.addEventListener('mouseenter', function() {
                const dot = this.querySelector('.timeline-dot');
                dot.style.transform = 'scale(1.1)';
                dot.style.boxShadow = '0 5px 20px rgba(139, 92, 246, 0.4)';
            });

            item.addEventListener('mouseleave', function() {
                const dot = this.querySelector('.timeline-dot');
                dot.style.transform = 'scale(1)';
                dot.style.boxShadow = 'none';
            });
        });

        // 分布图柱状条交互
        const distributionBars = document.querySelectorAll('.distribution-bar');
        distributionBars.forEach(bar => {
            bar.addEventListener('click', function() {
                this.style.background = 'linear-gradient(to top, rgba(251, 191, 36, 0.6), rgba(251, 191, 36, 0.9))';
                setTimeout(() => {
                    this.style.background = this.dataset.originalBg || this.style.background;
                }, 1000);
            });
        });

        // 计算步骤高亮效果
        const calculationSteps = document.querySelectorAll('.calculation-step');
        calculationSteps.forEach((step, index) => {
            step.addEventListener('mouseenter', function() {
                this.style.background = 'rgba(34, 197, 94, 0.05)';
                this.style.transform = 'translateY(-2px)';
                this.style.boxShadow = '0 8px 25px rgba(34, 197, 94, 0.2)';
            });

            step.addEventListener('mouseleave', function() {
                this.style.background = 'rgba(15, 23, 42, 0.9)';
                this.style.transform = 'translateY(0)';
                this.style.boxShadow = 'none';
            });
        });

        // 响应式表格处理
        const dataTables = document.querySelectorAll('.data-table');
        dataTables.forEach(table => {
            const wrapper = document.createElement('div');
            wrapper.style.overflowX = 'auto';
            wrapper.style.overflowY = 'hidden';
            table.parentNode.insertBefore(wrapper, table);
            wrapper.appendChild(table);
        });

        // 添加渐入动画
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        // 观察所有主要区块
        document.querySelectorAll('.solution-section, .solution-discovery, .matrix-section').forEach(el => {
            el.style.opacity = '0';
            el.style.transform = 'translateY(20px)';
            el.style.transition = 'all 0.6s ease-out';
            observer.observe(el);
        });
    });
</script>
