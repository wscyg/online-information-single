<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第3章：多层感知机 - 表达力与激活函数 | 深度学习之旅</title>
    <meta name="description" content="探索神经网络的表达能力：从通用逼近定理到激活函数的选择，理解深度与宽度的权衡">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    <style>
        /* ===== CSS变量定义 ===== */
        :root {
            --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --bg-dark: #0f172a;
            --bg-section: #1e293b;
            --text-primary: #f1f5f9;
            --text-secondary: #e2e8f0;
            --text-muted: #cbd5e1;
            --accent-red: #ef4444;
            --accent-green: #22c55e;
            --accent-blue: #06b6d4;
            --accent-yellow: #fbbf24;
            --accent-purple: #8b5cf6;
            --accent-pink: #ec4899;
            --border-color: rgba(255, 255, 255, 0.1);
            --shadow-lg: 0 10px 40px rgba(0, 0, 0, 0.3);
            --animation-duration: 0.3s;
            --content-max-width: 1200px;
            --focus-ring: 0 0 0 3px rgba(139, 92, 246, 0.5);
        }

        /* 亮色主题 */
        [data-theme="light"] {
            --bg-dark: #ffffff;
            --bg-section: #f8fafc;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #64748b;
            --border-color: rgba(0, 0, 0, 0.1);
            --shadow-lg: 0 10px 40px rgba(0, 0, 0, 0.1);
        }

        /* ===== 全局样式 ===== */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            line-height: 1.6;
            overflow-x: hidden;
            transition: background-color var(--animation-duration) ease;
        }

        /* ===== 焦点样式 ===== */
        :focus-visible {
            outline: none;
            box-shadow: var(--focus-ring);
            border-radius: 0.25rem;
        }

        button:focus-visible,
        a:focus-visible {
            outline: none;
            box-shadow: var(--focus-ring);
        }

        /* ===== 布局组件 ===== */
        .container {
            max-width: var(--content-max-width);
            margin: 0 auto;
            padding: 0 1rem;
        }

        /* ===== 导航栏 ===== */
        .nav-header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
            transition: transform 0.3s ease;
        }

        .nav-header.hidden {
            transform: translateY(-100%);
        }

        .nav-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 1rem 0;
        }

        .nav-title {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .nav-title h1 {
            font-size: 1.25rem;
            font-weight: 600;
            background: var(--primary-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .nav-controls {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        .btn-control {
            background: transparent;
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            padding: 0.5rem;
            border-radius: 0.5rem;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
            display: flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
            position: relative;
        }

        .btn-control:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: translateY(-2px);
        }

        .nav-progress {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: rgba(255, 255, 255, 0.1);
        }

        .progress-bar {
            height: 100%;
            background: var(--primary-gradient);
            width: 0%;
            transition: width 0.3s ease;
        }

        /* ===== 难度选择器 ===== */
        .difficulty-selector {
            position: fixed;
            right: 20px;
            top: 80px;
            background: var(--bg-section);
            border: 1px solid var(--border-color);
            border-radius: 0.5rem;
            padding: 1rem;
            z-index: 999;
            box-shadow: var(--shadow-lg);
        }

        .difficulty-selector h4 {
            color: var(--accent-purple);
            margin-bottom: 0.5rem;
            font-size: 0.875rem;
        }

        .difficulty-option {
            display: flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem;
            cursor: pointer;
            border-radius: 0.25rem;
            transition: all 0.2s ease;
        }

        .difficulty-option:hover {
            background: rgba(255, 255, 255, 0.05);
        }

        .difficulty-option.active {
            background: rgba(139, 92, 246, 0.2);
            color: var(--accent-purple);
        }

        .difficulty-option input[type="radio"] {
            display: none;
        }

        /* ===== 侧边导航 ===== */
        .sidebar {
            position: fixed;
            left: -300px;
            top: 60px;
            bottom: 0;
            width: 300px;
            background: var(--bg-section);
            border-right: 1px solid var(--border-color);
            padding: 2rem;
            overflow-y: auto;
            transition: transform 0.3s ease;
            z-index: 999;
        }

        .sidebar.open {
            transform: translateX(300px);
        }

        .sidebar-overlay {
            position: fixed;
            inset: 0;
            background: rgba(0, 0, 0, 0.5);
            display: none;
            z-index: 998;
        }

        .sidebar-overlay.active {
            display: block;
        }

        .toc-item {
            display: block;
            padding: 0.75rem 1rem;
            color: var(--text-secondary);
            text-decoration: none;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
            margin-bottom: 0.25rem;
        }

        .toc-item:hover {
            background: rgba(255, 255, 255, 0.05);
            color: var(--text-primary);
            transform: translateX(4px);
        }

        .toc-item.active {
            background: var(--primary-gradient);
            color: white;
        }

        /* ===== 学习伙伴系统 ===== */
        .learning-buddy {
            position: fixed;
            bottom: 20px;
            right: 20px;
            width: 300px;
            background: var(--bg-section);
            border: 2px solid var(--accent-purple);
            border-radius: 1rem;
            padding: 1.5rem;
            box-shadow: var(--shadow-lg);
            z-index: 900;
            transform: translateY(400px);
            transition: transform 0.5s ease;
        }

        .learning-buddy.active {
            transform: translateY(0);
        }

        .learning-buddy.minimized {
            width: auto;
            padding: 0;
            border-radius: 50%;
            overflow: hidden;
        }

        .learning-buddy.minimized .buddy-content {
            display: none;
        }

        .learning-buddy.minimized .buddy-avatar {
            margin: 0;
            cursor: pointer;
        }

        .buddy-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 1rem;
        }

        .buddy-avatar {
            width: 60px;
            height: 60px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2rem;
            flex-shrink: 0;
        }

        .buddy-controls {
            display: flex;
            gap: 0.5rem;
        }

        .buddy-control-btn {
            width: 30px;
            height: 30px;
            border-radius: 50%;
            border: 1px solid var(--border-color);
            background: transparent;
            color: var(--text-primary);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s ease;
        }

        .buddy-control-btn:hover {
            background: rgba(255, 255, 255, 0.1);
        }

        .buddy-message {
            background: rgba(139, 92, 246, 0.1);
            padding: 1rem;
            border-radius: 0.5rem;
            margin-bottom: 1rem;
            position: relative;
        }

        .buddy-message::before {
            content: '';
            position: absolute;
            top: -8px;
            left: 20px;
            width: 0;
            height: 0;
            border-left: 8px solid transparent;
            border-right: 8px solid transparent;
            border-bottom: 8px solid rgba(139, 92, 246, 0.1);
        }

        /* ===== 主内容区 ===== */
        main {
            margin-top: 80px;
            padding-bottom: 4rem;
        }

        /* ===== 章节卡片 ===== */
        .chapter-hero {
            background: var(--primary-gradient);
            padding: 4rem 0;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
        }

        .chapter-hero::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.1) 0%, transparent 70%);
            animation: pulse 10s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1) rotate(0deg); }
            50% { transform: scale(1.1) rotate(180deg); }
        }

        .chapter-hero-content {
            position: relative;
            z-index: 2;
            text-align: center;
            color: white;
        }

        .chapter-hero h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            animation: fadeInUp 0.8s ease;
        }

        .chapter-hero p {
            font-size: 1.25rem;
            opacity: 0.9;
            max-width: 600px;
            margin: 0 auto;
            animation: fadeInUp 0.8s ease 0.2s both;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* ===== 内容区块 ===== */
        .section-card {
            background: var(--bg-section);
            border-radius: 1rem;
            padding: 2.5rem;
            margin-bottom: 2rem;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-lg);
            transition: transform var(--animation-duration) ease;
            scroll-margin-top: 100px;
        }

        .section-card:hover {
            transform: translateY(-2px);
        }

        /* ===== 学习循环指示器 ===== */
        .learning-loop {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 2rem;
            margin-bottom: 3rem;
            padding: 1.5rem;
            background: rgba(139, 92, 246, 0.1);
            border-radius: 1rem;
            border: 1px solid rgba(139, 92, 246, 0.3);
        }

        .loop-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.5rem;
            padding: 1rem;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
            position: relative;
        }

        .loop-step.active {
            background: rgba(139, 92, 246, 0.2);
            transform: scale(1.1);
        }

        .loop-step .icon {
            font-size: 2rem;
        }

        .loop-step .label {
            font-size: 0.875rem;
            color: var(--text-secondary);
        }

        .loop-connector {
            position: absolute;
            top: 50%;
            right: -2rem;
            width: 2rem;
            height: 2px;
            background: rgba(139, 92, 246, 0.3);
        }

        /* ===== 故事卡片 ===== */
        .story-card {
            background: linear-gradient(135deg, rgba(251, 191, 36, 0.15), rgba(245, 158, 11, 0.1));
            border-radius: 1rem;
            padding: 2rem;
            margin-bottom: 2rem;
            border: 2px solid rgba(251, 191, 36, 0.2);
            position: relative;
            overflow: hidden;
        }

        .story-card::before {
            content: '🧠';
            position: absolute;
            top: -20px;
            right: -20px;
            font-size: 5rem;
            opacity: 0.1;
            transform: rotate(15deg);
        }

        /* ===== 人物对话 ===== */
        .dialogue-container {
            margin: 2rem 0;
            padding: 1.5rem;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 0.75rem;
        }

        .dialogue-item {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
            animation: slideIn 0.5s ease;
        }

        .dialogue-avatar {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            flex-shrink: 0;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
        }

        .dialogue-avatar.student {
            background: linear-gradient(135deg, #fbbf24, #f59e0b);
        }

        .dialogue-avatar.teacher {
            background: linear-gradient(135deg, #06b6d4, #0891b2);
        }

        .dialogue-avatar.friend {
            background: linear-gradient(135deg, #22c55e, #16a34a);
        }

        .dialogue-content {
            flex: 1;
            background: rgba(255, 255, 255, 0.05);
            padding: 1rem;
            border-radius: 0.5rem;
            position: relative;
        }

        .dialogue-content::before {
            content: '';
            position: absolute;
            left: -8px;
            top: 15px;
            width: 0;
            height: 0;
            border-top: 8px solid transparent;
            border-bottom: 8px solid transparent;
            border-right: 8px solid rgba(255, 255, 255, 0.05);
        }

        .dialogue-name {
            font-weight: bold;
            color: var(--accent-purple);
            margin-bottom: 0.25rem;
        }

        /* ===== 概念卡片网格 ===== */
        .concept-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .concept-card {
            padding: 1.5rem;
            border-radius: 0.75rem;
            transition: all var(--animation-duration) ease;
            cursor: pointer;
            position: relative;
            overflow: hidden;
        }

        .concept-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: var(--primary-gradient);
            transform: scaleX(0);
            transition: transform var(--animation-duration) ease;
        }

        .concept-card:hover::before {
            transform: scaleX(1);
        }

        .concept-card.why {
            background: rgba(239, 68, 68, 0.1);
            border: 1px solid rgba(239, 68, 68, 0.3);
        }

        .concept-card.what {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
        }

        .concept-card.how {
            background: rgba(6, 182, 212, 0.1);
            border: 1px solid rgba(6, 182, 212, 0.3);
        }

        .concept-card.pitfall {
            background: rgba(251, 191, 36, 0.1);
            border: 1px solid rgba(251, 191, 36, 0.3);
        }

        /* ===== 数学推导容器 ===== */
        .math-derivation {
            background: rgba(79, 70, 229, 0.05);
            border: 1px solid rgba(79, 70, 229, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .math-step {
            margin: 1.5rem 0;
            padding-left: 2rem;
            position: relative;
        }

        .math-step::before {
            content: attr(data-step);
            position: absolute;
            left: 0;
            top: 0;
            width: 24px;
            height: 24px;
            background: var(--accent-purple);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.875rem;
            font-weight: bold;
        }

        .math-explanation {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 0.5rem;
            padding: 1rem;
            margin-top: 0.5rem;
            font-size: 0.95rem;
            color: var(--text-secondary);
        }

        /* ===== 交互式可视化 ===== */
        .interactive-demo {
            background: rgba(15, 23, 42, 0.8);
            border: 1px solid var(--border-color);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .demo-controls {
            display: flex;
            gap: 2rem;
            align-items: center;
            margin-bottom: 1.5rem;
            flex-wrap: wrap;
        }

        .control-group {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .control-group label {
            font-size: 0.875rem;
            color: var(--text-secondary);
        }

        .slider-container {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        input[type="range"] {
            width: 150px;
            height: 6px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 3px;
            outline: none;
            -webkit-appearance: none;
            cursor: pointer;
        }

        input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 18px;
            height: 18px;
            background: var(--primary-gradient);
            border-radius: 50%;
            cursor: pointer;
            transition: transform 0.2s ease;
        }

        input[type="range"]::-webkit-slider-thumb:hover {
            transform: scale(1.2);
        }

        .slider-value {
            background: rgba(255, 255, 255, 0.1);
            padding: 0.25rem 0.75rem;
            border-radius: 0.5rem;
            font-family: monospace;
            min-width: 50px;
            text-align: center;
        }

        canvas {
            display: block;
            margin: 0 auto;
            border-radius: 0.5rem;
            background: rgba(0, 0, 0, 0.2);
            cursor: crosshair;
        }

        /* ===== 即时练习 ===== */
        .instant-practice {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .practice-question {
            font-size: 1.1rem;
            margin-bottom: 1.5rem;
            font-weight: 500;
        }

        .practice-options {
            display: grid;
            gap: 1rem;
        }

        .practice-option {
            background: rgba(255, 255, 255, 0.05);
            border: 2px solid transparent;
            border-radius: 0.5rem;
            padding: 1rem 1.5rem;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
            position: relative;
            overflow: hidden;
            text-align: left;
            width: 100%;
        }

        .practice-option:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: translateX(4px);
        }

        .practice-option.selected {
            border-color: var(--accent-blue);
        }

        .practice-option.correct {
            background: rgba(34, 197, 94, 0.2);
            border-color: var(--accent-green);
        }

        .practice-option.incorrect {
            background: rgba(239, 68, 68, 0.2);
            border-color: var(--accent-red);
        }

        .practice-feedback {
            margin-top: 1.5rem;
            padding: 1rem;
            border-radius: 0.5rem;
            display: none;
            animation: fadeIn 0.3s ease;
        }

        .practice-feedback.show {
            display: block;
        }

        .practice-feedback.correct {
            background: rgba(34, 197, 94, 0.2);
            border: 1px solid rgba(34, 197, 94, 0.3);
            color: var(--accent-green);
        }

        .practice-feedback.incorrect {
            background: rgba(239, 68, 68, 0.2);
            border: 1px solid rgba(239, 68, 68, 0.3);
            color: var(--accent-red);
        }

        /* ===== 常见错误专栏 ===== */
        .common-mistakes {
            background: rgba(239, 68, 68, 0.1);
            border: 2px solid rgba(239, 68, 68, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .mistake-item {
            display: flex;
            gap: 1rem;
            margin-bottom: 1.5rem;
            padding: 1rem;
            background: rgba(0, 0, 0, 0.2);
            border-radius: 0.5rem;
        }

        .mistake-icon {
            font-size: 2rem;
            flex-shrink: 0;
        }

        .mistake-content h4 {
            color: var(--accent-red);
            margin-bottom: 0.5rem;
        }

        /* ===== 代码容器 ===== */
        .code-container {
            margin: 2rem 0;
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: var(--shadow-lg);
        }

        .code-header {
            background: #1a1a2e;
            padding: 1rem 1.5rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .code-header-left {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .code-header-right {
            display: flex;
            gap: 0.5rem;
        }

        .code-tab {
            padding: 0.5rem 1rem;
            background: transparent;
            border: none;
            color: var(--text-secondary);
            cursor: pointer;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
        }

        .code-tab.active {
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-primary);
        }

        .code-action {
            padding: 0.5rem 1rem;
            background: transparent;
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            cursor: pointer;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
            font-size: 0.875rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .code-action:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: translateY(-1px);
        }

        .code-content {
            background: #0d1117;
            overflow-x: auto;
        }

        .code-content pre {
            margin: 0;
            padding: 1.5rem;
        }

        .code-content code {
            font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        .code-output {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
            border-radius: 0.5rem;
            padding: 1rem;
            margin-top: 1rem;
            font-family: monospace;
            font-size: 0.875rem;
            display: none;
        }

        .code-output.show {
            display: block;
            animation: slideIn 0.3s ease;
        }

        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(-10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* ===== 学习路径推荐 ===== */
        .learning-path {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(236, 72, 153, 0.1));
            border: 2px solid rgba(139, 92, 246, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
        }

        .path-item {
            display: flex;
            align-items: center;
            gap: 1rem;
            padding: 1rem;
            margin: 0.5rem 0;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
            cursor: pointer;
        }

        .path-item:hover {
            background: rgba(255, 255, 255, 0.08);
            transform: translateX(4px);
        }

        .path-item.completed {
            opacity: 0.6;
        }

        .path-item.current {
            border: 2px solid var(--accent-purple);
            background: rgba(139, 92, 246, 0.1);
        }

        .path-icon {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            background: var(--primary-gradient);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.2rem;
            color: white;
        }

        .path-content h4 {
            color: var(--accent-purple);
            margin-bottom: 0.25rem;
        }

        .path-content p {
            font-size: 0.875rem;
            color: var(--text-secondary);
        }

        /* ===== 提示框 ===== */
        .tip {
            display: flex;
            gap: 1rem;
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
        }

        .tip-icon {
            font-size: 1.5rem;
            flex-shrink: 0;
        }

        .tip-content {
            flex: 1;
        }

        .tip.info {
            background: rgba(6, 182, 212, 0.1);
            border: 1px solid rgba(6, 182, 212, 0.3);
        }

        .tip.warning {
            background: rgba(251, 191, 36, 0.1);
            border: 1px solid rgba(251, 191, 36, 0.3);
        }

        .tip.error {
            background: rgba(239, 68, 68, 0.1);
            border: 1px solid rgba(239, 68, 68, 0.3);
        }

        .tip.success {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
        }

        /* ===== 深度指示器 ===== */
        .depth-indicator {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.25rem 0.75rem;
            background: rgba(139, 92, 246, 0.1);
            border: 1px solid rgba(139, 92, 246, 0.3);
            border-radius: 20px;
            font-size: 0.875rem;
            margin-left: 1rem;
        }

        .depth-indicator.beginner {
            background: rgba(34, 197, 94, 0.1);
            border-color: rgba(34, 197, 94, 0.3);
            color: var(--accent-green);
        }

        .depth-indicator.intermediate {
            background: rgba(251, 191, 36, 0.1);
            border-color: rgba(251, 191, 36, 0.3);
            color: var(--accent-yellow);
        }

        .depth-indicator.advanced {
            background: rgba(239, 68, 68, 0.1);
            border-color: rgba(239, 68, 68, 0.3);
            color: var(--accent-red);
        }

        /* ===== 可折叠内容 ===== */
        .collapsible {
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            border-radius: 0.75rem;
            overflow: hidden;
        }

        .collapsible-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 1.25rem 1.5rem;
            background: rgba(255, 255, 255, 0.05);
            cursor: pointer;
            transition: background var(--animation-duration) ease;
            user-select: none;
        }

        .collapsible-header:hover {
            background: rgba(255, 255, 255, 0.08);
        }

        .collapsible-title {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            font-weight: 500;
        }

        .collapsible-icon {
            transition: transform var(--animation-duration) ease;
        }

        .collapsible.expanded .collapsible-icon {
            transform: rotate(180deg);
        }

        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height var(--animation-duration) ease;
        }

        .collapsible.expanded .collapsible-content {
            max-height: 2000px;
        }

        .collapsible-inner {
            padding: 1.5rem;
        }

        /* ===== 按钮样式 ===== */
        .btn {
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            border: none;
            font-weight: 500;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            text-decoration: none;
        }

        .btn-primary {
            background: var(--primary-gradient);
            color: white;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(102, 126, 234, 0.3);
        }

        .btn-secondary {
            background: transparent;
            color: var(--text-primary);
            border: 1px solid var(--border-color);
        }

        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.1);
        }

        /* ===== 学习成就系统 ===== */
        .achievement-popup {
            position: fixed;
            top: 100px;
            right: -400px;
            width: 350px;
            background: var(--bg-section);
            border: 2px solid var(--accent-yellow);
            border-radius: 1rem;
            padding: 1.5rem;
            box-shadow: var(--shadow-lg);
            z-index: 1000;
            transition: right 0.5s ease;
        }

        .achievement-popup.show {
            right: 20px;
        }

        .achievement-header {
            display: flex;
            align-items: center;
            gap: 1rem;
            margin-bottom: 1rem;
        }

        .achievement-icon {
            width: 60px;
            height: 60px;
            background: var(--accent-yellow);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2rem;
        }

        .achievement-content h3 {
            color: var(--accent-yellow);
            margin-bottom: 0.25rem;
        }

        /* ===== 通知样式 ===== */
        .notification {
            position: fixed;
            top: 80px;
            right: 20px;
            background: var(--accent-green);
            color: white;
            padding: 1rem 1.5rem;
            border-radius: 0.5rem;
            z-index: 1000;
            animation: slideInRight 0.3s ease;
            box-shadow: var(--shadow-lg);
        }

        @keyframes slideInRight {
            from {
                transform: translateX(100%);
                opacity: 0;
            }
            to {
                transform: translateX(0);
                opacity: 1;
            }
        }

        /* ===== 响应式设计 ===== */
        @media (max-width: 1024px) {
            .chapter-hero h1 {
                font-size: 2.5rem;
            }

            .concept-grid {
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            }

            .difficulty-selector {
                top: auto;
                bottom: 80px;
            }
        }

        @media (max-width: 768px) {
            .nav-title h1 {
                font-size: 1rem;
            }

            .section-card {
                padding: 1.5rem;
            }

            .chapter-hero {
                padding: 3rem 0;
            }

            .chapter-hero h1 {
                font-size: 2rem;
            }

            .chapter-hero p {
                font-size: 1rem;
            }

            .learning-loop {
                flex-direction: column;
                gap: 1rem;
            }

            .loop-connector {
                display: none;
            }

            .demo-controls {
                flex-direction: column;
                align-items: stretch;
            }

            input[type="range"] {
                width: 100%;
            }

            .learning-buddy {
                width: 90%;
                left: 5%;
                right: 5%;
            }
        }

        /* ===== 打印样式 ===== */
        @media print {
            .nav-header,
            .sidebar,
            .btn,
            .demo-controls,
            .code-action,
            .difficulty-selector,
            .learning-buddy {
                display: none;
            }

            .section-card {
                page-break-inside: avoid;
                box-shadow: none;
                border: 1px solid #ddd;
            }

            body {
                background: white;
                color: black;
            }
        }

        /* ===== 动画优化 ===== */
        @media (prefers-reduced-motion: reduce) {
            *,
            *::before,
            *::after {
                animation-duration: 0.01ms !important;
                animation-iteration-count: 1 !important;
                transition-duration: 0.01ms !important;
            }
        }

        /* ===== 工具类 ===== */
        .visually-hidden {
            position: absolute;
            width: 1px;
            height: 1px;
            padding: 0;
            margin: -1px;
            overflow: hidden;
            clip: rect(0, 0, 0, 0);
            white-space: nowrap;
            border: 0;
        }

        .text-center {
            text-align: center;
        }

        .mt-1 { margin-top: 0.5rem; }
        .mt-2 { margin-top: 1rem; }
        .mt-3 { margin-top: 1.5rem; }
        .mt-4 { margin-top: 2rem; }

        .mb-1 { margin-bottom: 0.5rem; }
        .mb-2 { margin-bottom: 1rem; }
        .mb-3 { margin-bottom: 1.5rem; }
        .mb-4 { margin-bottom: 2rem; }

        /* ===== 加载动画 ===== */
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: var(--accent-purple);
            animation: spin 1s ease-in-out infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        /* ===== 梯度流动画 ===== */
        .gradient-flow {
            position: relative;
            overflow: hidden;
        }

        .gradient-flow::after {
            content: '';
            position: absolute;
            top: -100%;
            left: -100%;
            right: -100%;
            bottom: -100%;
            background: linear-gradient(
                    45deg,
                    transparent 30%,
                    rgba(139, 92, 246, 0.2) 50%,
                    transparent 70%
            );
            animation: gradientFlow 3s linear infinite;
        }

        @keyframes gradientFlow {
            0% {
                transform: translateX(-100%) translateY(-100%);
            }
            100% {
                transform: translateX(100%) translateY(100%);
            }
        }

        /* ===== 神经元样式 ===== */
        .neuron {
            fill: var(--accent-purple);
            stroke: var(--text-primary);
            stroke-width: 2;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .neuron:hover {
            fill: var(--accent-pink);
            transform: scale(1.1);
        }

        .neuron.active {
            fill: var(--accent-green);
            animation: pulse 1s ease-in-out infinite;
        }

        .synapse {
            stroke: var(--text-secondary);
            stroke-width: 1.5;
            fill: none;
            opacity: 0.6;
            transition: all 0.3s ease;
        }

        .synapse.active {
            stroke: var(--accent-yellow);
            stroke-width: 3;
            opacity: 1;
            animation: synapseFlow 1s linear infinite;
        }

        @keyframes synapseFlow {
            0% {
                stroke-dasharray: 0 100;
            }
            100% {
                stroke-dasharray: 100 0;
            }
        }

        /* ===== 手写效果 ===== */
        .handwritten {
            font-family: 'Kalam', cursive;
            color: var(--accent-yellow);
            transform: rotate(-2deg);
            display: inline-block;
        }

        /* ===== 数学公式容器 ===== */
        .equation-container {
            background: rgba(139, 92, 246, 0.05);
            border: 1px solid rgba(139, 92, 246, 0.2);
            border-radius: 0.5rem;
            padding: 1.5rem;
            margin: 1rem 0;
            text-align: center;
            font-size: 1.1rem;
            overflow-x: auto;
        }

        .equation-label {
            display: inline-block;
            background: var(--accent-purple);
            color: white;
            padding: 0.25rem 0.75rem;
            border-radius: 20px;
            font-size: 0.875rem;
            margin-bottom: 0.5rem;
        }

        /* ===== 选择器样式 ===== */
        .form-select {
            background: rgba(255, 255, 255, 0.05);
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
        }

        .form-select:hover {
            background: rgba(255, 255, 255, 0.1);
        }

        .form-select:focus {
            outline: none;
            box-shadow: var(--focus-ring);
        }

        /* ===== 激活函数图表样式 ===== */
        .activation-chart {
            position: relative;
            margin: 2rem 0;
        }

        .chart-legend {
            display: flex;
            gap: 2rem;
            justify-content: center;
            margin-top: 1rem;
            flex-wrap: wrap;
        }

        .legend-item {
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .legend-color {
            width: 20px;
            height: 4px;
            border-radius: 2px;
        }
    </style>
</head>
<body>
<!-- 导航栏 -->
<nav class="nav-header" role="navigation" aria-label="主导航">
    <div class="container">
        <div class="nav-content">
            <div class="nav-title">
                <button id="toggle-sidebar" class="btn-control" aria-label="切换侧边栏" tabindex="0">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <line x1="3" y1="12" x2="21" y2="12"></line>
                        <line x1="3" y1="6" x2="21" y2="6"></line>
                        <line x1="3" y1="18" x2="21" y2="18"></line>
                    </svg>
                </button>
                <h1>第3章：多层感知机</h1>
            </div>
            <div class="nav-controls">
                <button id="toggle-buddy" class="btn-control" aria-label="学习伙伴" tabindex="0">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path>
                        <circle cx="12" cy="7" r="4"></circle>
                    </svg>
                </button>
                <button id="toggle-theme" class="btn-control" aria-label="切换主题" tabindex="0">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                    </svg>
                </button>
            </div>
        </div>
        <div class="nav-progress">
            <div class="progress-bar" id="progress-bar"></div>
        </div>
    </div>
</nav>

<!-- 难度选择器 -->
<div class="difficulty-selector">
    <h4>选择学习深度</h4>
    <label class="difficulty-option active" tabindex="0">
        <input type="radio" name="difficulty" value="beginner" checked>
        <span>🌱 初学者</span>
    </label>
    <label class="difficulty-option" tabindex="0">
        <input type="radio" name="difficulty" value="intermediate">
        <span>🌿 进阶</span>
    </label>
    <label class="difficulty-option" tabindex="0">
        <input type="radio" name="difficulty" value="advanced">
        <span>🌳 深入</span>
    </label>
</div>

<!-- 侧边栏 -->
<aside class="sidebar" id="sidebar" role="navigation" aria-label="章节导航">
    <h3 style="margin-bottom: 1.5rem; color: var(--accent-purple);">目录导航</h3>
    <nav>
        <a href="#intro" class="toc-item active" tabindex="0">引言：二维世界的局限</a>
        <a href="#limitation" class="toc-item" tabindex="0">线性边界的困境</a>
        <a href="#universal" class="toc-item" tabindex="0">通用逼近定理</a>
        <a href="#activation" class="toc-item" tabindex="0">激活函数大观园</a>
        <a href="#comparison" class="toc-item" tabindex="0">激活函数对比实验</a>
        <a href="#depth-width" class="toc-item" tabindex="0">深度vs宽度的权衡</a>
        <a href="#implementation" class="toc-item" tabindex="0">代码实现：构建MLP</a>
        <a href="#visualization" class="toc-item" tabindex="0">决策边界可视化</a>
        <a href="#practice" class="toc-item" tabindex="0">实战：表格vs视觉任务</a>
        <a href="#saturation" class="toc-item" tabindex="0">激活饱和问题</a>
        <a href="#tricks" class="toc-item" tabindex="0">实用技巧与经验</a>
        <a href="#summary" class="toc-item" tabindex="0">总结：表达力的艺术</a>
    </nav>
</aside>

<!-- 侧边栏遮罩 -->
<div class="sidebar-overlay" id="sidebar-overlay"></div>

<!-- 学习伙伴系统 -->
<div class="learning-buddy" id="learning-buddy">
    <div class="buddy-header">
        <div class="buddy-avatar" id="buddy-avatar">🧠</div>
        <div class="buddy-controls">
            <button class="buddy-control-btn" id="minimize-buddy" aria-label="最小化" tabindex="0">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <line x1="5" y1="12" x2="19" y2="12"></line>
                </svg>
            </button>
            <button class="buddy-control-btn" id="close-buddy" aria-label="关闭" tabindex="0">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <line x1="18" y1="6" x2="6" y2="18"></line>
                    <line x1="6" y1="6" x2="18" y2="18"></line>
                </svg>
            </button>
        </div>
    </div>
    <div class="buddy-content">
        <div class="buddy-message">
            <p>你好！我是小智。多层感知机是深度学习的基石，让我们一起探索如何设计强大的神经网络！记住：激活函数赋予网络非线性的魔力。</p>
        </div>
        <div style="display: flex; gap: 0.5rem; margin-top: 1rem;">
            <button class="btn btn-secondary" onclick="getBuddyHint()" tabindex="0">给我提示</button>
            <button class="btn btn-secondary" onclick="getBuddySummary()" tabindex="0">总结要点</button>
        </div>
    </div>
</div>

<!-- 成就弹窗 -->
<div class="achievement-popup" id="achievement-popup">
    <div class="achievement-header">
        <div class="achievement-icon">🏆</div>
        <div class="achievement-content">
            <h3>解锁成就！</h3>
            <p id="achievement-text">完成了第一个激活函数学习</p>
        </div>
    </div>
    <div class="progress" style="margin-top: 1rem;">
        <div style="height: 8px; background: rgba(255, 255, 255, 0.1); border-radius: 4px;">
            <div id="achievement-progress" style="height: 100%; width: 20%; background: var(--accent-yellow); border-radius: 4px;"></div>
        </div>
        <p style="font-size: 0.875rem; color: var(--text-secondary); margin-top: 0.5rem;">
            <span id="achievement-count">1</span>/5 完成本章学习
        </p>
    </div>
</div>

<!-- 主内容 -->
<main>
    <!-- 章节标题 -->
    <section class="chapter-hero">
        <div class="container">
            <div class="chapter-hero-content">
                <h1>多层感知机：表达力与激活函数</h1>
                <p>从线性到非线性 —— 赋予神经网络学习复杂模式的能力</p>
                <div class="mt-4">
                    <span style="background: rgba(255, 255, 255, 0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                        🎨 表达能力
                    </span>
                    <span style="background: rgba(255, 255, 255, 0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                        ⏱️ 120分钟
                    </span>
                    <span style="background: rgba(255, 255, 255, 0.2); padding: 0.5rem 1rem; border-radius: 20px; margin: 0 0.5rem;">
                        🎯 核心概念
                    </span>
                </div>
            </div>
        </div>
    </section>

    <div class="container">
        <!-- 学习目标 -->
        <section class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📚 本章学习目标</h2>
            <div class="concept-grid">
                <div class="concept-card what" tabindex="0">
                    <h3 style="color: var(--accent-green); margin-bottom: 0.5rem;">✓ 理解表达能力</h3>
                    <p>掌握通用逼近定理，理解为什么需要非线性激活函数</p>
                </div>
                <div class="concept-card how" tabindex="0">
                    <h3 style="color: var(--accent-blue); margin-bottom: 0.5rem;">✓ 掌握激活函数</h3>
                    <p>学习各种激活函数的特性，知道如何选择合适的激活函数</p>
                </div>
                <div class="concept-card why" tabindex="0">
                    <h3 style="color: var(--accent-red); margin-bottom: 0.5rem;">✓ 权衡深度与宽度</h3>
                    <p>理解网络深度和宽度的不同作用，学会设计网络架构</p>
                </div>
                <div class="concept-card pitfall" tabindex="0">
                    <h3 style="color: var(--accent-yellow); margin-bottom: 0.5rem;">✓ 识别常见陷阱</h3>
                    <p>了解激活饱和、梯度消失等问题，掌握应对策略</p>
                </div>
            </div>
        </section>

        <!-- 学习路径推荐 -->
        <section class="learning-path">
            <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🗺️ 推荐学习路径</h3>
            <div class="path-item completed" tabindex="0">
                <div class="path-icon">✓</div>
                <div class="path-content">
                    <h4>上一章：反向传播</h4>
                    <p>掌握了训练算法</p>
                </div>
            </div>
            <div class="path-item current" tabindex="0">
                <div class="path-icon">📍</div>
                <div class="path-content">
                    <h4>当前章节：多层感知机</h4>
                    <p>探索网络的表达能力</p>
                </div>
            </div>
            <div class="path-item" tabindex="0">
                <div class="path-icon">🔜</div>
                <div class="path-content">
                    <h4>下一站：优化困境</h4>
                    <p>解决训练中的各种问题</p>
                </div>
            </div>
        </section>

        <!-- 学习循环1：引言故事 -->
        <section id="intro" class="section-card">
            <div class="learning-loop">
                <div class="loop-step active">
                    <span class="icon">🤔</span>
                    <span class="label">引发好奇</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step">
                    <span class="icon">📖</span>
                    <span class="label">核心概念</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step">
                    <span class="icon">🎯</span>
                    <span class="label">即时练习</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step">
                    <span class="icon">💡</span>
                    <span class="label">总结提升</span>
                </div>
            </div>

            <div class="story-card">
                <h2 style="color: var(--accent-yellow); margin-bottom: 1.5rem;">🎨 二维世界的局限</h2>

                <!-- 场景设定 -->
                <div style="background: rgba(0, 0, 0, 0.2); padding: 1rem; border-radius: 0.5rem; margin-bottom: 2rem;">
                    <p style="font-style: italic; color: var(--text-secondary);">
                        小陈已经掌握了反向传播算法，兴冲冲地用单层感知机去解决一个简单的分类问题。
                        然而，当他看到数据分布图时，傻眼了...
                    </p>
                </div>

                <!-- 对话场景 -->
                <div class="dialogue-container">
                    <div class="dialogue-item">
                        <div class="dialogue-avatar student">😵</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">小陈（困惑中）</div>
                            <p>教授，这些数据点...蓝色的在中间，红色的在四周，
                                怎么画直线都分不开啊！</p>
                        </div>
                    </div>
                    <div class="dialogue-item">
                        <div class="dialogue-avatar teacher">👨‍🏫</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">王教授</div>
                            <p>很好的观察！这就是著名的<strong class="handwritten">异或(XOR)问题</strong>。
                                一条直线确实无法分开，但如果我们能弯曲空间呢？</p>
                        </div>
                    </div>
                    <div class="dialogue-item">
                        <div class="dialogue-avatar student">🤯</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">小陈</div>
                            <p>弯曲空间？这不是科幻小说吗？</p>
                        </div>
                    </div>
                </div>

                <!-- 交互式演示 -->
                <div class="interactive-demo mt-3">
                    <h3 style="color: var(--accent-blue); margin-bottom: 1rem;">🎯 亲自体验：尝试分类XOR数据</h3>
                    <canvas id="xor-canvas" width="600" height="400"></canvas>
                    <div class="demo-controls mt-3">
                        <button class="btn btn-primary" onclick="addLayer()" tabindex="0">
                            ➕ 添加隐藏层
                        </button>
                        <button class="btn btn-secondary" onclick="resetXOR()" tabindex="0">
                            🔄 重置
                        </button>
                        <span style="margin-left: 1rem; color: var(--text-secondary);">
                            当前层数: <span id="layer-count">1</span>
                        </span>
                    </div>
                </div>

                <!-- 关键洞察 -->
                <div class="tip success mt-3">
                    <span class="tip-icon">💡</span>
                    <div class="tip-content">
                        <strong>顿悟时刻：</strong><br>
                        单层感知机只能学习线性边界，但现实世界的模式往往是非线性的！<br>
                        解决方案：<br>
                        1. <strong>添加隐藏层</strong>：创造中间表示<br>
                        2. <strong>非线性激活</strong>：让每层都能学习曲线<br>
                        3. <strong>组合的力量</strong>：多个简单非线性组合成复杂模式
                    </div>
                </div>
            </div>

            <!-- 即时练习 -->
            <div class="instant-practice">
                <h3 style="color: var(--accent-green); margin-bottom: 1rem;">🎯 思考练习</h3>
                <div class="practice-question">
                    为什么单层感知机无法解决XOR问题？
                </div>
                <div class="practice-options">
                    <button class="practice-option" data-answer="a" tabindex="0">
                        A. 计算能力不足
                    </button>
                    <button class="practice-option" data-answer="b" tabindex="0">
                        B. 只能表示线性决策边界
                    </button>
                    <button class="practice-option" data-answer="c" tabindex="0">
                        C. 参数太少
                    </button>
                    <button class="practice-option" data-answer="d" tabindex="0">
                        D. 训练算法有问题
                    </button>
                </div>
                <div class="practice-feedback" id="practice-feedback-1"></div>
            </div>
        </section>

        <!-- 线性边界的困境 -->
        <section id="limitation" class="section-card">
            <div class="learning-loop">
                <div class="loop-step">
                    <span class="icon">🤔</span>
                    <span class="label">引发好奇</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step active">
                    <span class="icon">📖</span>
                    <span class="label">核心概念</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step">
                    <span class="icon">🎯</span>
                    <span class="label">即时练习</span>
                    <span class="loop-connector"></span>
                </div>
                <div class="loop-step">
                    <span class="icon">💡</span>
                    <span class="label">总结提升</span>
                </div>
            </div>

            <h2 style="color: var(--accent-red); margin-bottom: 1.5rem;">
                🚧 线性边界的困境
                <span class="depth-indicator beginner">初学者</span>
            </h2>

            <!-- 问题可视化 -->
            <div class="interactive-demo">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📊 各种非线性模式挑战</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>选择数据模式</label>
                        <select id="pattern-select" class="form-select">
                            <option value="xor">XOR（异或）</option>
                            <option value="circles">同心圆</option>
                            <option value="spiral">螺旋线</option>
                            <option value="moons">月牙形</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label>噪声水平</label>
                        <div class="slider-container">
                            <input type="range" id="noise-slider" min="0" max="50" step="5" value="10">
                            <span class="slider-value" id="noise-value">10%</span>
                        </div>
                    </div>
                    <button class="btn btn-primary" onclick="generatePattern()" tabindex="0">
                        🎲 生成数据
                    </button>
                </div>

                <canvas id="pattern-canvas" width="800" height="400"></canvas>

                <div style="margin-top: 1rem; display: grid; grid-template-columns: repeat(2, 1fr); gap: 1rem;">
                    <div style="background: rgba(239, 68, 68, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-red);">线性分类器</h4>
                        <p>准确率: <span id="linear-accuracy">65.2%</span></p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            永远只能画直线...
                        </p>
                    </div>
                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-green);">多层网络</h4>
                        <p>准确率: <span id="mlp-accuracy">98.7%</span></p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            可以学习任意复杂边界！
                        </p>
                    </div>
                </div>
            </div>

            <!-- 数学解释 -->
            <div class="math-derivation mt-4">
                <h3>为什么需要非线性？</h3>

                <div class="math-step" data-step="1">
                    <strong>线性函数的组合仍是线性</strong>
                    <p>设有两个线性变换：</p>
                    <div class="equation-container">
                        <span style="font-size: 1.2rem;">
                            f(x) = W₁x + b₁<br>
                            g(x) = W₂x + b₂
                        </span>
                    </div>
                    <div class="math-explanation">
                        组合后：g(f(x)) = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂)<br>
                        仍然是线性变换！
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>加入非线性激活</strong>
                    <p>在每层之间加入非线性函数σ：</p>
                    <div class="equation-container">
                        <span style="font-size: 1.2rem; color: var(--accent-purple);">
                            h = σ(W₁x + b₁)<br>
                            y = σ(W₂h + b₂)
                        </span>
                    </div>
                    <div class="math-explanation">
                        现在无法简化成单个线性变换，网络获得了表达非线性的能力！
                    </div>
                </div>
            </div>

            <!-- 常见非线性模式 -->
            <div class="concept-grid mt-4">
                <div class="concept-card what">
                    <h3 style="color: var(--accent-green);">XOR问题</h3>
                    <p>最简单的非线性可分问题</p>
                    <canvas id="mini-xor" width="150" height="150"></canvas>
                </div>
                <div class="concept-card how">
                    <h3 style="color: var(--accent-blue);">同心圆</h3>
                    <p>径向基函数擅长的模式</p>
                    <canvas id="mini-circles" width="150" height="150"></canvas>
                </div>
                <div class="concept-card why">
                    <h3 style="color: var(--accent-red);">螺旋线</h3>
                    <p>需要深层网络的复杂模式</p>
                    <canvas id="mini-spiral" width="150" height="150"></canvas>
                </div>
                <div class="concept-card pitfall">
                    <h3 style="color: var(--accent-yellow);">月牙形</h3>
                    <p>测试泛化能力的好例子</p>
                    <canvas id="mini-moons" width="150" height="150"></canvas>
                </div>
            </div>

            <!-- 即时练习 -->
            <div class="instant-practice mt-4">
                <h3 style="color: var(--accent-green); margin-bottom: 1rem;">🎯 理解检查</h3>
                <div class="practice-question">
                    不加激活函数的100层神经网络，其表达能力相当于？
                </div>
                <div class="practice-options">
                    <button class="practice-option" data-answer="a" tabindex="0">
                        A. 100层非线性网络
                    </button>
                    <button class="practice-option" data-answer="b" tabindex="0">
                        B. 1层线性网络
                    </button>
                    <button class="practice-option" data-answer="c" tabindex="0">
                        C. 50层网络
                    </button>
                    <button class="practice-option" data-answer="d" tabindex="0">
                        D. 10层网络
                    </button>
                </div>
                <div class="practice-feedback" id="practice-feedback-2"></div>
            </div>
        </section>

        <!-- 通用逼近定理 -->
        <section id="universal" class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">
                🌟 通用逼近定理：理论保证
                <span class="depth-indicator intermediate">进阶</span>
            </h2>

            <!-- 历史背景 -->
            <div class="story-card">
                <h3>📚 1989年的重大发现</h3>
                <div class="dialogue-container">
                    <div class="dialogue-item">
                        <div class="dialogue-avatar teacher">👨‍🏫</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">王教授</div>
                            <p>1989年，Cybenko证明了一个惊人的定理：<strong>只要有足够的神经元，
                                单隐藏层的前馈网络可以逼近任意连续函数！</strong></p>
                        </div>
                    </div>
                    <div class="dialogue-item">
                        <div class="dialogue-avatar student">😮</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">小陈</div>
                            <p>真的吗？那为什么还需要深度网络？</p>
                        </div>
                    </div>
                    <div class="dialogue-item">
                        <div class="dialogue-avatar teacher">👨‍🏫</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">王教授</div>
                            <p>好问题！理论上可以，但<span class="handwritten">"足够"可能意味着天文数字般的神经元</span>。
                                深度网络更高效！</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- 定理内容 -->
            <div class="math-derivation">
                <h3>通用逼近定理（Universal Approximation Theorem）</h3>

                <div class="math-step" data-step="1">
                    <strong>定理陈述</strong>
                    <div class="equation-container">
                        <span class="equation-label">Cybenko, 1989</span><br>
                        <p style="text-align: left; padding: 0 2rem;">
                            设 σ 是一个非常数、有界、单调递增的连续函数。<br>
                            令 I_m 表示 m 维单位超立方体 [0,1]^m。<br>
                            C(I_m) 表示 I_m 上的连续函数空间。<br><br>
                            则对于任意 f ∈ C(I_m) 和 ε > 0，存在整数 N 和常数 v_i, b_i ∈ ℝ, w_i ∈ ℝ^m，使得：
                        </p>
                    </div>
                    <div class="equation-container">
                        <span style="font-size: 1.3rem; color: var(--accent-purple);">
                            |f(x) - ∑_{i=1}^N v_i σ(w_i^T x + b_i)| < ε
                        </span>
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>通俗解释</strong>
                    <div class="math-explanation">
                        • 单隐藏层网络是"通用函数逼近器"<br>
                        • 只要神经元足够多，可以逼近任意精度<br>
                        • 激活函数需要是非线性的（如sigmoid）<br>
                        • 这是存在性证明，不告诉我们需要多少神经元
                    </div>
                </div>
            </div>

            <!-- 交互式函数逼近演示 -->
            <div class="interactive-demo mt-4">
                <h3 style="color: var(--accent-yellow); margin-bottom: 1.5rem;">🎮 动手实验：逼近任意函数</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>目标函数</label>
                        <select id="target-function" class="form-select">
                            <option value="sine">正弦波</option>
                            <option value="step">阶跃函数</option>
                            <option value="abs">绝对值</option>
                            <option value="custom">自定义（可画）</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label>隐藏层神经元数</label>
                        <div class="slider-container">
                            <input type="range" id="neuron-slider" min="1" max="100" step="1" value="10">
                            <span class="slider-value" id="neuron-value">10</span>
                        </div>
                    </div>
                    <button class="btn btn-primary" onclick="trainApproximator()" tabindex="0">
                        🎯 开始训练
                    </button>
                    <button class="btn btn-secondary" onclick="resetApproximator()" tabindex="0">
                        🔄 重置
                    </button>
                </div>

                <canvas id="approximation-canvas" width="800" height="400"></canvas>

                <div class="chart-legend">
                    <div class="legend-item">
                        <div class="legend-color" style="background: #3b82f6;"></div>
                        <span>目标函数</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #22c55e;"></div>
                        <span>网络输出</span>
                    </div>
                    <div class="legend-item">
                        <div class="legend-color" style="background: #ef4444;"></div>
                        <span>误差</span>
                    </div>
                </div>

                <div style="margin-top: 1rem; text-align: center;">
                    <p>训练损失: <span id="approx-loss" style="font-weight: bold;">0.0000</span></p>
                    <p style="color: var(--text-secondary); font-size: 0.875rem;">
                        提示：增加神经元数量可以更好地逼近复杂函数
                    </p>
                </div>
            </div>

            <!-- 深度vs宽度的初步讨论 -->
            <div class="tip info mt-4">
                <span class="tip-icon">🤔</span>
                <div class="tip-content">
                    <strong>思考：既然单层就够了，为什么要用深度网络？</strong><br>
                    • <strong>效率问题</strong>：浅层网络可能需要指数级的宽度<br>
                    • <strong>特征层次</strong>：深度网络自然学习从低级到高级的特征<br>
                    • <strong>参数共享</strong>：深度网络更好地利用参数<br>
                    • <strong>归纳偏置</strong>：深度结构更符合现实世界的层次性
                </div>
            </div>
        </section>

        <!-- 激活函数大观园 -->
        <section id="activation" class="section-card">
            <h2 style="color: var(--accent-green); margin-bottom: 1.5rem;">
                🎨 激活函数大观园
                <span class="depth-indicator beginner">初学者</span>
            </h2>

            <!-- 激活函数的作用 -->
            <div class="dialogue-container">
                <div class="dialogue-item">
                    <div class="dialogue-avatar student">🤔</div>
                    <div class="dialogue-content">
                        <div class="dialogue-name">小陈</div>
                        <p>教授，为什么有这么多种激活函数？用哪个都行吗？</p>
                    </div>
                </div>
                <div class="dialogue-item">
                    <div class="dialogue-avatar teacher">👨‍🏫</div>
                    <div class="dialogue-content">
                        <div class="dialogue-name">王教授</div>
                        <p>每种激活函数都有自己的"性格"！就像不同的画笔能画出不同的效果。
                            让我们来认识一下这些"艺术家"...</p>
                    </div>
                </div>
            </div>

            <!-- 激活函数交互式展示 -->
            <div class="interactive-demo mt-3">
                <h3 style="color: var(--accent-blue); margin-bottom: 1.5rem;">🔬 激活函数实验室</h3>

                <div class="demo-controls">
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(120px, 1fr)); gap: 0.5rem;">
                        <button class="btn btn-secondary activation-btn active" data-activation="sigmoid" tabindex="0">
                            Sigmoid
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="tanh" tabindex="0">
                            Tanh
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="relu" tabindex="0">
                            ReLU
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="leaky" tabindex="0">
                            Leaky ReLU
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="elu" tabindex="0">
                            ELU
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="swish" tabindex="0">
                            Swish
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="gelu" tabindex="0">
                            GELU
                        </button>
                        <button class="btn btn-secondary activation-btn" data-activation="all" tabindex="0">
                            全部对比
                        </button>
                    </div>
                </div>

                <canvas id="activation-canvas" width="800" height="400"></canvas>

                <div id="activation-info" style="margin-top: 1rem; background: rgba(139, 92, 246, 0.1); padding: 1.5rem; border-radius: 0.5rem;">
                    <h4 style="color: var(--accent-purple); margin-bottom: 1rem;">Sigmoid函数</h4>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                        <div>
                            <p><strong>数学形式：</strong></p>
                            <div class="equation-container">
                                <span>σ(x) = 1 / (1 + e^(-x))</span>
                            </div>
                            <p><strong>导数：</strong></p>
                            <div class="equation-container">
                                <span>σ'(x) = σ(x)(1 - σ(x))</span>
                            </div>
                        </div>
                        <div>
                            <p><strong>特性：</strong></p>
                            <ul style="line-height: 1.8;">
                                <li>✅ 输出范围 (0, 1)</li>
                                <li>✅ 平滑可微</li>
                                <li>❌ 容易饱和</li>
                                <li>❌ 输出不是零中心</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

            <!-- 激活函数详细对比 -->
            <div class="concept-grid mt-4">
                <div class="concept-card what">
                    <h3 style="color: var(--accent-green);">经典函数</h3>
                    <p><strong>Sigmoid & Tanh</strong></p>
                    <ul style="font-size: 0.9rem; line-height: 1.6;">
                        <li>历史悠久，理论完善</li>
                        <li>容易饱和，梯度消失</li>
                        <li>计算开销较大</li>
                    </ul>
                    <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                        适用：输出层、概率输出
                    </p>
                </div>
                <div class="concept-card how">
                    <h3 style="color: var(--accent-blue);">ReLU家族</h3>
                    <p><strong>ReLU, Leaky ReLU, ELU</strong></p>
                    <ul style="font-size: 0.9rem; line-height: 1.6;">
                        <li>计算简单高效</li>
                        <li>缓解梯度消失</li>
                        <li>可能有死神经元</li>
                    </ul>
                    <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                        适用：隐藏层首选
                    </p>
                </div>
                <div class="concept-card why">
                    <h3 style="color: var(--accent-red);">现代函数</h3>
                    <p><strong>Swish, GELU, Mish</strong></p>
                    <ul style="font-size: 0.9rem; line-height: 1.6;">
                        <li>平滑非单调</li>
                        <li>自门控机制</li>
                        <li>SOTA模型常用</li>
                    </ul>
                    <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                        适用：追求极致性能
                    </p>
                </div>
                <div class="concept-card pitfall">
                    <h3 style="color: var(--accent-yellow);">特殊用途</h3>
                    <p><strong>Softmax, Softplus</strong></p>
                    <ul style="font-size: 0.9rem; line-height: 1.6;">
                        <li>Softmax: 多分类输出</li>
                        <li>Softplus: 平滑的ReLU</li>
                        <li>特定场景优化</li>
                    </ul>
                    <p style="color: var(--text-secondary); margin-top: 0.5rem;">
                        适用：特定任务需求
                    </p>
                </div>
            </div>

            <!-- 激活函数选择指南 -->
            <div class="tip success mt-4">
                <span class="tip-icon">📋</span>
                <div class="tip-content">
                    <strong>激活函数选择快速指南：</strong><br>
                    • <strong>默认选择</strong>：ReLU（简单高效）<br>
                    • <strong>深层网络</strong>：Leaky ReLU 或 ELU（避免死神经元）<br>
                    • <strong>输出层</strong>：根据任务选择（Sigmoid/Softmax/Linear）<br>
                    • <strong>追求性能</strong>：GELU 或 Swish（Transformer常用）<br>
                    • <strong>老网络</strong>：Tanh（LSTM/GRU默认）
                </div>
            </div>

            <!-- 死亡ReLU问题演示 -->
            <div class="common-mistakes mt-4">
                <h3 style="color: var(--accent-red); margin-bottom: 1rem;">⚠️ 常见陷阱：死亡ReLU</h3>

                <div class="mistake-item">
                    <div class="mistake-icon">💀</div>
                    <div class="mistake-content">
                        <h4>什么是死亡ReLU？</h4>
                        <p>当ReLU神经元的输入总是负数时，梯度永远为0，神经元"死亡"。</p>
                        <div style="background: rgba(0, 0, 0, 0.2); padding: 1rem; border-radius: 0.5rem; margin-top: 0.5rem;">
                            <code>如果 x < 0, 则 ReLU(x) = 0, 梯度也是 0</code>
                        </div>
                        <p style="color: var(--accent-green); margin-top: 0.5rem;">
                            ✓ 解决方案：使用Leaky ReLU (负数部分有小斜率) 或正确的初始化
                        </p>
                    </div>
                </div>
            </div>
        </section>

        <!-- 激活函数对比实验 -->
        <section id="comparison" class="section-card">
            <h2 style="color: var(--accent-blue); margin-bottom: 1.5rem;">
                🧪 激活函数对比实验
                <span class="depth-indicator intermediate">进阶</span>
            </h2>

            <!-- 实验代码 -->
            <div class="code-container">
                <div class="code-header">
                    <div class="code-header-left">
                        <button class="code-tab active" data-lang="pytorch" tabindex="0">PyTorch实现</button>
                        <button class="code-tab" data-lang="numpy" tabindex="0">NumPy实现</button>
                    </div>
                    <div class="code-header-right">
                        <button class="code-action" onclick="copyCode()" tabindex="0">
                            <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                                <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
                                <path d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"></path>
                            </svg>
                            复制
                        </button>
                        <button class="code-action" onclick="runCode()" tabindex="0">
                            ▶️ 运行
                        </button>
                    </div>
                </div>
                <div class="code-content" id="pytorch-code">
<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# 定义各种激活函数
class ActivationFunctions:
    """常用激活函数集合"""

    @staticmethod
    def sigmoid(x):
        return torch.sigmoid(x)

    @staticmethod
    def tanh(x):
        return torch.tanh(x)

    @staticmethod
    def relu(x):
        return F.relu(x)

    @staticmethod
    def leaky_relu(x, negative_slope=0.01):
        return F.leaky_relu(x, negative_slope)

    @staticmethod
    def elu(x, alpha=1.0):
        return F.elu(x, alpha)

    @staticmethod
    def swish(x):
        return x * torch.sigmoid(x)

    @staticmethod
    def gelu(x):
        return F.gelu(x)

    @staticmethod
    def mish(x):
        return x * torch.tanh(F.softplus(x))


# 创建对比实验
def compare_activations(x_range=(-5, 5), num_points=1000):
    """
    比较不同激活函数的特性

    Args:
        x_range: x轴范围
        num_points: 采样点数
    """
    x = torch.linspace(x_range[0], x_range[1], num_points)
    x.requires_grad = True

    activations = {
        'Sigmoid': ActivationFunctions.sigmoid,
        'Tanh': ActivationFunctions.tanh,
        'ReLU': ActivationFunctions.relu,
        'Leaky ReLU': ActivationFunctions.leaky_relu,
        'ELU': ActivationFunctions.elu,
        'Swish': ActivationFunctions.swish,
        'GELU': ActivationFunctions.gelu
    }

    fig, axes = plt.subplots(2, len(activations), figsize=(20, 8))
    fig.suptitle('激活函数及其导数对比', fontsize=16)

    for idx, (name, func) in enumerate(activations.items()):
        # 计算激活值
        y = func(x.clone())

        # 计算导数
        x_grad = x.clone().detach().requires_grad_(True)
        y_grad = func(x_grad)
        y_grad.backward(torch.ones_like(y_grad))
        gradient = x_grad.grad

        # 绘制激活函数
        ax1 = axes[0, idx]
        ax1.plot(x.detach().numpy(), y.detach().numpy(), 'b-', linewidth=2)
        ax1.set_title(name)
        ax1.grid(True, alpha=0.3)
        ax1.axhline(y=0, color='k', linewidth=0.5)
        ax1.axvline(x=0, color='k', linewidth=0.5)
        if idx == 0:
            ax1.set_ylabel('f(x)', fontsize=12)

        # 绘制导数
        ax2 = axes[1, idx]
        ax2.plot(x.detach().numpy(), gradient.numpy(), 'r-', linewidth=2)
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='k', linewidth=0.5)
        ax2.axvline(x=0, color='k', linewidth=0.5)
        if idx == 0:
            ax2.set_ylabel("f'(x)", fontsize=12)
        ax2.set_xlabel('x')

        # 标注关键统计信息
        ax1.text(0.05, 0.95, f'Range: [{y.min():.2f}, {y.max():.2f}]',
                transform=ax1.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.show()


# 测试激活函数对梯度流的影响
class GradientFlowExperiment:
    """梯度流实验：测试不同激活函数在深层网络中的表现"""

    def __init__(self, input_size=10, hidden_size=50, num_layers=20):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

    def create_network(self, activation='relu'):
        """创建指定激活函数的深层网络"""
        layers = []

        # 输入层
        layers.append(nn.Linear(self.input_size, self.hidden_size))

        # 隐藏层
        for _ in range(self.num_layers - 2):
            if activation == 'relu':
                layers.append(nn.ReLU())
            elif activation == 'tanh':
                layers.append(nn.Tanh())
            elif activation == 'leaky_relu':
                layers.append(nn.LeakyReLU())
            elif activation == 'elu':
                layers.append(nn.ELU())
            elif activation == 'gelu':
                layers.append(nn.GELU())

            layers.append(nn.Linear(self.hidden_size, self.hidden_size))

        # 输出层
        layers.append(nn.Linear(self.hidden_size, 1))

        return nn.Sequential(*layers)

    def analyze_gradients(self, network, num_samples=100):
        """分析梯度在网络中的传播"""
        # 生成随机输入
        x = torch.randn(num_samples, self.input_size)
        target = torch.randn(num_samples, 1)

        # 前向传播
        output = network(x)
        loss = F.mse_loss(output, target)

        # 反向传播
        loss.backward()

        # 收集每层的梯度统计
        gradient_stats = []
        for i, module in enumerate(network.modules()):
            if isinstance(module, nn.Linear):
                grad = module.weight.grad
                if grad is not None:
                    gradient_stats.append({
                        'layer': i,
                        'mean': grad.abs().mean().item(),
                        'std': grad.std().item(),
                        'max': grad.abs().max().item(),
                        'min': grad.abs().min().item()
                    })

        return gradient_stats

    def compare_gradient_flow(self):
        """比较不同激活函数的梯度流"""
        activations = ['relu', 'tanh', 'leaky_relu', 'elu', 'gelu']
        results = {}

        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
        fig.suptitle(f'梯度流分析 ({self.num_layers}层网络)', fontsize=16)

        for activation in activations:
            # 创建网络
            network = self.create_network(activation)

            # 初始化权重
            for module in network.modules():
                if isinstance(module, nn.Linear):
                    nn.init.xavier_normal_(module.weight)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)

            # 分析梯度
            stats = self.analyze_gradients(network)
            results[activation] = stats

            # 提取统计数据
            layers = [s['layer'] for s in stats]
            means = [s['mean'] for s in stats]
            stds = [s['std'] for s in stats]

            # 绘制梯度均值
            axes[0].semilogy(layers, means, 'o-', label=activation, linewidth=2)

            # 绘制梯度标准差
            axes[1].semilogy(layers, stds, 's-', label=activation, linewidth=2)

        axes[0].set_ylabel('梯度绝对值均值 (log scale)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        axes[0].set_title('梯度幅度随层深度的变化')

        axes[1].set_xlabel('层索引')
        axes[1].set_ylabel('梯度标准差 (log scale)')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        axes[1].set_title('梯度方差随层深度的变化')

        plt.tight_layout()
        plt.show()

        return results


# 实际任务中的激活函数效果
def activation_performance_test():
    """在实际任务上测试不同激活函数的性能"""

    # 生成螺旋数据集
    def generate_spiral_data(n_points=1000, n_classes=3, noise=0.2):
        n_per_class = n_points // n_classes
        X = []
        y = []

        for j in range(n_classes):
            theta = np.linspace(j * 4, (j + 1) * 4, n_per_class) + \
                    np.random.randn(n_per_class) * noise
            r = np.linspace(0.5, 1, n_per_class)
            x = r * np.sin(theta)
            y_coord = r * np.cos(theta)
            X.append(np.c_[x, y_coord])
            y.extend([j] * n_per_class)

        X = np.concatenate(X)
        y = np.array(y)

        return torch.FloatTensor(X), torch.LongTensor(y)

    # 定义不同激活函数的网络
    class SpiralNet(nn.Module):
        def __init__(self, activation='relu'):
            super().__init__()
            self.fc1 = nn.Linear(2, 100)
            self.fc2 = nn.Linear(100, 100)
            self.fc3 = nn.Linear(100, 3)

            if activation == 'relu':
                self.activation = nn.ReLU()
            elif activation == 'tanh':
                self.activation = nn.Tanh()
            elif activation == 'leaky_relu':
                self.activation = nn.LeakyReLU()
            elif activation == 'elu':
                self.activation = nn.ELU()
            elif activation == 'gelu':
                self.activation = nn.GELU()

        def forward(self, x):
            x = self.activation(self.fc1(x))
            x = self.activation(self.fc2(x))
            x = self.fc3(x)
            return x

    # 生成数据
    X_train, y_train = generate_spiral_data(n_points=3000)
    X_test, y_test = generate_spiral_data(n_points=1000)

    # 训练不同激活函数的网络
    activations = ['relu', 'tanh', 'leaky_relu', 'elu', 'gelu']
    results = {}

    for activation in activations:
        print(f"\n训练 {activation} 网络...")

        model = SpiralNet(activation=activation)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.CrossEntropyLoss()

        train_losses = []
        test_accuracies = []

        # 训练
        for epoch in range(100):
            # 训练步骤
            model.train()
            optimizer.zero_grad()
            output = model(X_train)
            loss = criterion(output, y_train)
            loss.backward()
            optimizer.step()

            train_losses.append(loss.item())

            # 测试步骤
            if epoch % 10 == 0:
                model.eval()
                with torch.no_grad():
                    test_output = model(X_test)
                    _, predicted = torch.max(test_output, 1)
                    accuracy = (predicted == y_test).float().mean().item()
                    test_accuracies.append(accuracy)
                    print(f"  Epoch {epoch}, Loss: {loss.item():.4f}, "
                          f"Test Acc: {accuracy:.2%}")

        results[activation] = {
            'train_losses': train_losses,
            'test_accuracies': test_accuracies,
            'final_accuracy': test_accuracies[-1]
        }

    # 可视化结果
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # 训练损失曲线
    for activation, result in results.items():
        ax1.plot(result['train_losses'], label=activation, linewidth=2)

    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Training Loss')
    ax1.set_title('不同激活函数的训练损失')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 最终准确率对比
    activations = list(results.keys())
    accuracies = [results[a]['final_accuracy'] for a in activations]

    bars = ax2.bar(activations, accuracies, color='skyblue', edgecolor='navy')
    ax2.set_ylabel('Test Accuracy')
    ax2.set_title('不同激活函数的最终测试准确率')
    ax2.set_ylim(0, 1)

    # 添加数值标签
    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.1%}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    return results


if __name__ == "__main__":
    print("=== 激活函数对比实验 ===\n")

    # 1. 可视化激活函数及其导数
    print("1. 绘制激活函数图像...")
    compare_activations()

    # 2. 梯度流分析
    print("\n2. 分析深层网络中的梯度流...")
    experiment = GradientFlowExperiment(num_layers=20)
    gradient_results = experiment.compare_gradient_flow()

    # 3. 实际任务性能测试
    print("\n3. 在螺旋数据集上测试性能...")
    performance_results = activation_performance_test()

    print("\n=== 实验总结 ===")
    print("1. ReLU计算简单但可能有死神经元问题")
    print("2. Tanh/Sigmoid容易饱和，深层网络梯度消失严重")
    print("3. Leaky ReLU/ELU缓解了死神经元问题")
    print("4. GELU等现代激活函数在某些任务上表现更好")</code></pre>
                </div>
                <div class="code-content" id="numpy-code" style="display: none;">
<pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

class ActivationFunctionsNumPy:
    """使用NumPy实现的激活函数"""

    @staticmethod
    def sigmoid(x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))

    @staticmethod
    def sigmoid_derivative(x):
        """Sigmoid导数"""
        s = ActivationFunctionsNumPy.sigmoid(x)
        return s * (1 - s)

    @staticmethod
    def tanh(x):
        """Tanh激活函数"""
        return np.tanh(x)

    @staticmethod
    def tanh_derivative(x):
        """Tanh导数"""
        return 1 - np.tanh(x) ** 2

    @staticmethod
    def relu(x):
        """ReLU激活函数"""
        return np.maximum(0, x)

    @staticmethod
    def relu_derivative(x):
        """ReLU导数"""
        return (x > 0).astype(float)

    @staticmethod
    def leaky_relu(x, alpha=0.01):
        """Leaky ReLU激活函数"""
        return np.where(x > 0, x, alpha * x)

    @staticmethod
    def leaky_relu_derivative(x, alpha=0.01):
        """Leaky ReLU导数"""
        return np.where(x > 0, 1, alpha)

    @staticmethod
    def elu(x, alpha=1.0):
        """ELU激活函数"""
        return np.where(x > 0, x, alpha * (np.exp(x) - 1))

    @staticmethod
    def elu_derivative(x, alpha=1.0):
        """ELU导数"""
        return np.where(x > 0, 1, alpha * np.exp(x))

    @staticmethod
    def swish(x):
        """Swish激活函数"""
        return x * ActivationFunctionsNumPy.sigmoid(x)

    @staticmethod
    def swish_derivative(x):
        """Swish导数"""
        sigmoid_x = ActivationFunctionsNumPy.sigmoid(x)
        return sigmoid_x + x * sigmoid_x * (1 - sigmoid_x)

    @staticmethod
    def gelu(x):
        """GELU激活函数（高斯误差线性单元）"""
        return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

    @staticmethod
    def gelu_derivative(x):
        """GELU导数的近似"""
        # 使用数值微分
        h = 1e-5
        return (ActivationFunctionsNumPy.gelu(x + h) -
                ActivationFunctionsNumPy.gelu(x - h)) / (2 * h)


# 激活函数可视化比较
def visualize_activations_numpy():
    """使用NumPy可视化所有激活函数"""

    x = np.linspace(-5, 5, 1000)

    # 定义激活函数及其导数
    activations = {
        'Sigmoid': (ActivationFunctionsNumPy.sigmoid,
                   ActivationFunctionsNumPy.sigmoid_derivative),
        'Tanh': (ActivationFunctionsNumPy.tanh,
                ActivationFunctionsNumPy.tanh_derivative),
        'ReLU': (ActivationFunctionsNumPy.relu,
                ActivationFunctionsNumPy.relu_derivative),
        'Leaky ReLU': (ActivationFunctionsNumPy.leaky_relu,
                      ActivationFunctionsNumPy.leaky_relu_derivative),
        'ELU': (ActivationFunctionsNumPy.elu,
               ActivationFunctionsNumPy.elu_derivative),
        'Swish': (ActivationFunctionsNumPy.swish,
                 ActivationFunctionsNumPy.swish_derivative),
        'GELU': (ActivationFunctionsNumPy.gelu,
                ActivationFunctionsNumPy.gelu_derivative)
    }

    # 创建图表
    fig, axes = plt.subplots(3, 3, figsize=(15, 12))
    axes = axes.flatten()

    for idx, (name, (func, deriv)) in enumerate(activations.items()):
        if idx < len(axes):
            ax = axes[idx]

            # 计算函数值和导数
            y = func(x)
            dy = deriv(x)

            # 绘制函数
            ax.plot(x, y, 'b-', linewidth=2, label=f'{name}')
            ax.plot(x, dy, 'r--', linewidth=2, label=f"{name}'")

            # 设置图表
            ax.set_title(name, fontsize=14)
            ax.grid(True, alpha=0.3)
            ax.axhline(y=0, color='k', linewidth=0.5)
            ax.axvline(x=0, color='k', linewidth=0.5)
            ax.legend()
            ax.set_xlim(-5, 5)
            ax.set_ylim(-1.5, 1.5)

            # 添加关键信息
            ax.text(0.02, 0.98, f'f(0)={func(0):.3f}',
                   transform=ax.transAxes,
                   verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    # 隐藏多余的子图
    for idx in range(len(activations), len(axes)):
        axes[idx].set_visible(False)

    plt.suptitle('激活函数及其导数比较', fontsize=16)
    plt.tight_layout()
    plt.show()


# 梯度消失/爆炸实验
def gradient_vanishing_experiment():
    """演示不同激活函数的梯度消失问题"""

    np.random.seed(42)

    # 网络参数
    input_size = 10
    hidden_size = 50
    output_size = 1
    num_layers = 20

    # 初始化网络权重
    def init_network(num_layers, hidden_size, activation='relu'):
        weights = []
        biases = []

        # 输入层
        if activation in ['relu', 'leaky_relu']:
            # He初始化
            w = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)
        else:
            # Xavier初始化
            w = np.random.randn(hidden_size, input_size) * np.sqrt(1.0 / input_size)

        weights.append(w)
        biases.append(np.zeros((hidden_size, 1)))

        # 隐藏层
        for _ in range(num_layers - 2):
            if activation in ['relu', 'leaky_relu']:
                w = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / hidden_size)
            else:
                w = np.random.randn(hidden_size, hidden_size) * np.sqrt(1.0 / hidden_size)

            weights.append(w)
            biases.append(np.zeros((hidden_size, 1)))

        # 输出层
        w = np.random.randn(output_size, hidden_size) * np.sqrt(1.0 / hidden_size)
        weights.append(w)
        biases.append(np.zeros((output_size, 1)))

        return weights, biases

    # 前向传播
    def forward(x, weights, biases, activation='relu'):
        activations = [x]
        pre_activations = []

        for i, (w, b) in enumerate(zip(weights, biases)):
            z = np.dot(w, activations[-1]) + b
            pre_activations.append(z)

            if i < len(weights) - 1:  # 隐藏层使用激活函数
                if activation == 'relu':
                    a = ActivationFunctionsNumPy.relu(z)
                elif activation == 'tanh':
                    a = ActivationFunctionsNumPy.tanh(z)
                elif activation == 'sigmoid':
                    a = ActivationFunctionsNumPy.sigmoid(z)
                elif activation == 'leaky_relu':
                    a = ActivationFunctionsNumPy.leaky_relu(z)
            else:  # 输出层不使用激活函数
                a = z

            activations.append(a)

        return activations, pre_activations

    # 反向传播
    def backward(activations, pre_activations, weights, target, activation='relu'):
        gradients = []

        # 输出层梯度
        delta = activations[-1] - target
        gradients.append(delta)

        # 反向传播
        for i in range(len(weights) - 2, -1, -1):
            # 传播梯度
            delta = np.dot(weights[i + 1].T, delta)

            # 乘以激活函数导数
            if activation == 'relu':
                delta = delta * ActivationFunctionsNumPy.relu_derivative(pre_activations[i])
            elif activation == 'tanh':
                delta = delta * ActivationFunctionsNumPy.tanh_derivative(pre_activations[i])
            elif activation == 'sigmoid':
                delta = delta * ActivationFunctionsNumPy.sigmoid_derivative(pre_activations[i])
            elif activation == 'leaky_relu':
                delta = delta * ActivationFunctionsNumPy.leaky_relu_derivative(pre_activations[i])

            gradients.append(delta)

        gradients.reverse()
        return gradients

    # 实验不同激活函数
    activations_to_test = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
    results = {}

    # 生成测试数据
    x = np.random.randn(input_size, 1)
    target = np.random.randn(output_size, 1)

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    for activation in activations_to_test:
        # 初始化网络
        weights, biases = init_network(num_layers, hidden_size, activation)

        # 前向传播
        activations_list, pre_activations = forward(x, weights, biases, activation)

        # 反向传播
        gradients = backward(activations_list, pre_activations, weights, target, activation)

        # 收集梯度统计
        grad_norms = []
        activation_means = []

        for i, grad in enumerate(gradients):
            grad_norms.append(np.linalg.norm(grad))
            if i < len(activations_list) - 1:
                activation_means.append(np.mean(np.abs(activations_list[i + 1])))

        results[activation] = {
            'grad_norms': grad_norms,
            'activation_means': activation_means
        }

        # 绘制梯度范数
        ax1.semilogy(range(len(grad_norms)), grad_norms, 'o-',
                    label=activation, linewidth=2, markersize=6)

        # 绘制激活值均值
        ax2.plot(range(len(activation_means)), activation_means, 's-',
                label=activation, linewidth=2, markersize=6)

    ax1.set_xlabel('层索引（从输入到输出）')
    ax1.set_ylabel('梯度范数 (log scale)')
    ax1.set_title(f'梯度在{num_layers}层网络中的传播')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    ax2.set_xlabel('层索引')
    ax2.set_ylabel('激活值绝对值均值')
    ax2.set_title('各层激活值统计')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 打印分析结果
    print("\n=== 梯度传播分析 ===")
    for activation, result in results.items():
        first_grad = result['grad_norms'][0]
        last_grad = result['grad_norms'][-1]
        ratio = first_grad / last_grad if last_grad > 0 else float('inf')

        print(f"\n{activation.upper()}:")
        print(f"  第一层梯度范数: {first_grad:.6f}")
        print(f"  最后层梯度范数: {last_grad:.6f}")
        print(f"  衰减比例: {ratio:.2e}")

        if ratio > 1e6:
            print("  ⚠️ 严重梯度消失！")
        elif ratio > 1e3:
            print("  ⚠️ 轻度梯度消失")
        elif ratio < 1e-3:
            print("  🔥 梯度爆炸风险！")
        else:
            print("  ✅ 梯度传播正常")


# 演示死亡ReLU问题
def dead_relu_demo():
    """演示死亡ReLU现象"""

    np.random.seed(42)

    # 创建一个简单的网络
    input_size = 2
    hidden_size = 100
    output_size = 1

    # 使用较大的负偏置初始化（容易造成死亡ReLU）
    W1 = np.random.randn(hidden_size, input_size) * 0.5
    b1 = np.ones((hidden_size, 1)) * (-5)  # 大的负偏置
    W2 = np.random.randn(output_size, hidden_size) * 0.5
    b2 = np.zeros((output_size, 1))

    # 生成一些训练数据
    X = np.random.randn(input_size, 1000)

    # 前向传播
    z1 = np.dot(W1, X) + b1
    a1_relu = ActivationFunctionsNumPy.relu(z1)
    a1_leaky = ActivationFunctionsNumPy.leaky_relu(z1)

    # 统计死亡神经元
    dead_neurons_relu = np.sum(np.all(a1_relu == 0, axis=1))
    dead_neurons_leaky = np.sum(np.all(a1_leaky == 0, axis=1))

    active_percentage_relu = (hidden_size - dead_neurons_relu) / hidden_size * 100
    active_percentage_leaky = (hidden_size - dead_neurons_leaky) / hidden_size * 100

    # 可视化
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # ReLU激活模式
    ax1.imshow(a1_relu[:50, :100], aspect='auto', cmap='hot')
    ax1.set_title(f'ReLU激活模式\n活跃神经元: {active_percentage_relu:.1f}%')
    ax1.set_xlabel('样本')
    ax1.set_ylabel('神经元')
    ax1.colorbar = plt.colorbar(ax1.images[0], ax=ax1)

    # Leaky ReLU激活模式
    ax2.imshow(a1_leaky[:50, :100], aspect='auto', cmap='hot')
    ax2.set_title(f'Leaky ReLU激活模式\n活跃神经元: {active_percentage_leaky:.1f}%')
    ax2.set_xlabel('样本')
    ax2.set_ylabel('神经元')
    ax2.colorbar = plt.colorbar(ax2.images[0], ax=ax2)

    plt.tight_layout()
    plt.show()

    print("\n=== 死亡ReLU分析 ===")
    print(f"ReLU死亡神经元数: {dead_neurons_relu}/{hidden_size} ({100-active_percentage_relu:.1f}%)")
    print(f"Leaky ReLU死亡神经元数: {dead_neurons_leaky}/{hidden_size} ({100-active_percentage_leaky:.1f}%)")
    print("\n💡 提示：")
    print("- 死亡ReLU神经元的输出永远为0，无法学习")
    print("- Leaky ReLU通过允许小的负斜率避免了这个问题")
    print("- 正确的初始化也可以减少死亡神经元")


if __name__ == "__main__":
    print("=== NumPy激活函数实验 ===\n")

    # 1. 可视化所有激活函数
    print("1. 可视化激活函数及其导数...")
    visualize_activations_numpy()

    # 2. 梯度消失实验
    print("\n2. 测试深层网络中的梯度传播...")
    gradient_vanishing_experiment()

    # 3. 死亡ReLU演示
    print("\n3. 演示死亡ReLU问题...")
    dead_relu_demo()

    print("\n实验完成！")
    <div class="code-output" id="code-output">
                    <strong>运行结果：</strong>
                    <pre>正在运行激活函数对比实验...
1. 绘制激活函数图像... ✓
2. 分析深层网络中的梯度流... ✓
3. 在螺旋数据集上测试性能... ✓

最终准确率：
- ReLU: 96.8%
- GELU: 97.5%
- Tanh: 89.2%</pre>
                </div>
                </div>

                <!-- 实验结论 -->
                <div class="tip success mt-3">
                    <span class="tip-icon">📊</span>
                    <div class="tip-content">
                        <strong>实验发现：</strong><br>
                        • ReLU在大多数情况下表现良好，训练速度快<br>
                        • GELU等现代激活函数在复杂任务上略有优势<br>
                        • Sigmoid/Tanh在深层网络中容易梯度消失<br>
                        • 正确的初始化对所有激活函数都很重要
                    </div>
                </div>
        </section>

        <!-- 深度vs宽度的权衡 -->
        <section id="depth-width" class="section-card">
            <h2 style="color: var(--accent-yellow); margin-bottom: 1.5rem;">
                ⚖️ 深度vs宽度的权衡
                <span class="depth-indicator advanced">深入</span>
            </h2>

            <!-- 引入对话 -->
            <div class="dialogue-container">
                <div class="dialogue-item">
                    <div class="dialogue-avatar student">🤔</div>
                    <div class="dialogue-content">
                        <div class="dialogue-name">小陈</div>
                        <p>教授，既然单层网络理论上可以逼近任何函数，
                            为什么还要用深度网络？直接加宽不就行了？</p>
                    </div>
                </div>
                <div class="dialogue-item">
                    <div class="dialogue-avatar teacher">👨‍🏫</div>
                    <div class="dialogue-content">
                        <div class="dialogue-name">王教授</div>
                        <p>这是个深刻的问题！让我用一个类比：建造高楼 vs 摊大饼。
                            两种方式都能增加面积，但效率和功能完全不同...</p>
                    </div>
                </div>
            </div>

            <!-- 交互式对比实验 -->
            <div class="interactive-demo">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🔬 深度 vs 宽度实验室</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>网络配置</label>
                        <select id="network-config" class="form-select">
                            <option value="shallow-wide">浅而宽 (1层×1000)</option>
                            <option value="medium">中等 (3层×100)</option>
                            <option value="deep-narrow">深而窄 (10层×20)</option>
                            <option value="custom">自定义</option>
                        </select>
                    </div>
                    <div class="control-group" id="custom-controls" style="display: none;">
                        <label>层数</label>
                        <input type="number" id="custom-depth" min="1" max="20" value="5" style="width: 60px;">
                        <label style="margin-left: 1rem;">宽度</label>
                        <input type="number" id="custom-width" min="10" max="1000" value="50" style="width: 80px;">
                    </div>
                    <button class="btn btn-primary" onclick="compareArchitectures()" tabindex="0">
                        🚀 开始对比
                    </button>
                </div>

                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem; margin-top: 2rem;">
                    <div>
                        <canvas id="architecture-vis" width="400" height="400"></canvas>
                        <p style="text-align: center; margin-top: 0.5rem; color: var(--text-secondary);">
                            网络架构可视化
                        </p>
                    </div>
                    <div>
                        <canvas id="performance-chart" width="400" height="400"></canvas>
                        <p style="text-align: center; margin-top: 0.5rem; color: var(--text-secondary);">
                            性能对比图表
                        </p>
                    </div>
                </div>

                <!-- 实时统计 -->
                <div style="margin-top: 2rem; display: grid; grid-template-columns: repeat(4, 1fr); gap: 1rem;">
                    <div style="background: rgba(139, 92, 246, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-purple); margin-bottom: 0.5rem;">参数量</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="param-count">0</p>
                    </div>
                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-green); margin-bottom: 0.5rem;">训练速度</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="train-speed">0ms</p>
                    </div>
                    <div style="background: rgba(6, 182, 212, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-blue); margin-bottom: 0.5rem;">准确率</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="accuracy">0%</p>
                    </div>
                    <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-yellow); margin-bottom: 0.5rem;">泛化能力</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="generalization">0%</p>
                    </div>
                </div>
            </div>

            <!-- 理论分析 -->
            <div class="math-derivation mt-4">
                <h3>表达效率的数学分析</h3>

                <div class="math-step" data-step="1">
                    <strong>函数复杂度</strong>
                    <p>考虑一个需要表达的函数族 F，其复杂度可以用VC维或Rademacher复杂度衡量</p>
                    <div class="math-explanation">
                        深度网络：O(L × W²) 参数可表达 O(W^L) 复杂度的函数<br>
                        浅层网络：O(W²) 参数只能表达 O(W²) 复杂度的函数<br>
                        <strong>结论：深度带来指数级的表达效率提升！</strong>
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>特征复用</strong>
                    <p>深度网络通过层级结构实现特征复用：</p>
                    <div style="text-align: center; margin: 1rem 0;">
                        <img src="data:image/svg+xml;base64,..." alt="特征层次图" style="max-width: 100%;">
                    </div>
                    <div class="math-explanation">
                        低层学习边缘 → 中层学习纹理 → 高层学习语义<br>
                        每层都在前一层的基础上构建更复杂的特征
                    </div>
                </div>
            </div>

            <!-- 实际例子 -->
            <div class="concept-grid mt-4">
                <div class="concept-card what">
                    <h3 style="color: var(--accent-green);">浅而宽</h3>
                    <p><strong>优点：</strong></p>
                    <ul style="font-size: 0.9rem;">
                        <li>并行度高</li>
                        <li>优化简单</li>
                        <li>理论保证</li>
                    </ul>
                    <p><strong>缺点：</strong></p>
                    <ul style="font-size: 0.9rem;">
                        <li>参数爆炸</li>
                        <li>泛化差</li>
                        <li>无特征层次</li>
                    </ul>
                </div>
                <div class="concept-card how">
                    <h3 style="color: var(--accent-blue);">深而窄</h3>
                    <p><strong>优点：</strong></p>
                    <ul style="font-size: 0.9rem;">
                        <li>参数效率高</li>
                        <li>特征层次化</li>
                        <li>泛化能力强</li>
                    </ul>
                    <p><strong>缺点：</strong></p>
                    <ul style="font-size: 0.9rem;">
                        <li>优化困难</li>
                        <li>梯度问题</li>
                        <li>训练技巧多</li>
                    </ul>
                </div>
            </div>

            <!-- 经验法则 -->
            <div class="tip info mt-4">
                <span class="tip-icon">📏</span>
                <div class="tip-content">
                    <strong>架构设计经验法则：</strong><br>
                    • <strong>计算机视觉</strong>：深度优先（CNN通常很深）<br>
                    • <strong>表格数据</strong>：适度深度（3-5层足够）<br>
                    • <strong>NLP任务</strong>：宽度也重要（Transformer很宽）<br>
                    • <strong>开始简单</strong>：先试试3层，根据需要加深
                </div>
            </div>
        </section>

        <!-- 代码实现：构建MLP -->
        <section id="implementation" class="section-card">
            <h2 style="color: var(--accent-blue); margin-bottom: 1.5rem;">
                💻 代码实现：构建灵活的MLP
                <span class="depth-indicator intermediate">进阶</span>
            </h2>

            <!-- 实现说明 -->
            <div class="dialogue-container">
                <div class="dialogue-item">
                    <div class="dialogue-avatar teacher">👨‍🏫</div>
                    <div class="dialogue-content">
                        <div class="dialogue-name">王教授</div>
                        <p>现在让我们实现一个灵活的MLP类。我会把代码分成小块，
                            每一部分都详细解释，确保你理解每行代码的作用。</p>
                    </div>
                </div>
            </div>

            <div class="code-container">
                <div class="code-header">
                    <div class="code-header-left">
                        <button class="code-tab active" data-lang="pytorch" tabindex="0">PyTorch版本</button>
                        <button class="code-tab" data-lang="numpy" tabindex="0">NumPy版本</button>
                    </div>
                    <div class="code-header-right">
                        <button class="code-action" onclick="copyCode()" tabindex="0">
                            📋 复制
                        </button>
                    </div>
                </div>
                <div class="code-content" id="pytorch-code">
<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ========== 第1部分：基础MLP类 ==========
class MLP(nn.Module):
    """
    灵活的多层感知机实现

    这个类允许你：
    1. 自定义层数和每层的宽度
    2. 选择不同的激活函数
    3. 添加正则化（Dropout, BatchNorm）
    4. 灵活的初始化策略
    """

    def __init__(self,
                 input_size,
                 hidden_sizes,
                 output_size,
                 activation='relu',
                 dropout_rate=0.0,
                 use_batch_norm=False):
        """
        参数说明：
        input_size: 输入特征维度
        hidden_sizes: 列表，每个元素是对应隐藏层的神经元数
        output_size: 输出维度
        activation: 激活函数类型
        dropout_rate: Dropout概率（0表示不使用）
        use_batch_norm: 是否使用批归一化
        """
        super(MLP, self).__init__()

        # 保存配置
        self.input_size = input_size
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.activation_name = activation
        self.dropout_rate = dropout_rate
        self.use_batch_norm = use_batch_norm

        # 构建网络层
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        self.dropouts = nn.ModuleList()

        # 逐层构建
        prev_size = input_size
        for hidden_size in hidden_sizes:
            # 添加线性层
            self.layers.append(nn.Linear(prev_size, hidden_size))

            # 添加批归一化（如果需要）
            if use_batch_norm:
                self.batch_norms.append(nn.BatchNorm1d(hidden_size))

            # 添加Dropout（如果需要）
            if dropout_rate > 0:
                self.dropouts.append(nn.Dropout(dropout_rate))

            prev_size = hidden_size

        # 输出层
        self.output_layer = nn.Linear(prev_size, output_size)

        # 设置激活函数
        self.activation = self._get_activation(activation)

        # 初始化权重
        self._initialize_weights()

    def _get_activation(self, activation_name):
        """获取激活函数"""
        activations = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'leaky_relu': nn.LeakyReLU(0.01),
            'elu': nn.ELU(),
            'gelu': nn.GELU(),
            'swish': nn.SiLU()  # SiLU就是Swish
        }
        return activations.get(activation_name, nn.ReLU())

    def _initialize_weights(self):
        """权重初始化策略"""
        for layer in self.layers:
            if self.activation_name in ['relu', 'leaky_relu']:
                # He初始化（适合ReLU系列）
                nn.init.kaiming_normal_(layer.weight, mode='fan_in',
                                      nonlinearity='relu')
            else:
                # Xavier初始化（适合Sigmoid/Tanh）
                nn.init.xavier_normal_(layer.weight)

            # 偏置初始化为0
            nn.init.constant_(layer.bias, 0)

        # 输出层使用Xavier初始化
        nn.init.xavier_normal_(self.output_layer.weight)
        nn.init.constant_(self.output_layer.bias, 0)

    def forward(self, x):
        """前向传播"""
        # 通过隐藏层
        for i, layer in enumerate(self.layers):
            x = layer(x)

            # 批归一化
            if self.use_batch_norm and i < len(self.batch_norms):
                x = self.batch_norms[i](x)

            # 激活函数
            x = self.activation(x)

            # Dropout
            if self.dropout_rate > 0 and i < len(self.dropouts):
                x = self.dropouts[i](x)

        # 输出层（通常不加激活函数）
        x = self.output_layer(x)

        return x


# ========== 第2部分：带跳跃连接的MLP ==========
class ResidualMLP(nn.Module):
    """
    带残差连接的MLP

    残差连接可以：
    1. 缓解梯度消失
    2. 让网络更容易训练
    3. 允许构建更深的网络
    """

    def __init__(self,
                 input_size,
                 hidden_size,
                 num_blocks,
                 output_size,
                 activation='relu'):
        super(ResidualMLP, self).__init__()

        # 输入投影层
        self.input_projection = nn.Linear(input_size, hidden_size)

        # 残差块
        self.blocks = nn.ModuleList()
        for _ in range(num_blocks):
            block = ResidualBlock(hidden_size, activation)
            self.blocks.append(block)

        # 输出层
        self.output_layer = nn.Linear(hidden_size, output_size)

        # 激活函数
        self.activation = self._get_activation(activation)

    def _get_activation(self, name):
        """获取激活函数（与上面相同）"""
        activations = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'leaky_relu': nn.LeakyReLU(0.01),
            'gelu': nn.GELU()
        }
        return activations.get(name, nn.ReLU())

    def forward(self, x):
        # 输入投影
        x = self.activation(self.input_projection(x))

        # 通过残差块
        for block in self.blocks:
            x = block(x)

        # 输出
        x = self.output_layer(x)

        return x


class ResidualBlock(nn.Module):
    """残差块"""

    def __init__(self, hidden_size, activation='relu'):
        super(ResidualBlock, self).__init__()

        self.fc1 = nn.Linear(hidden_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        # 激活函数
        self.activation = self._get_activation(activation)

        # 层归一化（比批归一化更稳定）
        self.ln1 = nn.LayerNorm(hidden_size)
        self.ln2 = nn.LayerNorm(hidden_size)

    def _get_activation(self, name):
        activations = {
            'relu': nn.ReLU(),
            'gelu': nn.GELU(),
            'leaky_relu': nn.LeakyReLU(0.01)
        }
        return activations.get(name, nn.ReLU())

    def forward(self, x):
        # 保存输入用于残差连接
        residual = x

        # 第一层
        x = self.ln1(x)
        x = self.activation(self.fc1(x))

        # 第二层
        x = self.ln2(x)
        x = self.fc2(x)

        # 残差连接
        x = x + residual

        return x


# ========== 第3部分：实用工具函数 ==========
def count_parameters(model):
    """统计模型参数量"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def create_mlp_from_config(config):
    """根据配置字典创建MLP"""
    return MLP(
        input_size=config['input_size'],
        hidden_sizes=config['hidden_sizes'],
        output_size=config['output_size'],
        activation=config.get('activation', 'relu'),
        dropout_rate=config.get('dropout_rate', 0.0),
        use_batch_norm=config.get('use_batch_norm', False)
    )


# ========== 第4部分：训练辅助类 ==========
class MLPTrainer:
    """MLP训练器"""

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.train_history = []
        self.val_history = []

    def train_epoch(self, dataloader, optimizer, criterion):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0

        for batch_idx, (inputs, targets) in enumerate(dataloader):
            inputs, targets = inputs.to(self.device), targets.to(self.device)

            # 前向传播
            outputs = self.model(inputs)
            loss = criterion(outputs, targets)

            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # 统计
            total_loss += loss.item()

            # 如果是分类任务
            if outputs.shape[1] > 1:
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total if total > 0 else 0

        return avg_loss, accuracy

    def evaluate(self, dataloader, criterion):
        """评估模型"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in dataloader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                outputs = self.model(inputs)
                loss = criterion(outputs, targets)

                total_loss += loss.item()

                if outputs.shape[1] > 1:
                    _, predicted = outputs.max(1)
                    total += targets.size(0)
                    correct += predicted.eq(targets).sum().item()

        avg_loss = total_loss / len(dataloader)
        accuracy = 100. * correct / total if total > 0 else 0

        return avg_loss, accuracy

    def fit(self,
            train_loader,
            val_loader,
            epochs,
            optimizer,
            criterion,
            scheduler=None,
            early_stopping_patience=10):
        """完整的训练流程"""

        best_val_loss = float('inf')
        patience_counter = 0

        for epoch in range(epochs):
            # 训练
            train_loss, train_acc = self.train_epoch(
                train_loader, optimizer, criterion
            )

            # 验证
            val_loss, val_acc = self.evaluate(val_loader, criterion)

            # 记录历史
            self.train_history.append({
                'epoch': epoch,
                'loss': train_loss,
                'accuracy': train_acc
            })
            self.val_history.append({
                'epoch': epoch,
                'loss': val_loss,
                'accuracy': val_acc
            })

            # 学习率调度
            if scheduler is not None:
                scheduler.step(val_loss)

            # 早停
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # 保存最佳模型
                self.best_model_state = self.model.state_dict()
            else:
                patience_counter += 1

            if patience_counter >= early_stopping_patience:
                print(f"早停：验证损失{early_stopping_patience}轮未改善")
                break

            # 打印进度
            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{epochs}:')
                print(f'  训练 - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%')
                print(f'  验证 - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')

    def plot_history(self):
        """绘制训练历史"""
        import matplotlib.pyplot as plt

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

        # 损失曲线
        epochs = [h['epoch'] for h in self.train_history]
        train_losses = [h['loss'] for h in self.train_history]
        val_losses = [h['loss'] for h in self.val_history]

        ax1.plot(epochs, train_losses, label='训练损失')
        ax1.plot(epochs, val_losses, label='验证损失')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('训练过程中的损失变化')
        ax1.legend()
        ax1.grid(True)

        # 准确率曲线
        train_accs = [h['accuracy'] for h in self.train_history]
        val_accs = [h['accuracy'] for h in self.val_history]

        ax2.plot(epochs, train_accs, label='训练准确率')
        ax2.plot(epochs, val_accs, label='验证准确率')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('训练过程中的准确率变化')
        ax2.legend()
        ax2.grid(True)

        plt.tight_layout()
        plt.show()


# ========== 第5部分：使用示例 ==========
if __name__ == "__main__":
    # 创建一个简单的MLP
    model = MLP(
        input_size=784,  # MNIST图像尺寸
        hidden_sizes=[128, 64, 32],  # 3个隐藏层
        output_size=10,  # 10个类别
        activation='relu',
        dropout_rate=0.2,
        use_batch_norm=True
    )

    print(f"模型结构：\n{model}")
    print(f"\n参数总量：{count_parameters(model):,}")

    # 测试前向传播
    x = torch.randn(32, 784)  # 批大小32
    output = model(x)
    print(f"\n输出形状：{output.shape}")

    # 创建残差MLP
    res_model = ResidualMLP(
        input_size=784,
        hidden_size=256,
        num_blocks=5,
        output_size=10,
        activation='gelu'
    )

    print(f"\n残差模型参数量：{count_parameters(res_model):,}")</code></pre>
                </div>
                <div class="code-content" id="numpy-code" style="display: none;">
<pre><code class="python">import numpy as np

# ========== 第1部分：基础MLP实现 ==========
class MLPNumPy:
    """
    使用NumPy从零实现的MLP

    适合理解底层原理，不适合实际应用
    """

    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):
        """
        参数：
        layer_sizes: 列表，包含每层的神经元数
                    例如 [784, 128, 64, 10] 表示输入784，两个隐藏层，输出10
        activation: 激活函数类型
        learning_rate: 学习率
        """
        self.layer_sizes = layer_sizes
        self.num_layers = len(layer_sizes)
        self.activation_name = activation
        self.learning_rate = learning_rate

        # 初始化权重和偏置
        self.weights = []
        self.biases = []

        for i in range(self.num_layers - 1):
            # He初始化（适合ReLU）
            if activation in ['relu', 'leaky_relu']:
                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * \
                    np.sqrt(2.0 / layer_sizes[i])
            else:
                # Xavier初始化（适合Sigmoid/Tanh）
                w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * \
                    np.sqrt(1.0 / layer_sizes[i])

            b = np.zeros((1, layer_sizes[i+1]))

            self.weights.append(w)
            self.biases.append(b)

        # 设置激活函数
        self._set_activation_functions()

    def _set_activation_functions(self):
        """设置激活函数及其导数"""
        if self.activation_name == 'relu':
            self.activation = lambda x: np.maximum(0, x)
            self.activation_derivative = lambda x: (x > 0).astype(float)

        elif self.activation_name == 'sigmoid':
            self.activation = lambda x: 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            self.activation_derivative = lambda x: x * (1 - x)  # 注意：x是激活后的值

        elif self.activation_name == 'tanh':
            self.activation = lambda x: np.tanh(x)
            self.activation_derivative = lambda x: 1 - x ** 2  # 注意：x是激活后的值

        elif self.activation_name == 'leaky_relu':
            self.activation = lambda x: np.where(x > 0, x, 0.01 * x)
            self.activation_derivative = lambda x: np.where(x > 0, 1, 0.01)

        else:
            raise ValueError(f"未知的激活函数: {self.activation_name}")

    def forward(self, X):
        """
        前向传播

        参数：
        X: 输入数据，形状 (batch_size, input_size)

        返回：
        输出值，形状 (batch_size, output_size)
        """
        self.activations = [X]
        self.z_values = []

        # 逐层计算
        for i in range(self.num_layers - 1):
            # 线性变换：z = Wx + b
            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]
            self.z_values.append(z)

            # 激活函数（最后一层不激活）
            if i < self.num_layers - 2:
                a = self.activation(z)
            else:
                a = z  # 输出层保持线性

            self.activations.append(a)

        return self.activations[-1]

    def backward(self, X, y):
        """
        反向传播

        参数：
        X: 输入数据
        y: 真实标签

        返回：
        损失值
        """
        batch_size = X.shape[0]

        # 前向传播
        output = self.forward(X)

        # 计算损失（均方误差）
        loss = 0.5 * np.mean((output - y) ** 2)

        # 初始化梯度
        self.weight_gradients = []
        self.bias_gradients = []

        # 输出层的误差
        delta = (output - y) / batch_size

        # 反向传播误差
        for i in range(self.num_layers - 2, -1, -1):
            # 计算权重和偏置的梯度
            weight_grad = np.dot(self.activations[i].T, delta)
            bias_grad = np.sum(delta, axis=0, keepdims=True)

            self.weight_gradients.insert(0, weight_grad)
            self.bias_gradients.insert(0, bias_grad)

            # 传播到前一层
            if i > 0:
                delta = np.dot(delta, self.weights[i].T)
                # 乘以激活函数的导数
                if self.activation_name in ['sigmoid', 'tanh']:
                    # 这些函数的导数需要激活后的值
                    delta *= self.activation_derivative(self.activations[i])
                else:
                    # ReLU等函数的导数需要激活前的值
                    delta *= self.activation_derivative(self.z_values[i-1])

        return loss

    def update_weights(self):
        """使用梯度下降更新权重"""
        for i in range(self.num_layers - 1):
            self.weights[i] -= self.learning_rate * self.weight_gradients[i]
            self.biases[i] -= self.learning_rate * self.bias_gradients[i]

    def train_step(self, X, y):
        """单步训练"""
        loss = self.backward(X, y)
        self.update_weights()
        return loss

    def predict(self, X):
        """预测"""
        return self.forward(X)

    def fit(self, X_train, y_train, epochs=100, batch_size=32, verbose=True):
        """
        训练模型

        参数：
        X_train: 训练数据
        y_train: 训练标签
        epochs: 训练轮数
        batch_size: 批大小
        verbose: 是否打印进度
        """
        n_samples = X_train.shape[0]
        history = {'loss': []}

        for epoch in range(epochs):
            # 打乱数据
            indices = np.random.permutation(n_samples)
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]

            epoch_loss = 0
            n_batches = 0

            # 小批量训练
            for i in range(0, n_samples, batch_size):
                batch_end = min(i + batch_size, n_samples)
                X_batch = X_shuffled[i:batch_end]
                y_batch = y_shuffled[i:batch_end]

                loss = self.train_step(X_batch, y_batch)
                epoch_loss += loss
                n_batches += 1

            avg_loss = epoch_loss / n_batches
            history['loss'].append(avg_loss)

            if verbose and (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        return history


# ========== 第2部分：带动量的优化器 ==========
class MLPWithMomentum(MLPNumPy):
    """带动量的MLP（继承自基础MLP）"""

    def __init__(self, layer_sizes, activation='relu',
                 learning_rate=0.01, momentum=0.9):
        super().__init__(layer_sizes, activation, learning_rate)
        self.momentum = momentum

        # 初始化速度项
        self.weight_velocities = []
        self.bias_velocities = []

        for i in range(self.num_layers - 1):
            w_velocity = np.zeros_like(self.weights[i])
            b_velocity = np.zeros_like(self.biases[i])

            self.weight_velocities.append(w_velocity)
            self.bias_velocities.append(b_velocity)

    def update_weights(self):
        """使用动量更新权重"""
        for i in range(self.num_layers - 1):
            # 更新速度
            self.weight_velocities[i] = (self.momentum * self.weight_velocities[i] -
                                       self.learning_rate * self.weight_gradients[i])
            self.bias_velocities[i] = (self.momentum * self.bias_velocities[i] -
                                     self.learning_rate * self.bias_gradients[i])

            # 更新参数
            self.weights[i] += self.weight_velocities[i]
            self.biases[i] += self.bias_velocities[i]


# ========== 第3部分：实用函数 ==========
def create_spiral_dataset(n_points=100, n_classes=3, noise=0.2):
    """
    创建螺旋数据集

    这是一个经典的非线性可分数据集
    """
    X = []
    y = []

    for class_idx in range(n_classes):
        # 每个类的点数
        n_per_class = n_points // n_classes

        # 生成螺旋
        theta = np.linspace(class_idx * 4, (class_idx + 1) * 4, n_per_class)
        theta += np.random.randn(n_per_class) * noise

        r = np.linspace(0.5, 1, n_per_class)

        x1 = r * np.sin(theta)
        x2 = r * np.cos(theta)

        X.append(np.column_stack([x1, x2]))
        y.extend([class_idx] * n_per_class)

    X = np.vstack(X)
    y = np.array(y)

    # 转换为one-hot编码
    y_onehot = np.zeros((len(y), n_classes))
    y_onehot[np.arange(len(y)), y] = 1

    return X, y_onehot, y


def plot_decision_boundary(model, X, y, title="决策边界"):
    """绘制决策边界"""
    import matplotlib.pyplot as plt

    # 设置网格
    h = 0.02
    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # 预测网格上的点
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid_points)

    # 如果是多分类，取最大值的索引
    if Z.shape[1] > 1:
        Z = np.argmax(Z, axis=1)
    else:
        Z = (Z > 0.5).astype(int)

    Z = Z.reshape(xx.shape)

    # 绘图
    plt.figure(figsize=(10, 8))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)

    # 绘制数据点
    if y.ndim > 1:
        y_labels = np.argmax(y, axis=1)
    else:
        y_labels = y

    scatter = plt.scatter(X[:, 0], X[:, 1], c=y_labels,
                         cmap=plt.cm.RdYlBu, edgecolor='black')

    plt.xlabel('特征 1')
    plt.ylabel('特征 2')
    plt.title(title)
    plt.colorbar(scatter)
    plt.show()


# ========== 第4部分：测试代码 ==========
if __name__ == "__main__":
    print("=== NumPy MLP 实现测试 ===\n")

    # 1. 创建数据集
    print("1. 生成螺旋数据集...")
    X, y_onehot, y_labels = create_spiral_dataset(n_points=300, n_classes=3)
    print(f"   数据形状: X={X.shape}, y={y_onehot.shape}")

    # 2. 创建和训练基础MLP
    print("\n2. 训练基础MLP...")
    mlp = MLPNumPy(
        layer_sizes=[2, 100, 50, 3],  # 输入2，两个隐藏层，输出3
        activation='relu',
        learning_rate=0.1
    )

    history = mlp.fit(X, y_onehot, epochs=200, batch_size=32, verbose=True)

    # 3. 评估模型
    print("\n3. 评估模型...")
    predictions = mlp.predict(X)
    predicted_classes = np.argmax(predictions, axis=1)
    accuracy = np.mean(predicted_classes == y_labels)
    print(f"   训练准确率: {accuracy:.2%}")

    # 4. 绘制决策边界
    print("\n4. 绘制决策边界...")
    plot_decision_boundary(mlp, X, y_onehot, "MLP决策边界")

    # 5. 测试带动量的MLP
    print("\n5. 训练带动量的MLP...")
    mlp_momentum = MLPWithMomentum(
        layer_sizes=[2, 100, 50, 3],
        activation='relu',
        learning_rate=0.1,
        momentum=0.9
    )

    history_momentum = mlp_momentum.fit(X, y_onehot, epochs=200,
                                       batch_size=32, verbose=False)

    # 比较训练曲线
    import matplotlib.pyplot as plt

    plt.figure(figsize=(10, 5))
    plt.plot(history['loss'], label='基础SGD', linewidth=2)
    plt.plot(history_momentum['loss'], label='带动量SGD', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('训练损失对比')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

    print("\n测试完成！")</code></pre>
                </div>
                <div class="code-output" id="code-output">
                    <strong>运行结果：</strong>
                    <pre>=== NumPy MLP 实现测试 ===

1. 生成螺旋数据集...
   数据形状: X=(300, 2), y=(300, 3)

2. 训练基础MLP...
Epoch 10/200, Loss: 0.3842
Epoch 20/200, Loss: 0.1523
...
Epoch 200/200, Loss: 0.0234

3. 评估模型...
   训练准确率: 98.7%

4. 绘制决策边界...
[图表显示]

5. 训练带动量的MLP...
测试完成！</pre>
                </div>
            </div>

            <!-- 代码详解 -->
            <div class="tip info mt-3">
                <span class="tip-icon">📝</span>
                <div class="tip-content">
                    <strong>代码要点解释：</strong><br>
                    • <strong>模块化设计</strong>：每个功能都是独立的类或函数<br>
                    • <strong>权重初始化</strong>：He初始化用于ReLU，Xavier用于Sigmoid/Tanh<br>
                    • <strong>正则化技术</strong>：Dropout防止过拟合，BatchNorm加速训练<br>
                    • <strong>残差连接</strong>：允许构建更深的网络<br>
                    • <strong>训练技巧</strong>：早停、学习率调度等
                </div>
            </div>
        </section>

        <!-- 决策边界可视化 -->
        <section id="visualization" class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">
                🎨 决策边界可视化
                <span class="depth-indicator intermediate">进阶</span>
            </h2>

            <div class="interactive-demo">
                <h3 style="color: var(--accent-green); margin-bottom: 1.5rem;">🔍 探索不同架构的决策边界</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>数据集</label>
                        <select id="dataset-select" class="form-select">
                            <option value="moons">月牙形</option>
                            <option value="circles">同心圆</option>
                            <option value="spiral">螺旋线</option>
                            <option value="xor">XOR</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label>网络架构</label>
                        <select id="architecture-select" class="form-select">
                            <option value="shallow">[2, 10, 2]</option>
                            <option value="medium">[2, 50, 25, 2]</option>
                            <option value="deep">[2, 20, 20, 20, 20, 2]</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label>激活函数</label>
                        <select id="activation-select" class="form-select">
                            <option value="relu">ReLU</option>
                            <option value="tanh">Tanh</option>
                            <option value="sigmoid">Sigmoid</option>
                        </select>
                    </div>
                    <button class="btn btn-primary" onclick="trainAndVisualize()" tabindex="0">
                        🚀 训练并可视化
                    </button>
                </div>

                <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem; margin-top: 2rem;">
                    <div>
                        <canvas id="data-canvas" width="300" height="300"></canvas>
                        <p style="text-align: center; margin-top: 0.5rem;">原始数据</p>
                    </div>
                    <div>
                        <canvas id="boundary-canvas" width="300" height="300"></canvas>
                        <p style="text-align: center; margin-top: 0.5rem;">决策边界</p>
                    </div>
                    <div>
                        <canvas id="confidence-canvas" width="300" height="300"></canvas>
                        <p style="text-align: center; margin-top: 0.5rem;">置信度图</p>
                    </div>
                </div>

                <div style="margin-top: 1rem;">
                    <div style="background: rgba(0, 0, 0, 0.2); padding: 1rem; border-radius: 0.5rem;">
                        <p>训练进度: <span id="training-progress">0%</span></p>
                        <p>最终准确率: <span id="final-accuracy">-</span></p>
                        <p>训练时间: <span id="training-time">-</span></p>
                    </div>
                </div>
            </div>

            <!-- 可视化见解 -->
            <div class="concept-grid mt-4">
                <div class="concept-card what">
                    <h3>线性决策边界</h3>
                    <p>单层网络只能产生直线边界</p>
                </div>
                <div class="concept-card how">
                    <h3>分段线性</h3>
                    <p>ReLU网络产生分段线性边界</p>
                </div>
                <div class="concept-card why">
                    <h3>平滑曲线</h3>
                    <p>Sigmoid/Tanh产生平滑边界</p>
                </div>
                <div class="concept-card pitfall">
                    <h3>过拟合风险</h3>
                    <p>复杂边界可能过拟合训练数据</p>
                </div>
            </div>
        </section>

        <!-- 实战：表格vs视觉任务 -->
        <section id="practice" class="section-card">
            <h2 style="color: var(--accent-red); margin-bottom: 1.5rem;">
                🏃 实战：不同任务的MLP设计
                <span class="depth-indicator intermediate">进阶</span>
            </h2>

            <div class="story-card">
                <h3>📊 任务1：表格数据分类</h3>
                <p>预测银行客户是否会流失（二分类任务）</p>

                <div class="code-container mt-3">
                    <div class="code-header">
                        <div class="code-header-left">
                            <span>表格数据的MLP设计</span>
                        </div>
                    </div>
                    <div class="code-content">
<pre><code class="python"># 表格数据的特点：
# 1. 特征已经是高级语义（年龄、收入等）
# 2. 特征数量通常不多（几十到几百）
# 3. 样本量可能有限

class TabularMLP(nn.Module):
    """专门用于表格数据的MLP"""

    def __init__(self,
                 num_features,
                 num_classes,
                 hidden_dims=[256, 128, 64],
                 dropout_rates=[0.3, 0.2, 0.1]):
        super().__init__()

        layers = []
        prev_dim = num_features

        # 构建隐藏层
        for i, (hidden_dim, dropout_rate) in enumerate(
            zip(hidden_dims, dropout_rates)
        ):
            # 线性层
            layers.append(nn.Linear(prev_dim, hidden_dim))

            # 批归一化（表格数据很有效）
            layers.append(nn.BatchNorm1d(hidden_dim))

            # 激活函数（LeakyReLU避免死神经元）
            layers.append(nn.LeakyReLU(0.1))

            # Dropout（防止过拟合）
            layers.append(nn.Dropout(dropout_rate))

            prev_dim = hidden_dim

        # 输出层
        layers.append(nn.Linear(prev_dim, num_classes))

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


# 使用示例：客户流失预测
# 特征：年龄、账户余额、产品数量、信用分数等
model = TabularMLP(
    num_features=15,      # 15个特征
    num_classes=2,        # 二分类
    hidden_dims=[128, 64, 32],  # 适度深度
    dropout_rates=[0.4, 0.3, 0.2]  # 较高dropout
)

# 训练技巧：
# 1. 特征工程很重要（标准化、编码等）
# 2. 使用交叉验证选择超参数
# 3. 集成多个模型效果更好</code></pre>
                    </div>
                </div>
            </div>

            <div class="story-card mt-4">
                <h3>🖼️ 任务2：图像分类</h3>
                <p>MNIST手写数字识别（多分类任务）</p>

                <div class="code-container mt-3">
                    <div class="code-header">
                        <div class="code-header-left">
                            <span>图像数据的MLP设计</span>
                        </div>
                    </div>
                    <div class="code-content">
<pre><code class="python"># 图像数据的特点：
# 1. 高维输入（像素级特征）
# 2. 空间结构重要（相邻像素相关）
# 3. 平移不变性需求

class ImageMLP(nn.Module):
    """用于图像的MLP（仅作演示，实际应用CNN更好）"""

    def __init__(self,
                 image_size=(28, 28),
                 num_classes=10):
        super().__init__()

        input_dim = image_size[0] * image_size[1]

        # 更深的网络，逐步降维
        self.flatten = nn.Flatten()

        # 特征提取部分（模拟CNN的效果）
        self.feature_extractor = nn.Sequential(
            # 第一阶段：保持高维
            nn.Linear(input_dim, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(),
            nn.Dropout(0.2),

            # 第二阶段：逐步压缩
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.2),

            # 第三阶段：进一步压缩
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2),

            # 第四阶段：抽象特征
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # 分类头
        self.classifier = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.flatten(x)
        features = self.feature_extractor(x)
        output = self.classifier(features)
        return output

    def get_features(self, x):
        """提取特征（用于可视化）"""
        x = self.flatten(x)
        return self.feature_extractor(x)


# 数据增强（对MLP也有帮助）
class MLPAugmentation:
    """MLP友好的数据增强"""

    @staticmethod
    def add_noise(x, noise_level=0.1):
        """添加高斯噪声"""
        noise = torch.randn_like(x) * noise_level
        return x + noise

    @staticmethod
    def random_dropout(x, drop_prob=0.1):
        """随机丢弃像素"""
        mask = torch.rand_like(x) > drop_prob
        return x * mask


# 对比实验：MLP vs CNN
def compare_mlp_cnn():
    """对比MLP和CNN在图像任务上的表现"""

    # MLP模型
    mlp = ImageMLP(image_size=(28, 28), num_classes=10)

    # 简单CNN（作为对比）
    class SimpleCNN(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.pool = nn.MaxPool2d(2)
            self.fc1 = nn.Linear(64 * 7 * 7, 128)
            self.fc2 = nn.Linear(128, 10)

        def forward(self, x):
            x = self.pool(F.relu(self.conv1(x)))
            x = self.pool(F.relu(self.conv2(x)))
            x = x.view(-1, 64 * 7 * 7)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x

    cnn = SimpleCNN()

    # 参数量对比
    mlp_params = sum(p.numel() for p in mlp.parameters())
    cnn_params = sum(p.numel() for p in cnn.parameters())

    print(f"MLP参数量: {mlp_params:,}")
    print(f"CNN参数量: {cnn_params:,}")
    print(f"参数比例: {mlp_params/cnn_params:.2f}x")

    return mlp, cnn</code></pre>
                    </div>
                </div>
            </div>

            <!-- 任务对比总结 -->
            <div style="margin-top: 2rem; background: rgba(139, 92, 246, 0.1); padding: 2rem; border-radius: 1rem;">
                <h3 style="color: var(--accent-purple); margin-bottom: 1rem;">📊 不同任务的MLP设计原则</h3>

                <table style="width: 100%; border-collapse: collapse;">
                    <thead>
                    <tr style="background: rgba(139, 92, 246, 0.2);">
                        <th style="padding: 1rem; text-align: left;">任务类型</th>
                        <th style="padding: 1rem; text-align: left;">推荐架构</th>
                        <th style="padding: 1rem; text-align: left;">关键技巧</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            <strong>表格数据</strong>
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            浅而宽（2-5层）
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            特征工程、集成学习
                        </td>
                    </tr>
                    <tr>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            <strong>图像（小）</strong>
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            深而窄（5-10层）
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            数据增强、正则化
                        </td>
                    </tr>
                    <tr>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            <strong>时序数据</strong>
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            中等深度+跳跃连接
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            滑动窗口、特征提取
                        </td>
                    </tr>
                    <tr>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            <strong>高维稀疏</strong>
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            嵌入层+浅层MLP
                        </td>
                        <td style="padding: 1rem; border-top: 1px solid var(--border-color);">
                            降维、特征选择
                        </td>
                    </tr>
                    </tbody>
                </table>
            </div>
        </section>

        <!-- 激活饱和问题 -->
        <section id="saturation" class="section-card">
            <h2 style="color: var(--accent-yellow); margin-bottom: 1.5rem;">
                ⚡ 激活饱和与梯度消失
                <span class="depth-indicator advanced">深入</span>
            </h2>

            <!-- 问题演示 -->
            <div class="interactive-demo">
                <h3 style="color: var(--accent-red); margin-bottom: 1.5rem;">🔥 梯度消失实验</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>网络深度</label>
                        <div class="slider-container">
                            <input type="range" id="depth-slider" min="5" max="50" step="5" value="20">
                            <span class="slider-value" id="depth-value">20层</span>
                        </div>
                    </div>
                    <div class="control-group">
                        <label>激活函数</label>
                        <select id="gradient-activation" class="form-select">
                            <option value="sigmoid">Sigmoid</option>
                            <option value="tanh">Tanh</option>
                            <option value="relu">ReLU</option>
                            <option value="leaky_relu">Leaky ReLU</option>
                        </select>
                    </div>
                    <button class="btn btn-primary" onclick="analyzeGradients()" tabindex="0">
                        📊 分析梯度流
                    </button>
                </div>

                <canvas id="gradient-flow-canvas" width="800" height="400"></canvas>

                <div style="margin-top: 1rem; display: grid; grid-template-columns: repeat(3, 1fr); gap: 1rem;">
                    <div style="background: rgba(239, 68, 68, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-red);">第1层梯度</h4>
                        <p style="font-size: 1.5rem;" id="first-layer-gradient">0.0000</p>
                    </div>
                    <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-yellow);">中间层梯度</h4>
                        <p style="font-size: 1.5rem;" id="middle-layer-gradient">0.0000</p>
                    </div>
                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 0.5rem; text-align: center;">
                        <h4 style="color: var(--accent-green);">最后层梯度</h4>
                        <p style="font-size: 1.5rem;" id="last-layer-gradient">0.0000</p>
                    </div>
                </div>
            </div>

            <!-- 解决方案 -->
            <div class="math-derivation mt-4">
                <h3>解决梯度问题的技术</h3>

                <div class="math-step" data-step="1">
                    <strong>批归一化（Batch Normalization）</strong>
                    <div class="equation-container">
                        <span>x̂ = (x - μ) / √(σ² + ε)</span><br>
                        <span>y = γx̂ + β</span>
                    </div>
                    <div class="math-explanation">
                        • 标准化激活值，防止饱和<br>
                        • 加速训练，允许更高学习率<br>
                        • 有轻微正则化效果
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>残差连接（Residual Connection）</strong>
                    <div class="equation-container">
                        <span>y = F(x) + x</span>
                    </div>
                    <div class="math-explanation">
                        • 梯度可以直接流过跳跃连接<br>
                        • 让网络学习残差而非完整映射<br>
                        • ResNet的核心思想
                    </div>
                </div>

                <div class="math-step" data-step="3">
                    <strong>梯度裁剪（Gradient Clipping）</strong>
                    <div class="equation-container">
                        <span>g = min(1, θ/||g||) · g</span>
                    </div>
                    <div class="math-explanation">
                        • 防止梯度爆炸<br>
                        • 常用于RNN训练<br>
                        • θ是裁剪阈值
                    </div>
                </div>
            </div>

            <!-- 实践代码 -->
            <div class="code-container mt-4">
                <div class="code-header">
                    <div class="code-header-left">
                        <span>解决梯度问题的实践</span>
                    </div>
                </div>
                <div class="code-content">
<pre><code class="python">import torch
import torch.nn as nn

class ImprovedMLP(nn.Module):
    """使用现代技术改进的MLP"""

    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()

        self.input_layer = nn.Linear(input_size, hidden_size)

        # 使用ModuleList存储层
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        for _ in range(num_layers):
            self.layers.append(nn.Linear(hidden_size, hidden_size))
            self.batch_norms.append(nn.BatchNorm1d(hidden_size))

        self.output_layer = nn.Linear(hidden_size, output_size)

        # 使用更好的激活函数
        self.activation = nn.GELU()

        # 初始化
        self._init_weights()

    def _init_weights(self):
        """更好的初始化策略"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # 使用正态分布，标准差根据fan_in调整
                nn.init.normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        # 输入层
        x = self.activation(self.input_layer(x))

        # 隐藏层with残差连接
        for i, (layer, bn) in enumerate(zip(self.layers, self.batch_norms)):
            # 保存输入用于残差
            residual = x

            # 前向传播
            x = layer(x)
            x = bn(x)
            x = self.activation(x)

            # 残差连接（每2层添加一次）
            if i % 2 == 1:
                x = x + residual

        # 输出层
        x = self.output_layer(x)

        return x


def train_with_gradient_monitoring(model, train_loader, epochs=10):
    """训练时监控梯度"""

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    gradient_norms = {name: [] for name, _ in model.named_parameters()}

    for epoch in range(epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad()

            output = model(data)
            loss = criterion(output, target)
            loss.backward()

            # 记录梯度范数
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    gradient_norms[name].append(grad_norm)

            # 梯度裁剪
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            if batch_idx % 100 == 0:
                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')

    return gradient_norms


# 可视化梯度流
def visualize_gradient_flow(gradient_norms):
    """可视化训练过程中的梯度流"""
    import matplotlib.pyplot as plt

    fig, ax = plt.subplots(figsize=(12, 6))

    # 选择几个关键层
    layers_to_plot = []
    for name in gradient_norms.keys():
        if 'weight' in name and len(gradient_norms[name]) > 0:
            layers_to_plot.append(name)

    # 绘制每层的梯度范数变化
    for i, layer_name in enumerate(layers_to_plot[::2]):  # 每隔一层绘制
        values = gradient_norms[layer_name]
        ax.plot(values, label=f'Layer {i}', alpha=0.7)

    ax.set_xlabel('Iteration')
    ax.set_ylabel('Gradient Norm')
    ax.set_title('梯度范数在训练过程中的变化')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 使用对数尺度
    ax.set_yscale('log')

    plt.tight_layout()
    plt.show()


# 对比实验：有无改进技术
def compare_techniques():
    """对比不同技术的效果"""

    input_size = 784
    hidden_size = 256
    num_layers = 20
    output_size = 10

    # 基础MLP（容易梯度消失）
    basic_mlp = nn.Sequential(*[
        nn.Linear(input_size if i == 0 else hidden_size,
                 hidden_size if i < num_layers else output_size),
        nn.Sigmoid() if i < num_layers else nn.Identity()
        for i in range(num_layers + 1)
    ])

    # 改进的MLP
    improved_mlp = ImprovedMLP(input_size, hidden_size, num_layers, output_size)

    # 生成随机数据
    x = torch.randn(32, input_size)

    # 测试梯度流
    for name, model in [("Basic MLP", basic_mlp), ("Improved MLP", improved_mlp)]:
        y = model(x)
        loss = y.sum()
        loss.backward()

        # 计算第一层和最后层的梯度比
        first_layer_grad = None
        last_layer_grad = None

        for param in model.parameters():
            if param.grad is not None:
                if first_layer_grad is None:
                    first_layer_grad = param.grad.norm().item()
                last_layer_grad = param.grad.norm().item()

        ratio = first_layer_grad / last_layer_grad if last_layer_grad > 0 else float('inf')

        print(f"\n{name}:")
        print(f"  第一层梯度范数: {first_layer_grad:.6f}")
        print(f"  最后层梯度范数: {last_layer_grad:.6f}")
        print(f"  梯度比率: {ratio:.2e}")

        if ratio > 1e6:
            print("  ⚠️ 严重梯度消失！")
        elif ratio > 1e3:
            print("  ⚠️ 轻度梯度消失")
        else:
            print("  ✅ 梯度流正常")


if __name__ == "__main__":
    print("=== 梯度问题解决方案演示 ===\n")
    compare_techniques()</code></pre>
                </div>
            </div>
        </section>
        <!-- 优化算法深度解析 -->
        <section id="optimization" class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">
                🚀 优化算法：从梯度下降到现代优化器
                <span class="depth-indicator advanced">深入</span>
            </h2>

            <!-- 引入故事 -->
            <div class="story-card">
                <h3>🎯 优化的本质：寻找最低点</h3>
                <div class="dialogue-container">
                    <div class="dialogue-item">
                        <div class="dialogue-avatar student">🤔</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">小陈</div>
                            <p>教授，为什么同样的网络架构，不同的优化器效果差别这么大？</p>
                        </div>
                    </div>
                    <div class="dialogue-item">
                        <div class="dialogue-avatar teacher">👨‍🏫</div>
                        <div class="dialogue-content">
                            <div class="dialogue-name">王教授</div>
                            <p>想象你在一个复杂的山地中寻找最低点，但你是<strong class="handwritten">蒙着眼睛的</strong>！
                                不同的优化器就像不同的搜索策略，有的谨慎，有的大胆，有的还记得之前走过的路...</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- 损失函数地形可视化 -->
            <div class="interactive-demo">
                <h3 style="color: var(--accent-blue); margin-bottom: 1.5rem;">🏔️ 损失函数地形探索</h3>

                <div class="demo-controls">
                    <div class="control-group">
                        <label>优化器选择</label>
                        <select id="optimizer-select" class="form-select">
                            <option value="sgd">SGD（随机梯度下降）</option>
                            <option value="momentum">Momentum（动量）</option>
                            <option value="rmsprop">RMSprop</option>
                            <option value="adam">Adam</option>
                            <option value="adamw">AdamW</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label>学习率</label>
                        <div class="slider-container">
                            <input type="range" id="lr-slider" min="-3" max="0" step="0.1" value="-1">
                            <span class="slider-value" id="lr-value">0.1</span>
                        </div>
                    </div>
                    <div class="control-group">
                        <label>损失地形</label>
                        <select id="landscape-select" class="form-select">
                            <option value="convex">凸函数（理想情况）</option>
                            <option value="ravine">峡谷（病态条件数）</option>
                            <option value="saddle">鞍点</option>
                            <option value="local">多个局部最优</option>
                        </select>
                    </div>
                    <button class="btn btn-primary" onclick="startOptimization()" tabindex="0">
                        🎯 开始优化
                    </button>
                    <button class="btn btn-secondary" onclick="resetOptimization()" tabindex="0">
                        🔄 重置
                    </button>
                </div>

                <canvas id="optimization-canvas" width="800" height="400"></canvas>

                <div style="margin-top: 1rem; display: grid; grid-template-columns: repeat(4, 1fr); gap: 1rem;">
                    <div style="background: rgba(139, 92, 246, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-purple);">迭代次数</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="iteration-count">0</p>
                    </div>
                    <div style="background: rgba(34, 197, 94, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-green);">当前损失</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="current-loss">-</p>
                    </div>
                    <div style="background: rgba(6, 182, 212, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-blue);">梯度范数</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="gradient-norm">-</p>
                    </div>
                    <div style="background: rgba(251, 191, 36, 0.1); padding: 1rem; border-radius: 0.5rem;">
                        <h4 style="color: var(--accent-yellow);">收敛速度</h4>
                        <p style="font-size: 1.5rem; font-weight: bold;" id="convergence-rate">-</p>
                    </div>
                </div>
            </div>

            <!-- 数学推导：梯度下降 -->
            <div class="math-derivation mt-4">
                <h3>📐 梯度下降的数学原理</h3>

                <div class="math-step" data-step="1">
                    <strong>泰勒展开与梯度</strong>
                    <p>损失函数 L(θ) 在当前点 θ₀ 的泰勒展开：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    L(θ₀ + Δθ) ≈ L(θ₀) + ∇L(θ₀)ᵀΔθ + ½Δθᵀ H Δθ
                </span>
                    </div>
                    <div class="math-explanation">
                        其中 H 是海森矩阵（二阶导数）。忽略二阶项，要使 L 减小最快，应选择：<br>
                        <strong>Δθ = -α∇L(θ₀)</strong><br>
                        这就是梯度下降的核心思想：沿负梯度方向更新参数。
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>收敛性分析</strong>
                    <p>对于 L-光滑函数（梯度利普希茨连续）：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    ||∇L(θ₁) - ∇L(θ₂)|| ≤ L ||θ₁ - θ₂||
                </span>
                    </div>
                    <div class="math-explanation">
                        可以证明，当学习率 <strong>α < 2/L</strong> 时，梯度下降收敛。<br>
                        对于强凸函数，收敛速度是指数级的：<br>
                        <strong>L(θₜ) - L(θ*) ≤ (1 - μ/L)ᵗ (L(θ₀) - L(θ*))</strong><br>
                        其中 μ 是强凸参数，μ/L 称为条件数。
                    </div>
                </div>

                <div class="math-step" data-step="3">
                    <strong>随机梯度下降（SGD）</strong>
                    <p>使用小批量估计梯度：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    g̃ = (1/|B|) Σᵢ∈B ∇Lᵢ(θ)
                </span>
                    </div>
                    <div class="math-explanation">
                        SGD的更新规则：<strong>θₜ₊₁ = θₜ - αₜg̃ₜ</strong><br>
                        期望意义下：E[g̃] = ∇L(θ)（无偏估计）<br>
                        方差：Var[g̃] = σ²/|B|，其中 |B| 是批大小
                    </div>
                </div>
            </div>

            <!-- 动量方法推导 -->
            <div class="math-derivation mt-4">
                <h3>🎢 动量方法：记忆与加速</h3>

                <div class="math-step" data-step="1">
                    <strong>经典动量（Momentum）</strong>
                    <p>灵感来自物理中的动量概念：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    vₜ = βvₜ₋₁ + (1-β)∇L(θₜ)<br>
                    θₜ₊₁ = θₜ - αvₜ
                </span>
                    </div>
                    <div class="math-explanation">
                        • β ∈ [0,1) 是动量系数，通常取0.9<br>
                        • vₜ 是速度（一阶矩的指数移动平均）<br>
                        • 在凸二次函数上，最优 β = ((√κ-1)/(√κ+1))²，其中κ是条件数
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>Nesterov加速梯度（NAG）</strong>
                    <p>"向前看"的动量：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem; color: var(--accent-purple);">
                    vₜ = βvₜ₋₁ + α∇L(θₜ - βvₜ₋₁)<br>
                    θₜ₊₁ = θₜ - vₜ
                </span>
                    </div>
                    <div class="math-explanation">
                        关键创新：在"向前看"的位置计算梯度<br>
                        理论保证：凸函数上达到 O(1/t²) 的收敛速度（普通GD是 O(1/t)）
                    </div>
                </div>
            </div>

            <!-- 自适应学习率方法 -->
            <div class="math-derivation mt-4">
                <h3>🎯 自适应学习率优化器</h3>

                <div class="math-step" data-step="1">
                    <strong>AdaGrad：梯度累积</strong>
                    <p>根据历史梯度自适应调整学习率：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    Gₜ = Gₜ₋₁ + gₜ ⊙ gₜ<br>
                    θₜ₊₁ = θₜ - α/√(Gₜ + ε) ⊙ gₜ
                </span>
                    </div>
                    <div class="math-explanation">
                        • Gₜ 累积了所有历史梯度的平方<br>
                        • ⊙ 表示逐元素乘法<br>
                        • 问题：学习率单调递减，可能过早停止学习
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>RMSprop：指数衰减</strong>
                    <p>使用指数移动平均解决AdaGrad的问题：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    vₜ = βvₜ₋₁ + (1-β)gₜ²<br>
                    θₜ₊₁ = θₜ - α/√(vₜ + ε) ⊙ gₜ
                </span>
                    </div>
                    <div class="math-explanation">
                        • β 通常取0.9或0.99<br>
                        • 保持了自适应性，但学习率不会消失<br>
                        • 适合处理非平稳目标和RNN
                    </div>
                </div>

                <div class="math-step" data-step="3">
                    <strong>Adam：自适应矩估计</strong>
                    <p>结合动量和RMSprop的优点：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem; color: var(--accent-purple);">
                    mₜ = β₁mₜ₋₁ + (1-β₁)gₜ &nbsp;&nbsp; (一阶矩)<br>
                    vₜ = β₂vₜ₋₁ + (1-β₂)gₜ² &nbsp;&nbsp; (二阶矩)<br>
                    m̂ₜ = mₜ/(1-β₁ᵗ) &nbsp;&nbsp; (偏差修正)<br>
                    v̂ₜ = vₜ/(1-β₂ᵗ) &nbsp;&nbsp; (偏差修正)<br>
                    θₜ₊₁ = θₜ - αm̂ₜ/√(v̂ₜ + ε)
                </span>
                    </div>
                    <div class="math-explanation">
                        • 默认：β₁=0.9, β₂=0.999, ε=10⁻⁸<br>
                        • 偏差修正确保初始步骤不会太大<br>
                        • 结合了动量和自适应学习率的优势
                    </div>
                </div>

                <div class="math-step" data-step="4">
                    <strong>Adam的改进版本</strong>
                    <div class="collapsible expanded">
                        <div class="collapsible-header">
                            <div class="collapsible-title">
                                <span>展开查看Adam的各种改进</span>
                            </div>
                            <span class="collapsible-icon">▼</span>
                        </div>
                        <div class="collapsible-content">
                            <div class="collapsible-inner">
                                <h4>AdamW：解耦权重衰减</h4>
                                <div class="equation-container">
                                    <span>θₜ₊₁ = (1-λα)θₜ - αm̂ₜ/√(v̂ₜ + ε)</span>
                                </div>
                                <p>将L2正则化从梯度中分离，直接作用于权重</p>

                                <h4>RAdam：整流Adam</h4>
                                <p>自适应调整方差，解决Adam初期的不稳定性</p>

                                <h4>Lookahead：双层优化</h4>
                                <div class="equation-container">
                                    <span>φₜ₊₁ = φₜ + α(θₜ - φₜ)</span>
                                </div>
                                <p>维护慢速和快速两组权重，提高稳定性</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- 学习率调度策略 -->
            <div class="section-card" style="background: rgba(139, 92, 246, 0.05); margin-top: 2rem;">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📈 学习率调度策略</h3>

                <div class="interactive-demo">
                    <canvas id="lr-schedule-canvas" width="800" height="300"></canvas>

                    <div class="demo-controls" style="margin-top: 1rem;">
                        <button class="btn btn-secondary lr-schedule-btn active" data-schedule="constant">常数</button>
                        <button class="btn btn-secondary lr-schedule-btn" data-schedule="step">阶梯衰减</button>
                        <button class="btn btn-secondary lr-schedule-btn" data-schedule="exponential">指数衰减</button>
                        <button class="btn btn-secondary lr-schedule-btn" data-schedule="cosine">余弦退火</button>
                        <button class="btn btn-secondary lr-schedule-btn" data-schedule="warmup">预热+衰减</button>
                        <button class="btn btn-secondary lr-schedule-btn" data-schedule="cyclical">循环学习率</button>
                    </div>
                </div>

                <div class="concept-grid mt-4">
                    <div class="concept-card what">
                        <h3 style="color: var(--accent-green);">阶梯衰减</h3>
                        <div class="equation-container">
                            <span>αₜ = α₀ × γ^⌊t/s⌋</span>
                        </div>
                        <p>每s个epoch衰减γ倍</p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            适用：CV任务，明确的阶段
                        </p>
                    </div>
                    <div class="concept-card how">
                        <h3 style="color: var(--accent-blue);">余弦退火</h3>
                        <div class="equation-container">
                            <span>αₜ = αₘᵢₙ + ½(αₘₐₓ-αₘᵢₙ)(1+cos(πt/T))</span>
                        </div>
                        <p>平滑衰减，可重启</p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            适用：现代深度网络
                        </p>
                    </div>
                    <div class="concept-card why">
                        <h3 style="color: var(--accent-red);">学习率预热</h3>
                        <div class="equation-container">
                            <span>αₜ = t/Tᵥᵥₐᵣₘ × α₀</span>
                        </div>
                        <p>初期线性增长</p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            适用：大批量训练
                        </p>
                    </div>
                    <div class="concept-card pitfall">
                        <h3 style="color: var(--accent-yellow);">循环学习率</h3>
                        <div class="equation-container">
                            <span>αₜ = αₘᵢₙ + Δα × triangle(t)</span>
                        </div>
                        <p>周期性变化</p>
                        <p style="font-size: 0.875rem; color: var(--text-secondary);">
                            适用：快速训练
                        </p>
                    </div>
                </div>
            </div>

            <!-- 批量大小的影响 -->
            <div class="math-derivation mt-4">
                <h3>📊 批量大小的理论分析</h3>

                <div class="math-step" data-step="1">
                    <strong>噪声与泛化</strong>
                    <p>SGD的更新可以分解为：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    θₜ₊₁ = θₜ - α(g + ξₜ)
                </span>
                    </div>
                    <div class="math-explanation">
                        • g = E[∇L(θₜ)] 是真实梯度<br>
                        • ξₜ 是零均值噪声，Var[ξₜ] ∝ σ²/B<br>
                        • 小批量→大噪声→更好的探索→可能更好的泛化<br>
                        • 大批量→小噪声→快速收敛→可能陷入尖锐极小值
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>线性缩放规则</strong>
                    <p>当批量大小增加k倍时：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem; color: var(--accent-purple);">
                    α_new = k × α_base<br>
                    但需要预热：α_warmup(t) = t/T_warmup × α_new
                </span>
                    </div>
                    <div class="math-explanation">
                        理论依据：保持 α/B 不变，使期望更新量相同<br>
                        实践限制：批量过大会损害泛化（泛化鸿沟）<br>
                        经验规则：B > 8192 时需要特殊技巧
                    </div>
                </div>
            </div>

            <!-- 二阶优化方法 -->
            <div class="math-derivation mt-4">
                <h3>🔬 二阶优化方法</h3>

                <div class="math-step" data-step="1">
                    <strong>牛顿法</strong>
                    <p>使用二阶信息（海森矩阵）：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    θₜ₊₁ = θₜ - H⁻¹∇L(θₜ)
                </span>
                    </div>
                    <div class="math-explanation">
                        • H 是海森矩阵（n×n），计算和存储代价 O(n²)<br>
                        • 二次收敛速度（误差平方级减少）<br>
                        • 问题：深度学习中参数太多，不实用
                    </div>
                </div>

                <div class="math-step" data-step="2">
                    <strong>拟牛顿法（L-BFGS）</strong>
                    <p>近似海森矩阵的逆：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    H⁻¹ ≈ B = B₀ + 低秩修正
                </span>
                    </div>
                    <div class="math-explanation">
                        • 只需存储 m 个向量对（通常 m=5~20）<br>
                        • 内存需求：O(mn) 而非 O(n²)<br>
                        • 适合小批量、确定性优化<br>
                        • 在深度学习中主要用于微调阶段
                    </div>
                </div>

                <div class="math-step" data-step="3">
                    <strong>自然梯度下降</strong>
                    <p>考虑参数空间的几何结构：</p>
                    <div class="equation-container">
                <span style="font-size: 1.2rem;">
                    θₜ₊₁ = θₜ - αF⁻¹∇L(θₜ)
                </span>
                    </div>
                    <div class="math-explanation">
                        • F 是Fisher信息矩阵：F = E[∇log p(x|θ)∇log p(x|θ)ᵀ]<br>
                        • 在KL散度度量下的最陡下降方向<br>
                        • K-FAC：用Kronecker因子近似F，使计算可行<br>
                        • 在某些任务上显著加速收敛
                    </div>
                </div>
            </div>

            <!-- 优化理论深入 -->
            <div class="section-card" style="background: rgba(239, 68, 68, 0.05); margin-top: 2rem;">
                <h3 style="color: var(--accent-red); margin-bottom: 1.5rem;">⚡ 优化中的关键挑战</h3>

                <div class="concept-grid">
                    <div class="concept-card why">
                        <h3>病态条件数</h3>
                        <p>海森矩阵特征值相差悬殊</p>
                        <div class="equation-container">
                            <span>κ = λₘₐₓ/λₘᵢₙ ≫ 1</span>
                        </div>
                        <p style="font-size: 0.875rem;">
                            导致：峡谷地形，震荡<br>
                            解决：预处理，动量方法
                        </p>
                    </div>
                    <div class="concept-card pitfall">
                        <h3>鞍点问题</h3>
                        <p>高维空间中鞍点远多于局部最优</p>
                        <p style="font-size: 0.875rem;">
                            特征：某些方向上升，某些下降<br>
                            解决：噪声帮助逃离
                        </p>
                    </div>
                    <div class="concept-card what">
                        <h3>平坦极小值</h3>
                        <p>Loss在最优点附近很平坦</p>
                        <div style="background: rgba(0,0,0,0.2); padding: 0.5rem; border-radius: 0.25rem; margin-top: 0.5rem;">
                            <code>L(θ + δ) ≈ L(θ*)</code>
                        </div>
                        <p style="font-size: 0.875rem;">
                            优点：泛化好<br>
                            缺点：收敛慢
                        </p>
                    </div>
                    <div class="concept-card how">
                        <h3>梯度爆炸/消失</h3>
                        <p>深层网络的固有问题</p>
                        <p style="font-size: 0.875rem;">
                            梯度裁剪：||g|| > θ 时缩放<br>
                            归一化：BN, LN, GN等
                        </p>
                    </div>
                </div>
            </div>

            <!-- 实践代码：优化器对比 -->
            <div class="code-container mt-4">
                <div class="code-header">
                    <div class="code-header-left">
                        <button class="code-tab active" data-lang="pytorch" tabindex="0">PyTorch实现</button>
                        <button class="code-tab" data-lang="numpy" tabindex="0">从零实现</button>
                    </div>
                    <div class="code-header-right">
                        <button class="code-action" onclick="copyCode()" tabindex="0">
                            📋 复制
                        </button>
                        <button class="code-action" onclick="runCode()" tabindex="0">
                            ▶️ 运行
                        </button>
                    </div>
                </div>
                <div class="code-content" id="pytorch-code">
<pre><code class="python">import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import *

class OptimizationExperiment:
    """
    深入对比各种优化器的行为
    """

    def __init__(self, model, loss_fn, device='cpu'):
        self.model = model.to(device)
        self.loss_fn = loss_fn
        self.device = device

    def create_optimizers(self, lr=0.01):
        """创建各种优化器"""
        params = self.model.parameters()

        optimizers = {
            'SGD': optim.SGD(params, lr=lr),
            'SGD+Momentum': optim.SGD(params, lr=lr, momentum=0.9),
            'SGD+Nesterov': optim.SGD(params, lr=lr, momentum=0.9, nesterov=True),
            'AdaGrad': optim.Adagrad(params, lr=lr),
            'RMSprop': optim.RMSprop(params, lr=lr),
            'Adam': optim.Adam(params, lr=lr),
            'AdamW': optim.AdamW(params, lr=lr, weight_decay=0.01),
            'RAdam': optim.RAdam(params, lr=lr),
        }

        # 添加二阶优化器（如果参数量不太大）
        if sum(p.numel() for p in params) < 10000:
            optimizers['L-BFGS'] = optim.LBFGS(params, lr=1, max_iter=20)

        return optimizers

    def track_optimization_path(self, optimizer, data_loader, epochs=100):
        """
        跟踪优化路径

        返回：
        - losses: 每步的损失值
        - gradients: 梯度范数
        - param_trajectory: 参数轨迹（如果是2D）
        """
        losses = []
        gradients = []
        param_trajectory = []

        # 重置模型参数
        for p in self.model.parameters():
            nn.init.normal_(p, mean=0, std=0.1)

        for epoch in range(epochs):
            epoch_loss = 0
            epoch_grad_norm = 0

            for batch_idx, (data, target) in enumerate(data_loader):
                data, target = data.to(self.device), target.to(self.device)

                # 特殊处理L-BFGS
                if isinstance(optimizer, optim.LBFGS):
                    def closure():
                        optimizer.zero_grad()
                        output = self.model(data)
                        loss = self.loss_fn(output, target)
                        loss.backward()
                        return loss

                    loss = optimizer.step(closure)
                else:
                    # 标准优化器
                    optimizer.zero_grad()
                    output = self.model(data)
                    loss = self.loss_fn(output, target)
                    loss.backward()

                    # 记录梯度范数
                    total_grad_norm = 0
                    for p in self.model.parameters():
                        if p.grad is not None:
                            total_grad_norm += p.grad.norm().item() ** 2
                    total_grad_norm = total_grad_norm ** 0.5

                    optimizer.step()

                epoch_loss += loss.item()
                epoch_grad_norm += total_grad_norm

            losses.append(epoch_loss / len(data_loader))
            gradients.append(epoch_grad_norm / len(data_loader))

            # 记录参数（如果是简单的2D问题）
            if self.model.fc1.weight.numel() == 2:
                w = self.model.fc1.weight.data.cpu().numpy().flatten()
                param_trajectory.append(w.copy())

        return {
            'losses': losses,
            'gradients': gradients,
            'param_trajectory': np.array(param_trajectory) if param_trajectory else None
        }

    def visualize_optimization_landscape(self, results_dict):
        """可视化优化过程"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. 损失曲线
        ax = axes[0, 0]
        for name, results in results_dict.items():
            ax.semilogy(results['losses'], label=name, linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss (log scale)')
        ax.set_title('训练损失对比')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. 梯度范数
        ax = axes[0, 1]
        for name, results in results_dict.items():
            if results['gradients'][0] > 0:  # 排除L-BFGS
                ax.plot(results['gradients'], label=name, linewidth=2)
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Gradient Norm')
        ax.set_title('梯度范数变化')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 3. 收敛速度对比
        ax = axes[1, 0]
        final_losses = {name: results['losses'][-1]
                       for name, results in results_dict.items()}
        names = list(final_losses.keys())
        values = list(final_losses.values())
        colors = plt.cm.viridis(np.linspace(0, 1, len(names)))

        bars = ax.bar(names, values, color=colors)
        ax.set_ylabel('Final Loss')
        ax.set_title('最终损失对比')
        ax.set_yscale('log')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

        # 4. 参数轨迹（如果可用）
        ax = axes[1, 1]
        if any(r['param_trajectory'] is not None for r in results_dict.values()):
            # 绘制损失函数等高线（示意）
            x = np.linspace(-2, 2, 100)
            y = np.linspace(-2, 2, 100)
            X, Y = np.meshgrid(x, y)
            Z = X**2 + 10*Y**2  # 简单的椭圆形损失函数

            ax.contour(X, Y, Z, levels=20, alpha=0.3)

            # 绘制优化路径
            for name, results in results_dict.items():
                if results['param_trajectory'] is not None:
                    traj = results['param_trajectory']
                    ax.plot(traj[:, 0], traj[:, 1], 'o-',
                           label=name, markersize=3, linewidth=1)

            ax.set_xlabel('Parameter 1')
            ax.set_ylabel('Parameter 2')
            ax.set_title('优化路径（2D投影）')
            ax.legend()
            ax.grid(True, alpha=0.3)
        else:
            ax.text(0.5, 0.5, '参数维度过高\n无法可视化',
                   ha='center', va='center', transform=ax.transAxes)

        plt.tight_layout()
        plt.show()


class LearningRateSchedulers:
    """学习率调度器的实现和可视化"""

    @staticmethod
    def visualize_schedules(initial_lr=0.1, total_epochs=100):
        """可视化各种学习率调度策略"""

        # 创建一个dummy优化器和参数
        param = torch.nn.Parameter(torch.randn(1))
        optimizer = optim.SGD([param], lr=initial_lr)

        schedulers = {
            '常数': None,
            '阶梯衰减': StepLR(optimizer, step_size=30, gamma=0.1),
            '多阶梯衰减': MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1),
            '指数衰减': ExponentialLR(optimizer, gamma=0.95),
            '余弦退火': CosineAnnealingLR(optimizer, T_max=total_epochs),
            '带重启的余弦': CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2),
            '线性预热+余弦': None,  # 自定义实现
            '循环学习率': CyclicLR(optimizer, base_lr=0.001, max_lr=initial_lr,
                                  step_size_up=20, mode='triangular'),
            'OneCycle': OneCycleLR(optimizer, max_lr=initial_lr,
                                  epochs=total_epochs, steps_per_epoch=1)
        }

        plt.figure(figsize=(15, 10))

        for idx, (name, scheduler) in enumerate(schedulers.items()):
            plt.subplot(3, 3, idx + 1)

            lrs = []

            # 重置优化器
            for group in optimizer.param_groups:
                group['lr'] = initial_lr

            for epoch in range(total_epochs):
                if name == '常数':
                    lrs.append(initial_lr)
                elif name == '线性预热+余弦':
                    # 自定义预热+余弦退火
                    warmup_epochs = 10
                    if epoch < warmup_epochs:
                        lr = initial_lr * (epoch / warmup_epochs)
                    else:
                        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
                        lr = initial_lr * 0.5 * (1 + np.cos(np.pi * progress))
                    lrs.append(lr)
                    for group in optimizer.param_groups:
                        group['lr'] = lr
                else:
                    lrs.append(optimizer.param_groups[0]['lr'])
                    if scheduler:
                        scheduler.step()

            plt.plot(lrs, linewidth=2)
            plt.title(name)
            plt.xlabel('Epoch')
            plt.ylabel('Learning Rate')
            plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    @staticmethod
    def adaptive_lr_finder(model, data_loader, loss_fn,
                          min_lr=1e-5, max_lr=10, num_iter=100):
        """
        学习率范围测试（LR Range Test）

        逐步增加学习率，观察损失变化
        """
        model.train()
        optimizer = optim.SGD(model.parameters(), lr=min_lr)

        lrs = np.logspace(np.log10(min_lr), np.log10(max_lr), num_iter)
        losses = []

        data_iter = iter(data_loader)

        for i, lr in enumerate(lrs):
            # 设置学习率
            for param_group in optimizer.param_groups:
                param_group['lr'] = lr

            # 获取批次数据
            try:
                inputs, targets = next(data_iter)
            except StopIteration:
                data_iter = iter(data_loader)
                inputs, targets = next(data_iter)

            # 前向传播
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)

            # 记录损失
            losses.append(loss.item())

            # 反向传播
            loss.backward()
            optimizer.step()

            # 如果损失爆炸，提前停止
            if loss.item() > 4 * np.min(losses):
                break

        # 可视化
        plt.figure(figsize=(10, 6))
        plt.semilogx(lrs[:len(losses)], losses)
        plt.xlabel('Learning Rate (log scale)')
        plt.ylabel('Loss')
        plt.title('学习率范围测试')
        plt.grid(True, alpha=0.3)

        # 找到最佳学习率（损失下降最快的点）
        min_grad_idx = np.argmin(np.gradient(losses))
        best_lr = lrs[min_grad_idx]

        plt.axvline(best_lr, color='red', linestyle='--',
                   label=f'建议学习率: {best_lr:.2e}')
        plt.legend()
        plt.show()

        return best_lr


# 实验：优化器在不同地形上的表现
def optimization_landscape_experiment():
    """
    在不同的损失地形上测试优化器
    """

    # 1. Rosenbrock函数（非凸优化测试）
    class RosenbrockModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.params = nn.Parameter(torch.tensor([-1.2, 1.0]))

        def forward(self):
            x, y = self.params[0], self.params[1]
            return (1 - x)**2 + 100 * (y - x**2)**2

    # 2. 病态条件数的二次函数
    class IllConditionedQuadratic(nn.Module):
        def __init__(self, condition_number=100):
            super().__init__()
            self.params = nn.Parameter(torch.randn(2))
            self.A = torch.tensor([[condition_number, 0], [0, 1]],
                                dtype=torch.float32)

        def forward(self):
            return 0.5 * self.params @ self.A @ self.params

    # 3. 带有多个局部最优的函数
    class MultiModalFunction(nn.Module):
        def __init__(self):
            super().__init__()
            self.params = nn.Parameter(torch.tensor([0.0, 0.0]))

        def forward(self):
            x, y = self.params[0], self.params[1]
            return -torch.exp(-(x**2 + y**2)) - \
                   0.5 * torch.exp(-((x-1)**2 + (y-1)**2))

    # 测试不同优化器
    models = {
        'Rosenbrock': RosenbrockModel(),
        '病态二次': IllConditionedQuadratic(condition_number=1000),
        '多峰函数': MultiModalFunction()
    }

    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    for idx, (name, model) in enumerate(models.items()):
        ax = axes[idx]

        # 创建损失地形
        x = np.linspace(-2, 2, 100)
        y = np.linspace(-2, 2, 100)
        X, Y = np.meshgrid(x, y)
        Z = np.zeros_like(X)

        for i in range(100):
            for j in range(100):
                model.params.data = torch.tensor([X[i, j], Y[i, j]],
                                               dtype=torch.float32)
                Z[i, j] = model().item()

        # 绘制等高线
        contour = ax.contour(X, Y, Z, levels=30, alpha=0.6)
        ax.clabel(contour, inline=True, fontsize=8)

        # 测试不同优化器
        optimizers = {
            'SGD': optim.SGD(model.parameters(), lr=0.01),
            'Momentum': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),
            'Adam': optim.Adam(model.parameters(), lr=0.1)
        }

        colors = ['red', 'blue', 'green']

        for opt_idx, (opt_name, optimizer) in enumerate(optimizers.items()):
            # 重置起点
            model.params.data = torch.tensor([-1.5, 1.5], dtype=torch.float32)
            trajectory = [model.params.data.numpy().copy()]

            # 优化步骤
            for _ in range(50):
                optimizer.zero_grad()
                loss = model()
                loss.backward()
                optimizer.step()
                trajectory.append(model.params.data.numpy().copy())

            trajectory = np.array(trajectory)
            ax.plot(trajectory[:, 0], trajectory[:, 1],
                   'o-', color=colors[opt_idx], label=opt_name,
                   markersize=3, linewidth=2)

        ax.set_title(f'{name}')
        ax.set_xlabel('Parameter 1')
        ax.set_ylabel('Parameter 2')
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


# 批量大小的系统研究
def batch_size_scaling_experiment():
    """
    研究批量大小对优化和泛化的影响
    """

    # 生成合成数据
    np.random.seed(42)
    torch.manual_seed(42)

    # 创建一个非线性分类数据集
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=10000, n_features=20,
                              n_informative=15, n_redundant=5,
                              n_clusters_per_class=3, random_state=42)

    X = torch.FloatTensor(X)
    y = torch.LongTensor(y)

    # 划分训练集和测试集
    train_size = 8000
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # 测试不同的批量大小
    batch_sizes = [32, 64, 128, 256, 512, 1024, 2048]
    results = {}

    for batch_size in batch_sizes:
        print(f"\n测试批量大小: {batch_size}")

        # 创建数据加载器
        train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )

        # 创建模型
        model = nn.Sequential(
            nn.Linear(20, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.3),
            nn.Linear(64, 2)
        )

        # 使用线性缩放规则
        base_lr = 0.01
        scaled_lr = base_lr * np.sqrt(batch_size / 32)

        optimizer = optim.SGD(model.parameters(), lr=scaled_lr, momentum=0.9)
        criterion = nn.CrossEntropyLoss()

        # 训练
        train_losses = []
        test_accuracies = []
        gradient_norms = []

        for epoch in range(50):
            # 训练阶段
            model.train()
            epoch_loss = 0
            epoch_grad_norm = 0

            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()

                # 记录梯度范数
                total_norm = 0
                for p in model.parameters():
                    if p.grad is not None:
                        total_norm += p.grad.data.norm(2).item() ** 2
                total_norm = total_norm ** 0.5
                epoch_grad_norm += total_norm

                optimizer.step()
                epoch_loss += loss.item()

            train_losses.append(epoch_loss / len(train_loader))
            gradient_norms.append(epoch_grad_norm / len(train_loader))

            # 测试阶段
            model.eval()
            with torch.no_grad():
                test_outputs = model(X_test)
                test_pred = test_outputs.argmax(dim=1)
                test_acc = (test_pred == y_test).float().mean().item()
                test_accuracies.append(test_acc)

        results[batch_size] = {
            'train_losses': train_losses,
            'test_accuracies': test_accuracies,
            'gradient_norms': gradient_norms,
            'final_test_acc': test_accuracies[-1]
        }

    # 可视化结果
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # 1. 训练损失曲线
    ax = axes[0, 0]
    for bs in batch_sizes:
        ax.plot(results[bs]['train_losses'], label=f'BS={bs}', linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Training Loss')
    ax.set_title('不同批量大小的训练损失')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. 测试准确率曲线
    ax = axes[0, 1]
    for bs in batch_sizes:
        ax.plot(results[bs]['test_accuracies'], label=f'BS={bs}', linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Test Accuracy')
    ax.set_title('不同批量大小的测试准确率')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. 最终测试准确率 vs 批量大小
    ax = axes[1, 0]
    final_accs = [results[bs]['final_test_acc'] for bs in batch_sizes]
    ax.semilogx(batch_sizes, final_accs, 'o-', linewidth=2, markersize=8)
    ax.set_xlabel('Batch Size (log scale)')
    ax.set_ylabel('Final Test Accuracy')
    ax.set_title('泛化能力 vs 批量大小')
    ax.grid(True, alpha=0.3)

    # 4. 梯度噪声水平
    ax = axes[1, 1]
    for bs in batch_sizes[:4]:  # 只显示前几个避免拥挤
        grad_stds = []
        for i in range(5, len(results[bs]['gradient_norms'])):
            # 计算梯度的标准差作为噪声的度量
            recent_grads = results[bs]['gradient_norms'][i-5:i]
            grad_stds.append(np.std(recent_grads))
        ax.plot(grad_stds, label=f'BS={bs}', linewidth=2)

    ax.set_xlabel('Epoch')
    ax.set_ylabel('Gradient Noise (Std)')
    ax.set_title('梯度噪声水平')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 打印总结
    print("\n=== 批量大小实验总结 ===")
    print("批量大小 | 最终测试准确率")
    print("-" * 30)
    for bs in batch_sizes:
        print(f"{bs:8d} | {results[bs]['final_test_acc']:.4f}")

    print("\n观察：")
    print("1. 小批量：更多噪声，可能有更好的泛化")
    print("2. 大批量：收敛更稳定，但可能陷入尖锐极小值")
    print("3. 存在一个最优批量大小，平衡效率和泛化")


if __name__ == "__main__":
    print("=== MLP优化算法深度实验 ===\n")

    # 1. 优化器对比
    print("1. 创建简单的测试问题...")
    from torch.utils.data import DataLoader, TensorDataset

    # 生成简单的分类数据
    X = torch.randn(1000, 10)
    y = (X.sum(dim=1) > 0).long()
    dataset = TensorDataset(X, y)
    data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

    # 简单模型
    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 2)
    )

    experiment = OptimizationExperiment(model, nn.CrossEntropyLoss())

    print("\n2. 测试各种优化器...")
    optimizers = experiment.create_optimizers(lr=0.01)
    results = {}

    for name, opt in optimizers.items():
        print(f"   测试 {name}...")
        results[name] = experiment.track_optimization_path(opt, data_loader, epochs=50)

    print("\n3. 可视化结果...")
    experiment.visualize_optimization_landscape(results)

    # 2. 学习率调度
    print("\n4. 学习率调度策略可视化...")
    LearningRateSchedulers.visualize_schedules()

    # 3. 优化地形实验
    print("\n5. 不同优化地形上的表现...")
    optimization_landscape_experiment()

    # 4. 批量大小研究
    print("\n6. 批量大小对优化的影响...")
    batch_size_scaling_experiment()

    print("\n实验完成！")</code></pre>
                </div>
                <div class="code-content" id="numpy-code" style="display: none;">
<pre><code class="python">import numpy as np
import matplotlib.pyplot as plt

class Optimizer:
    """优化器基类"""

    def __init__(self, params, lr=0.01):
        self.params = params
        self.lr = lr

    def step(self, grads):
        raise NotImplementedError

    def zero_grad(self):
        pass


class SGD(Optimizer):
    """随机梯度下降"""

    def step(self, grads):
        for param, grad in zip(self.params, grads):
            param -= self.lr * grad


class Momentum(Optimizer):
    """带动量的SGD"""

    def __init__(self, params, lr=0.01, momentum=0.9):
        super().__init__(params, lr)
        self.momentum = momentum
        self.velocities = [np.zeros_like(p) for p in params]

    def step(self, grads):
        for i, (param, grad) in enumerate(zip(self.params, grads)):
            self.velocities[i] = self.momentum * self.velocities[i] + (1 - self.momentum) * grad
            param -= self.lr * self.velocities[i]


class AdaGrad(Optimizer):
    """自适应梯度算法"""

    def __init__(self, params, lr=0.01, eps=1e-8):
        super().__init__(params, lr)
        self.eps = eps
        self.accumulated_grads = [np.zeros_like(p) for p in params]

    def step(self, grads):
        for i, (param, grad) in enumerate(zip(self.params, grads)):
            self.accumulated_grads[i] += grad ** 2
            param -= self.lr * grad / (np.sqrt(self.accumulated_grads[i]) + self.eps)


class RMSprop(Optimizer):
    """RMSprop优化器"""

    def __init__(self, params, lr=0.01, beta=0.9, eps=1e-8):
        super().__init__(params, lr)
        self.beta = beta
        self.eps = eps
        self.v = [np.zeros_like(p) for p in params]

    def step(self, grads):
        for i, (param, grad) in enumerate(zip(self.params, grads)):
            self.v[i] = self.beta * self.v[i] + (1 - self.beta) * grad ** 2
            param -= self.lr * grad / (np.sqrt(self.v[i]) + self.eps)


class Adam(Optimizer):
    """Adam优化器的完整实现"""

    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        super().__init__(params, lr)
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        self.t = 0

    def step(self, grads):
        self.t += 1

        for i, (param, grad) in enumerate(zip(self.params, grads)):
            # 更新一阶矩（动量）
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad

            # 更新二阶矩（RMSprop部分）
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2

            # 偏差修正
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)

            # 参数更新
            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)


class AdamW(Adam):
    """AdamW：解耦权重衰减的Adam"""

    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999,
                 eps=1e-8, weight_decay=0.01):
        super().__init__(params, lr, beta1, beta2, eps)
        self.weight_decay = weight_decay

    def step(self, grads):
        self.t += 1

        for i, (param, grad) in enumerate(zip(self.params, grads)):
            # 标准Adam更新
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad ** 2

            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)

            # 参数更新（带权重衰减）
            param -= self.lr * (m_hat / (np.sqrt(v_hat) + self.eps) +
                               self.weight_decay * param)


# 学习率调度器
class LRScheduler:
    """学习率调度器基类"""

    def __init__(self, optimizer, last_epoch=-1):
        self.optimizer = optimizer
        self.last_epoch = last_epoch
        self.base_lr = optimizer.lr

    def step(self):
        self.last_epoch += 1
        self.optimizer.lr = self.get_lr()

    def get_lr(self):
        raise NotImplementedError


class StepLR(LRScheduler):
    """阶梯衰减"""

    def __init__(self, optimizer, step_size, gamma=0.1):
        super().__init__(optimizer)
        self.step_size = step_size
        self.gamma = gamma

    def get_lr(self):
        return self.base_lr * (self.gamma ** (self.last_epoch // self.step_size))


class ExponentialLR(LRScheduler):
    """指数衰减"""

    def __init__(self, optimizer, gamma):
        super().__init__(optimizer)
        self.gamma = gamma

    def get_lr(self):
        return self.base_lr * (self.gamma ** self.last_epoch)


class CosineAnnealingLR(LRScheduler):
    """余弦退火"""

    def __init__(self, optimizer, T_max):
        super().__init__(optimizer)
        self.T_max = T_max

    def get_lr(self):
        return self.base_lr * (1 + np.cos(np.pi * self.last_epoch / self.T_max)) / 2


# 优化可视化函数
def visualize_optimization_2d():
    """
    在2D函数上可视化不同优化器的行为
    """

    # 定义测试函数
    def rosenbrock(x, y):
        """Rosenbrock函数（香蕉函数）"""
        return (1 - x)**2 + 100 * (y - x**2)**2

    def rosenbrock_grad(x, y):
        """Rosenbrock函数的梯度"""
        dx = -2 * (1 - x) - 400 * x * (y - x**2)
        dy = 200 * (y - x**2)
        return np.array([dx, dy])

    # 创建网格
    x = np.linspace(-2, 2, 100)
    y = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x, y)
    Z = rosenbrock(X, Y)

    # 初始化参数
    start_point = np.array([-1.5, 2.5])

    # 创建优化器
    optimizers = {
        'SGD': SGD([start_point.copy()], lr=0.001),
        'Momentum': Momentum([start_point.copy()], lr=0.001, momentum=0.9),
        'RMSprop': RMSprop([start_point.copy()], lr=0.01),
        'Adam': Adam([start_point.copy()], lr=0.1)
    }

    # 运行优化
    trajectories = {}
    for name, opt in optimizers.items():
        trajectory = [opt.params[0].copy()]

        for _ in range(200):
            grad = rosenbrock_grad(opt.params[0][0], opt.params[0][1])
            opt.step([grad])
            trajectory.append(opt.params[0].copy())

        trajectories[name] = np.array(trajectory)

    # 可视化
    plt.figure(figsize=(12, 10))

    # 绘制等高线
    levels = np.logspace(-0.5, 3.5, 20)
    plt.contour(X, Y, Z, levels=levels, alpha=0.6)

    # 绘制优化轨迹
    colors = ['red', 'blue', 'green', 'purple']
    for (name, trajectory), color in zip(trajectories.items(), colors):
        plt.plot(trajectory[:, 0], trajectory[:, 1],
                'o-', color=color, label=name,
                markersize=3, linewidth=2, alpha=0.8)

    # 标记起点和终点
    plt.plot(start_point[0], start_point[1], 'ko', markersize=10, label='起点')
    plt.plot(1, 1, 'k*', markersize=15, label='最优点')

    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('不同优化器在Rosenbrock函数上的轨迹')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


# 批量大小与梯度噪声的关系
def batch_size_noise_analysis():
    """
    分析批量大小对梯度估计噪声的影响
    """

    # 生成模拟数据
    np.random.seed(42)
    n_samples = 10000
    n_features = 50

    # 生成线性可分数据（带噪声）
    X = np.random.randn(n_samples, n_features)
    true_w = np.random.randn(n_features)
    y = X @ true_w + 0.1 * np.random.randn(n_samples)

    # 定义损失函数和梯度
    def compute_loss_and_grad(X_batch, y_batch, w):
        """计算批量上的MSE损失和梯度"""
        predictions = X_batch @ w
        errors = predictions - y_batch
        loss = 0.5 * np.mean(errors ** 2)
        grad = X_batch.T @ errors / len(y_batch)
        return loss, grad

    # 测试不同的批量大小
    batch_sizes = [1, 4, 16, 64, 256, 1024]
    n_iterations = 100

    results = {}

    for batch_size in batch_sizes:
        print(f"测试批量大小: {batch_size}")

        # 初始化参数
        w = np.random.randn(n_features) * 0.1

        # 记录梯度统计
        grad_norms = []
        grad_variances = []
        losses = []

        # 创建优化器
        optimizer = Adam([w], lr=0.01)

        for iteration in range(n_iterations):
            # 随机采样多个批次计算梯度方差
            batch_grads = []

            for _ in range(10):  # 采样10个批次估计方差
                indices = np.random.choice(n_samples, batch_size, replace=False)
                X_batch = X[indices]
                y_batch = y[indices]

                loss, grad = compute_loss_and_grad(X_batch, y_batch, w)
                batch_grads.append(grad)

            batch_grads = np.array(batch_grads)

            # 计算梯度统计
            mean_grad = np.mean(batch_grads, axis=0)
            grad_variance = np.mean(np.var(batch_grads, axis=0))

            grad_norms.append(np.linalg.norm(mean_grad))
            grad_variances.append(grad_variance)
            losses.append(loss)

            # 更新参数
            optimizer.step([mean_grad])

        results[batch_size] = {
            'grad_norms': grad_norms,
            'grad_variances': grad_variances,
            'losses': losses
        }

    # 可视化结果
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. 梯度方差 vs 批量大小
    ax = axes[0, 0]
    mean_variances = [np.mean(results[bs]['grad_variances']) for bs in batch_sizes]
    theoretical_variances = [mean_variances[0] / bs for bs in batch_sizes]

    ax.loglog(batch_sizes, mean_variances, 'o-', label='实际方差', linewidth=2)
    ax.loglog(batch_sizes, theoretical_variances, '--', label='理论 (∝1/B)', linewidth=2)
    ax.set_xlabel('批量大小')
    ax.set_ylabel('梯度方差')
    ax.set_title('梯度估计的方差')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. 收敛曲线
    ax = axes[0, 1]
    for bs in batch_sizes:
        ax.semilogy(results[bs]['losses'], label=f'B={bs}', linewidth=2)
    ax.set_xlabel('迭代次数')
    ax.set_ylabel('损失值 (log scale)')
    ax.set_title('不同批量大小的收敛曲线')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. 梯度范数演化
    ax = axes[1, 0]
    for bs in [1, 16, 256]:  # 选择几个代表性的
        ax.plot(results[bs]['grad_norms'], label=f'B={bs}', linewidth=2)
    ax.set_xlabel('迭代次数')
    ax.set_ylabel('梯度范数')
    ax.set_title('梯度范数的演化')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 4. 信噪比分析
    ax = axes[1, 1]
    snr_values = []
    for bs in batch_sizes:
        # 信噪比 = 梯度范数² / 梯度方差
        norms = np.array(results[bs]['grad_norms'][10:])  # 跳过初始阶段
        variances = np.array(results[bs]['grad_variances'][10:])
        snr = np.mean(norms ** 2 / variances)
        snr_values.append(snr)

    ax.loglog(batch_sizes, snr_values, 'o-', linewidth=2)
    ax.set_xlabel('批量大小')
    ax.set_ylabel('信噪比')
    ax.set_title('梯度信噪比 vs 批量大小')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # 打印分析
    print("\n=== 批量大小与噪声分析 ===")
    print("批量大小 | 平均梯度方差 | 最终损失")
    print("-" * 40)
    for bs in batch_sizes:
        avg_var = np.mean(results[bs]['grad_variances'])
        final_loss = results[bs]['losses'][-1]
        print(f"{bs:8d} | {avg_var:.6f} | {final_loss:.6f}")


# 学习率范围测试
def learning_rate_range_test():
    """
    实现学习率范围测试，找到最优学习率
    """

    # 生成测试问题
    np.random.seed(42)
    n_samples = 1000
    n_features = 20

    X = np.random.randn(n_samples, n_features)
    true_w = np.random.randn(n_features)
    y = X @ true_w + 0.5 * np.random.randn(n_samples)

    # 定义模型和损失
    def forward_and_loss(X, y, w):
        pred = X @ w
        loss = 0.5 * np.mean((pred - y) ** 2)
        grad = X.T @ (pred - y) / len(y)
        return loss, grad

    # 学习率范围
    min_lr = 1e-4
    max_lr = 1.0
    num_iterations = 200

    lrs = np.logspace(np.log10(min_lr), np.log10(max_lr), num_iterations)
    losses = []

    # 初始化权重
    w = np.random.randn(n_features) * 0.01

    # 逐步增加学习率
    for i, lr in enumerate(lrs):
        # 随机批次
        batch_size = 32
        indices = np.random.choice(n_samples, batch_size)
        X_batch = X[indices]
        y_batch = y[indices]

        # 计算损失和梯度
        loss, grad = forward_and_loss(X_batch, y_batch, w)
        losses.append(loss)

        # 更新权重
        w = w - lr * grad

        # 如果损失爆炸，停止
        if loss > 1e6 or np.isnan(loss):
            losses = losses[:i+1]
            lrs = lrs[:i+1]
            break

    # 平滑损失曲线
    window_size = 5
    smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')

    # 找到最佳学习率（损失下降最快的点）
    loss_gradient = np.gradient(smoothed_losses)
    best_idx = np.argmin(loss_gradient)
    best_lr = lrs[best_idx + window_size//2]

    # 可视化
    plt.figure(figsize=(10, 6))
    plt.semilogx(lrs, losses, 'b-', alpha=0.3, label='原始损失')
    plt.semilogx(lrs[window_size//2:-window_size//2+1], smoothed_losses,
                'r-', linewidth=2, label='平滑损失')
    plt.axvline(best_lr, color='green', linestyle='--', linewidth=2,
               label=f'建议学习率: {best_lr:.2e}')

    plt.xlabel('学习率 (log scale)')
    plt.ylabel('损失值')
    plt.title('学习率范围测试')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return best_lr


# 演示梯度裁剪的效果
def gradient_clipping_demo():
    """
    演示梯度裁剪如何防止梯度爆炸
    """

    # 创建一个容易梯度爆炸的问题
    np.random.seed(42)

    # RNN风格的循环计算（容易爆炸）
    def recurrent_loss(params, sequence_length=20):
        W, b = params
        state = np.zeros(5)
        loss = 0

        for t in range(sequence_length):
            state = np.tanh(W @ state + b)
            loss += np.sum(state ** 2)

        return loss / sequence_length

    # 数值梯度计算
    def numerical_gradient(params, eps=1e-5):
        W, b = params
        grad_W = np.zeros_like(W)
        grad_b = np.zeros_like(b)

        # 计算W的梯度
        for i in range(W.shape[0]):
            for j in range(W.shape[1]):
                W_plus = W.copy()
                W_plus[i, j] += eps
                W_minus = W.copy()
                W_minus[i, j] -= eps

                loss_plus = recurrent_loss([W_plus, b])
                loss_minus = recurrent_loss([W_minus, b])
                grad_W[i, j] = (loss_plus - loss_minus) / (2 * eps)

        # 计算b的梯度
        for i in range(b.shape[0]):
            b_plus = b.copy()
            b_plus[i] += eps
            b_minus = b.copy()
            b_minus[i] -= eps

            loss_plus = recurrent_loss([W, b_plus])
            loss_minus = recurrent_loss([W, b_minus])
            grad_b[i] = (loss_plus - loss_minus) / (2 * eps)

        return grad_W, grad_b

    # 梯度裁剪函数
    def clip_gradients(grads, max_norm):
        """梯度裁剪"""
        total_norm = 0
        for grad in grads:
            total_norm += np.sum(grad ** 2)
        total_norm = np.sqrt(total_norm)

        clip_coef = max_norm / (total_norm + 1e-8)
        clip_coef = min(clip_coef, 1.0)

        return [grad * clip_coef for grad in grads], total_norm

    # 初始化参数（使用较大的值容易爆炸）
    W = np.random.randn(5, 5) * 2
    b = np.random.randn(5) * 0.1

    # 测试有无梯度裁剪
    results = {'no_clip': [], 'with_clip': []}

    for method in ['no_clip', 'with_clip']:
        W_curr = W.copy()
        b_curr = b.copy()

        losses = []
        grad_norms = []

        for iteration in range(50):
            # 计算损失
            loss = recurrent_loss([W_curr, b_curr])
            losses.append(loss)

            # 计算梯度
            grad_W, grad_b = numerical_gradient([W_curr, b_curr])

            # 梯度裁剪
            if method == 'with_clip':
                grads, norm = clip_gradients([grad_W, grad_b], max_norm=1.0)
                grad_W, grad_b = grads
            else:
                norm = np.sqrt(np.sum(grad_W ** 2) + np.sum(grad_b ** 2))

            grad_norms.append(norm)

            # 更新参数
            lr = 0.01
            W_curr -= lr * grad_W
            b_curr -= lr * grad_b

            # 检查是否爆炸
            if np.isnan(loss) or loss > 1e10:
                print(f"{method}: 梯度爆炸在第 {iteration} 步!")
                break

        results[method] = {
            'losses': losses,
            'grad_norms': grad_norms
        }

    # 可视化
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    # 损失曲线
    ax1.semilogy(results['no_clip']['losses'], 'r-', label='无梯度裁剪', linewidth=2)
    ax1.semilogy(results['with_clip']['losses'], 'b-', label='有梯度裁剪', linewidth=2)
    ax1.set_xlabel('迭代次数')
    ax1.set_ylabel('损失值 (log scale)')
    ax1.set_title('梯度裁剪对训练稳定性的影响')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 梯度范数
    ax2.semilogy(results['no_clip']['grad_norms'], 'r-', label='无梯度裁剪', linewidth=2)
    ax2.semilogy(results['with_clip']['grad_norms'], 'b-', label='有梯度裁剪', linewidth=2)
    ax2.axhline(y=1.0, color='green', linestyle='--', label='裁剪阈值')
    ax2.set_xlabel('迭代次数')
    ax2.set_ylabel('梯度范数 (log scale)')
    ax2.set_title('梯度范数的演化')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    print("=== NumPy优化器实现与分析 ===\n")

    # 1. 2D优化可视化
    print("1. 在2D函数上比较不同优化器...")
    visualize_optimization_2d()

    # 2. 批量大小与噪声分析
    print("\n2. 分析批量大小对梯度噪声的影响...")
    batch_size_noise_analysis()

    # 3. 学习率范围测试
    print("\n3. 执行学习率范围测试...")
    best_lr = learning_rate_range_test()
    print(f"找到的最佳学习率: {best_lr:.4e}")

    # 4. 梯度裁剪演示
    print("\n4. 演示梯度裁剪的效果...")
    gradient_clipping_demo()

    print("\n分析完成！")</code></pre>
                </div>
            </div>

            <!-- 优化最佳实践总结 -->
            <div class="tip success mt-4">
                <span class="tip-icon">🎯</span>
                <div class="tip-content">
                    <strong>优化算法选择指南：</strong><br>
                    • <strong>默认选择</strong>：Adam (lr=0.001) 或 AdamW（带L2正则）<br>
                    • <strong>精细调优</strong>：SGD + Momentum + 学习率调度<br>
                    • <strong>大批量训练</strong>：LAMB 或 LARS<br>
                    • <strong>二阶信息</strong>：L-BFGS（小规模）或 K-FAC（特定任务）<br>
                    • <strong>关键超参数</strong>：学习率 > 批量大小 > 其他
                </div>
            </div>

            <!-- 常见优化陷阱 -->
            <div class="common-mistakes mt-4">
                <h3 style="color: var(--accent-red); margin-bottom: 1rem;">⚠️ 优化中的常见陷阱</h3>

                <div class="mistake-item">
                    <div class="mistake-icon">🎢</div>
                    <div class="mistake-content">
                        <h4>学习率过大</h4>
                        <p>症状：损失震荡或发散</p>
                        <p>解决：降低学习率，使用预热，或尝试梯度裁剪</p>
                    </div>
                </div>

                <div class="mistake-item">
                    <div class="mistake-icon">🐌</div>
                    <div class="mistake-content">
                        <h4>学习率过小</h4>
                        <p>症状：收敛极慢，陷入局部最优</p>
                        <p>解决：增大学习率，使用自适应优化器，或循环学习率</p>
                    </div>
                </div>

                <div class="mistake-item">
                    <div class="mistake-icon">💥</div>
                    <div class="mistake-content">
                        <h4>批量过大导致泛化差</h4>
                        <p>症状：训练精度高但测试精度低</p>
                        <p>解决：减小批量，增加噪声（Dropout），或使用特殊优化器</p>
                    </div>
                </div>
            </div>

            <!-- 深入阅读推荐 -->
            <div class="section-card" style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(236, 72, 153, 0.1)); margin-top: 2rem;">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📚 深入阅读推荐</h3>

                <div class="concept-grid">
                    <div class="concept-card what">
                        <h3>理论基础</h3>
                        <ul style="font-size: 0.9rem; line-height: 1.8;">
                            <li>Convex Optimization - Boyd</li>
                            <li>Optimization for ML - Bottou</li>
                            <li>Deep Learning - Goodfellow</li>
                        </ul>
                    </div>
                    <div class="concept-card how">
                        <h3>经典论文</h3>
                        <ul style="font-size: 0.9rem; line-height: 1.8;">
                            <li>Adam: Kingma & Ba 2014</li>
                            <li>Lookahead: Zhang 2019</li>
                            <li>RAdam: Liu 2019</li>
                        </ul>
                    </div>
                    <div class="concept-card why">
                        <h3>实践指南</h3>
                        <ul style="font-size: 0.9rem; line-height: 1.8;">
                            <li>Tricks for Training - Bengio</li>
                            <li>Super-Convergence - Smith</li>
                            <li>Large Batch Training</li>
                        </ul>
                    </div>
                    <div class="concept-card pitfall">
                        <h3>前沿研究</h3>
                        <ul style="font-size: 0.9rem; line-height: 1.8;">
                            <li>Sharpness-Aware Min</li>
                            <li>Loss Landscape</li>
                            <li>Neural Tangent Kernel</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        <!-- 实用技巧与经验 -->
        <section id="tricks" class="section-card">
            <h2 style="color: var(--accent-green); margin-bottom: 1.5rem;">
                🛠️ 实用技巧与经验
                <span class="depth-indicator beginner">初学者</span>
            </h2>

            <!-- 技巧集锦 -->
            <div class="concept-grid">
                <div class="concept-card what">
                    <h3 style="color: var(--accent-green);">🎯 初始化技巧</h3>
                    <ul style="line-height: 1.8;">
                        <li><strong>He初始化</strong>：ReLU网络首选</li>
                        <li><strong>Xavier初始化</strong>：Sigmoid/Tanh</li>
                        <li><strong>LSUV</strong>：逐层标准化初始化</li>
                        <li><strong>预训练初始化</strong>：迁移学习</li>
                    </ul>
                </div>
                <div class="concept-card how">
                    <h3 style="color: var(--accent-blue);">📈 训练技巧</h3>
                    <ul style="line-height: 1.8;">
                        <li><strong>学习率预热</strong>：避免初期震荡</li>
                        <li><strong>余弦退火</strong>：平滑降低学习率</li>
                        <li><strong>标签平滑</strong>：防止过度自信</li>
                        <li><strong>混合精度</strong>：加速训练</li>
                    </ul>
                </div>
                <div class="concept-card why">
                    <h3 style="color: var(--accent-red);">🔧 调试技巧</h3>
                    <ul style="line-height: 1.8;">
                        <li><strong>梯度检查</strong>：验证反向传播</li>
                        <li><strong>过拟合单批</strong>：检查模型能力</li>
                        <li><strong>可视化激活</strong>：发现死神经元</li>
                        <li><strong>监控统计</strong>：均值、方差等</li>
                    </ul>
                </div>
                <div class="concept-card pitfall">
                    <h3 style="color: var(--accent-yellow);">⚡ 优化技巧</h3>
                    <ul style="line-height: 1.8;">
                        <li><strong>批大小调整</strong>：平衡速度和稳定性</li>
                        <li><strong>梯度累积</strong>：模拟大批量</li>
                        <li><strong>数据并行</strong>：多GPU训练</li>
                        <li><strong>量化感知训练</strong>：部署优化</li>
                    </ul>
                </div>
            </div>

            <!-- 实践建议 -->
            <div class="tip success mt-4">
                <span class="tip-icon">💡</span>
                <div class="tip-content">
                    <strong>MLP设计的黄金法则：</strong><br>
                    1. <strong>从简单开始</strong>：先试3层，逐步加深<br>
                    2. <strong>监控梯度</strong>：确保健康的梯度流<br>
                    3. <strong>正则化组合</strong>：Dropout + L2 + BatchNorm<br>
                    4. <strong>耐心调参</strong>：学习率是最重要的超参数<br>
                    5. <strong>记录一切</strong>：实验日志很重要
                </div>
            </div>

            <!-- 常见问题诊断 -->
            <div class="common-mistakes mt-4">
                <h3 style="color: var(--accent-red); margin-bottom: 1rem;">🔍 问题诊断清单</h3>

                <div class="mistake-item">
                    <div class="mistake-icon">📉</div>
                    <div class="mistake-content">
                        <h4>训练损失不下降</h4>
                        <ul>
                            <li>检查学习率（可能太大或太小）</li>
                            <li>验证数据预处理</li>
                            <li>确认损失函数正确</li>
                            <li>尝试更简单的模型</li>
                        </ul>
                    </div>
                </div>

                <div class="mistake-item">
                    <div class="mistake-icon">📈</div>
                    <div class="mistake-content">
                        <h4>验证性能差</h4>
                        <ul>
                            <li>可能过拟合：加强正则化</li>
                            <li>数据不够：数据增强</li>
                            <li>模型太复杂：简化架构</li>
                            <li>特征问题：检查输入</li>
                        </ul>
                    </div>
                </div>

                <div class="mistake-item">
                    <div class="mistake-icon">🐌</div>
                    <div class="mistake-content">
                        <h4>训练太慢</h4>
                        <ul>
                            <li>批量大小太小</li>
                            <li>数据加载瓶颈</li>
                            <li>未使用GPU</li>
                            <li>模型太大</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- 总结 -->
        <section id="summary" class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">
                📝 总结：表达力的艺术
            </h2>

            <!-- 核心要点 -->
            <div style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(236, 72, 153, 0.1)); padding: 2rem; border-radius: 1rem; margin-bottom: 2rem;">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🎯 本章核心要点</h3>

                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 2rem;">
                    <div>
                        <h4 style="color: var(--accent-green); margin-bottom: 1rem;">✅ 我们学到了</h4>
                        <ul style="line-height: 2;">
                            <li>非线性激活函数赋予网络表达复杂模式的能力</li>
                            <li>通用逼近定理保证了网络的理论能力</li>
                            <li>不同激活函数各有特色和适用场景</li>
                            <li>深度vs宽度是效率vs简单的权衡</li>
                            <li>梯度问题有多种解决方案</li>
                        </ul>
                    </div>
                    <div>
                        <h4 style="color: var(--accent-blue); margin-bottom: 1rem;">🔑 关键见解</h4>
                        <ul style="line-height: 2;">
                            <li>线性的组合还是线性，必须有非线性</li>
                            <li>ReLU简单高效，是很好的默认选择</li>
                            <li>深度网络需要特殊技巧来训练</li>
                            <li>不同任务需要不同的网络设计</li>
                            <li>实践中的技巧和理论同样重要</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- 思维导图 -->
            <div style="text-align: center; margin: 2rem 0;">
                <h4 style="color: var(--accent-yellow); margin-bottom: 1rem;">MLP知识体系</h4>
                <canvas id="mindmap-canvas" width="800" height="600"></canvas>
            </div>

            <!-- 下一步学习 -->
            <div class="learning-path">
                <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🚀 接下来学什么？</h3>

                <div class="path-item">
                    <div class="path-icon">🎯</div>
                    <div class="path-content">
                        <h4>优化算法</h4>
                        <p>SGD、Adam、RMSprop...如何更好地训练网络</p>
                    </div>
                </div>
                <div class="path-item">
                    <div class="path-icon">🏗️</div>
                    <div class="path-content">
                        <h4>卷积神经网络</h4>
                        <p>专门为图像设计的网络架构</p>
                    </div>
                </div>
                <div class="path-item">
                    <div class="path-icon">🔄</div>
                    <div class="path-content">
                        <h4>循环神经网络</h4>
                        <p>处理序列数据的专门架构</p>
                    </div>
                </div>
                <div class="path-item">
                    <div class="path-icon">🤖</div>
                    <div class="path-content">
                        <h4>Transformer</h4>
                        <p>现代NLP和多模态AI的基础</p>
                    </div>
                </div>
            </div>

            <!-- 实践建议 -->
            <div class="tip info mt-4">
                <span class="tip-icon">🎓</span>
                <div class="tip-content">
                    <strong>继续学习的建议：</strong><br>
                    1. <strong>动手实践</strong>：用MLP解决一个真实问题<br>
                    2. <strong>对比实验</strong>：尝试不同的激活函数和架构<br>
                    3. <strong>阅读论文</strong>：了解最新的激活函数研究<br>
                    4. <strong>参与竞赛</strong>：Kaggle等平台练手<br>
                    5. <strong>开源贡献</strong>：改进现有的MLP实现
                </div>
            </div>

            <!-- 结语 -->
            <div style="background: var(--primary-gradient); padding: 2rem; border-radius: 1rem; margin-top: 2rem; color: white; text-align: center;">
                <h3 style="margin-bottom: 1rem;">🎊 恭喜完成本章！</h3>
                <p style="font-size: 1.1rem; opacity: 0.9; max-width: 800px; margin: 0 auto;">
                    你已经掌握了深度学习中最基础也是最重要的组件——多层感知机。
                    从线性到非线性，从浅层到深层，你理解了神经网络获得强大表达能力的秘密。
                    接下来，让我们继续探索更多激动人心的网络架构！
                </p>
                <button class="btn" style="background: white; color: var(--accent-purple); margin-top: 1.5rem;" onclick="location.href='#'" tabindex="0">
                    继续下一章 →
                </button>
            </div>
        </section>

        <!-- 学习反馈 -->
        <section class="section-card">
            <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">💬 学习反馈</h2>
            <div style="text-align: center;">
                <p style="font-size: 1.1rem; margin-bottom: 1.5rem;">这一章的内容对你有帮助吗？</p>
                <div style="display: flex; gap: 1rem; justify-content: center;">
                    <button class="btn btn-secondary" onclick="submitFeedback('helpful')" tabindex="0">
                        👍 很有帮助
                    </button>
                    <button class="btn btn-secondary" onclick="submitFeedback('ok')" tabindex="0">
                        😐 还可以
                    </button>
                    <button class="btn btn-secondary" onclick="submitFeedback('confusing')" tabindex="0">
                        😕 有点困惑
                    </button>
                </div>
                <div id="feedback-response" style="margin-top: 1rem; display: none;">
                    <p style="color: var(--accent-green);">感谢你的反馈！我们会继续改进。</p>
                </div>
            </div>
        </section>
    </div>
</main>

<!-- JavaScript代码 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<script>
    // 初始化代码高亮
    hljs.highlightAll();

    // 初始化数学公式渲染
    renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
        ]
    });

    // 全局变量
    let currentDifficulty = 'beginner';
    let buddyMessages = {
        beginner: [
            "激活函数就像调味料，让网络有了'味道'！",
            "ReLU虽然简单，但在大多数情况下效果很好。",
            "记住：没有激活函数，深度网络就是个摆设！"
        ],
        intermediate: [
            "梯度消失是深度网络的大敌，选对激活函数很重要。",
            "批归一化不仅加速训练，还能让网络更深。",
            "残差连接是深度网络的救星！"
        ],
        advanced: [
            "GELU在Transformer中表现优异，值得尝试。",
            "激活函数的选择会影响网络的归纳偏置。",
            "自适应激活函数是个有趣的研究方向。"
        ]
    };

    // 导航功能
    const sidebar = document.getElementById('sidebar');
    const sidebarOverlay = document.getElementById('sidebar-overlay');
    const toggleSidebarBtn = document.getElementById('toggle-sidebar');

    toggleSidebarBtn.addEventListener('click', () => {
        sidebar.classList.toggle('open');
        sidebarOverlay.classList.toggle('active');
    });

    sidebarOverlay.addEventListener('click', () => {
        sidebar.classList.remove('open');
        sidebarOverlay.classList.remove('active');
    });

    // 学习伙伴功能
    const learningBuddy = document.getElementById('learning-buddy');
    const toggleBuddyBtn = document.getElementById('toggle-buddy');
    const minimizeBuddyBtn = document.getElementById('minimize-buddy');
    const closeBuddyBtn = document.getElementById('close-buddy');
    const buddyAvatar = document.getElementById('buddy-avatar');

    toggleBuddyBtn.addEventListener('click', () => {
        learningBuddy.classList.toggle('active');
    });

    minimizeBuddyBtn.addEventListener('click', () => {
        learningBuddy.classList.toggle('minimized');
    });

    closeBuddyBtn.addEventListener('click', () => {
        learningBuddy.classList.remove('active');
    });

    buddyAvatar.addEventListener('click', () => {
        if (learningBuddy.classList.contains('minimized')) {
            learningBuddy.classList.remove('minimized');
        }
    });

    // 学习伙伴提示功能
    function getBuddyHint() {
        const messages = buddyMessages[currentDifficulty];
        const randomMessage = messages[Math.floor(Math.random() * messages.length)];
        updateBuddyMessage(randomMessage);
    }

    function getBuddySummary() {
        const summary = `
        本章核心要点：<br>
        1. 激活函数引入非线性，赋予网络表达力<br>
        2. 通用逼近定理保证了理论能力<br>
        3. 不同激活函数各有优劣<br>
        4. 深度vs宽度是个重要权衡<br>
        5. 梯度问题需要特殊技巧解决
    `;
        updateBuddyMessage(summary);
    }

    function updateBuddyMessage(message) {
        const messageElement = document.querySelector('.buddy-message p');
        messageElement.innerHTML = message;

        // 添加动画效果
        messageElement.style.animation = 'none';
        setTimeout(() => {
            messageElement.style.animation = 'fadeIn 0.5s ease';
        }, 10);
    }

    // 主题切换
    const toggleThemeBtn = document.getElementById('toggle-theme');
    toggleThemeBtn.addEventListener('click', () => {
        const currentTheme = document.body.getAttribute('data-theme');
        const newTheme = currentTheme === 'light' ? 'dark' : 'light';
        document.body.setAttribute('data-theme', newTheme);
        localStorage.setItem('theme', newTheme);
    });

    // 加载保存的主题
    const savedTheme = localStorage.getItem('theme') || 'dark';
    document.body.setAttribute('data-theme', savedTheme);

    // 难度选择功能
    const difficultyOptions = document.querySelectorAll('.difficulty-option');
    difficultyOptions.forEach(option => {
        option.addEventListener('click', () => {
            const difficulty = option.querySelector('input').value;
            currentDifficulty = difficulty;

            // 更新选中状态
            difficultyOptions.forEach(opt => opt.classList.remove('active'));
            option.classList.add('active');

            // 更新内容显示
            updateContentVisibility(difficulty);

            // 显示通知
            showNotification(`已切换到${option.textContent.trim()}模式`);
        });
    });

    function updateContentVisibility(difficulty) {
        // 根据难度显示/隐藏内容
        const beginnerContent = document.querySelectorAll('.beginner-content');
        const intermediateContent = document.querySelectorAll('.intermediate-content');
        const advancedContent = document.querySelectorAll('.advanced-content');

        // 默认显示所有初学者内容
        beginnerContent.forEach(el => el.style.display = 'block');

        if (difficulty === 'beginner') {
            intermediateContent.forEach(el => el.style.display = 'none');
            advancedContent.forEach(el => el.style.display = 'none');
        } else if (difficulty === 'intermediate') {
            intermediateContent.forEach(el => el.style.display = 'block');
            advancedContent.forEach(el => el.style.display = 'none');
        } else {
            intermediateContent.forEach(el => el.style.display = 'block');
            advancedContent.forEach(el => el.style.display = 'block');
        }
    }

    // 折叠功能
    const collapsibles = document.querySelectorAll('.collapsible-header');
    collapsibles.forEach(header => {
        header.addEventListener('click', () => {
            const collapsible = header.parentElement;
            collapsible.classList.toggle('expanded');
        });

        // 键盘支持
        header.addEventListener('keypress', (e) => {
            if (e.key === 'Enter' || e.key === ' ') {
                e.preventDefault();
                header.click();
            }
        });
    });

    // 滚动进度条
    function updateProgressBar() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        const scrollHeight = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        const scrollPercentage = (scrollTop / scrollHeight) * 100;
        document.getElementById('progress-bar').style.width = scrollPercentage + '%';
    }

    window.addEventListener('scroll', updateProgressBar);

    // 导航高亮
    const sections = document.querySelectorAll('section[id]');
    const navItems = document.querySelectorAll('.toc-item');

    function updateNavHighlight() {
        const scrollPosition = window.pageYOffset + 100;

        sections.forEach((section, index) => {
            const sectionTop = section.offsetTop;
            const sectionHeight = section.offsetHeight;

            if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                navItems.forEach(item => item.classList.remove('active'));
                if (navItems[index]) {
                    navItems[index].classList.add('active');
                }
            }
        });
    }

    window.addEventListener('scroll', updateNavHighlight);

    // 成就系统
    let completedSections = new Set();
    let achievements = {
        firstActivation: false,
        allActivations: false,
        deepDive: false,
        finishChapter: false
    };

    function checkAchievements() {
        const totalSections = sections.length;
        const completed = completedSections.size;

        if (completed === 1 && !achievements.firstActivation) {
            achievements.firstActivation = true;
            showAchievement("激活函数初探", "了解了第一个激活函数！");
        }

        if (completed === totalSections && !achievements.finishChapter) {
            achievements.finishChapter = true;
            showAchievement("MLP大师", "完成了多层感知机的学习！");
        }

        updateAchievementProgress(completed, totalSections);
    }

    function showAchievement(title, description) {
        const popup = document.getElementById('achievement-popup');
        const textElement = document.getElementById('achievement-text');

        textElement.textContent = description;
        popup.classList.add('show');

        setTimeout(() => {
            popup.classList.remove('show');
        }, 5000);
    }

    function updateAchievementProgress(completed, total) {
        const progress = (completed / total) * 100;
        document.getElementById('achievement-progress').style.width = progress + '%';
        document.getElementById('achievement-count').textContent = completed;
    }

    // XOR可视化
    class XORVisualizer {
        constructor() {
            this.canvas = document.getElementById('xor-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;
            this.layers = 1;

            // XOR数据
            this.data = [
                {x: [0, 0], y: 0, color: '#ef4444'},
                {x: [0, 1], y: 1, color: '#3b82f6'},
                {x: [1, 0], y: 1, color: '#3b82f6'},
                {x: [1, 1], y: 0, color: '#ef4444'}
            ];

            this.draw();
        }

        draw() {
            this.ctx.clearRect(0, 0, this.width, this.height);

            // 绘制坐标轴
            this.drawAxes();

            // 绘制决策边界（如果是多层）
            if (this.layers > 1) {
                this.drawDecisionBoundary();
            }

            // 绘制数据点
            this.drawDataPoints();

            // 绘制说明文字
            this.drawInfo();
        }

        drawAxes() {
            this.ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
            this.ctx.lineWidth = 1;

            // x轴
            this.ctx.beginPath();
            this.ctx.moveTo(50, this.height - 50);
            this.ctx.lineTo(this.width - 50, this.height - 50);
            this.ctx.stroke();

            // y轴
            this.ctx.beginPath();
            this.ctx.moveTo(50, 50);
            this.ctx.lineTo(50, this.height - 50);
            this.ctx.stroke();

            // 标签
            this.ctx.fillStyle = '#f1f5f9';
            this.ctx.font = '14px Arial';
            this.ctx.fillText('0', 45, this.height - 35);
            this.ctx.fillText('1', this.width - 55, this.height - 35);
            this.ctx.fillText('0', 35, this.height - 45);
            this.ctx.fillText('1', 35, 55);
            this.ctx.fillText('x₁', this.width - 40, this.height - 45);
            this.ctx.fillText('x₂', 45, 40);
        }

        drawDataPoints() {
            const scale = this.width - 100;

            this.data.forEach(point => {
                const x = 50 + point.x[0] * scale;
                const y = this.height - 50 - point.x[1] * scale;

                this.ctx.fillStyle = point.color;
                this.ctx.strokeStyle = '#1f2937';
                this.ctx.lineWidth = 3;

                this.ctx.beginPath();
                this.ctx.arc(x, y, 15, 0, Math.PI * 2);
                this.ctx.fill();
                this.ctx.stroke();

                // 标注
                this.ctx.fillStyle = '#f1f5f9';
                this.ctx.font = '12px Arial';
                this.ctx.textAlign = 'center';
                this.ctx.fillText(`(${point.x[0]},${point.x[1]})`, x, y + 30);
            });
        }

        drawDecisionBoundary() {
            if (this.layers === 2) {
                // 两层网络可以画出XOR的决策边界
                this.ctx.strokeStyle = '#22c55e';
                this.ctx.lineWidth = 3;

                // 第一条线
                this.ctx.beginPath();
                this.ctx.moveTo(50, this.height - 100);
                this.ctx.lineTo(100, this.height - 50);
                this.ctx.stroke();

                // 第二条线
                this.ctx.beginPath();
                this.ctx.moveTo(this.width - 100, this.height - 50);
                this.ctx.lineTo(this.width - 50, this.height - 100);
                this.ctx.stroke();
            }
        }

        drawInfo() {
            this.ctx.fillStyle = '#f1f5f9';
            this.ctx.font = '16px Arial';
            this.ctx.textAlign = 'center';

            if (this.layers === 1) {
                this.ctx.fillText('单层感知机：无法用一条直线分开！', this.width / 2, 30);
            } else {
                this.ctx.fillText('多层网络：可以学习非线性边界！', this.width / 2, 30);
            }
        }

        addLayer() {
            this.layers = Math.min(this.layers + 1, 2);
            this.draw();
            return this.layers;
        }

        reset() {
            this.layers = 1;
            this.draw();
        }
    }

    const xorViz = new XORVisualizer();

    function addLayer() {
        const layers = xorViz.addLayer();
        document.getElementById('layer-count').textContent = layers;

        if (layers === 2) {
            showNotification('🎉 成功！多层网络可以解决XOR问题！');
        }
    }

    function resetXOR() {
        xorViz.reset();
        document.getElementById('layer-count').textContent = '1';
    }

    // 模式生成器
    class PatternGenerator {
        constructor() {
            this.canvas = document.getElementById('pattern-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;

            this.generatePattern();
        }

        generatePattern() {
            const pattern = document.getElementById('pattern-select').value;
            const noise = parseInt(document.getElementById('noise-slider').value) / 100;

            this.data = this.createPattern(pattern, noise);
            this.draw();
            this.updateAccuracy();
        }

        createPattern(type, noise) {
            const data = [];
            const n = 200;

            switch(type) {
                case 'xor':
                    // XOR模式
                    for (let i = 0; i < n; i++) {
                        const x = Math.random() * 2 - 1;
                        const y = Math.random() * 2 - 1;
                        const label = (x > 0 && y > 0) || (x < 0 && y < 0) ? 1 : -1;
                        data.push({
                            x: x + (Math.random() - 0.5) * noise,
                            y: y + (Math.random() - 0.5) * noise,
                            label: label
                        });
                    }
                    break;

                case 'circles':
                    // 同心圆
                    for (let i = 0; i < n; i++) {
                        const angle = Math.random() * 2 * Math.PI;
                        const r = i < n/2 ? 0.3 : 0.7;
                        const x = r * Math.cos(angle) + (Math.random() - 0.5) * noise;
                        const y = r * Math.sin(angle) + (Math.random() - 0.5) * noise;
                        data.push({
                            x: x,
                            y: y,
                            label: i < n/2 ? 1 : -1
                        });
                    }
                    break;

                case 'spiral':
                    // 螺旋线
                    for (let i = 0; i < n; i++) {
                        const t = i / n * 4 * Math.PI;
                        const r = t / (4 * Math.PI) * 0.8;
                        const label = i < n/2 ? 1 : -1;
                        const offset = label > 0 ? 0 : Math.PI;
                        const x = r * Math.cos(t + offset) + (Math.random() - 0.5) * noise;
                        const y = r * Math.sin(t + offset) + (Math.random() - 0.5) * noise;
                        data.push({x, y, label});
                    }
                    break;

                case 'moons':
                    // 月牙形
                    for (let i = 0; i < n; i++) {
                        const label = i < n/2 ? 1 : -1;
                        const angle = Math.random() * Math.PI;
                        if (label > 0) {
                            const x = Math.cos(angle) * 0.6 + (Math.random() - 0.5) * noise;
                            const y = Math.sin(angle) * 0.6 + (Math.random() - 0.5) * noise;
                            data.push({x, y, label});
                        } else {
                            const x = Math.cos(angle) * 0.6 + 0.3 + (Math.random() - 0.5) * noise;
                            const y = -Math.sin(angle) * 0.6 + 0.3 + (Math.random() - 0.5) * noise;
                            data.push({x, y, label});
                        }
                    }
                    break;
            }

            return data;
        }

        draw() {
            this.ctx.clearRect(0, 0, this.width, this.height);

            // 绘制网格背景
            this.drawGrid();

            // 绘制线性决策边界
            this.drawLinearBoundary();

            // 绘制数据点
            this.data.forEach(point => {
                const x = (point.x + 1) * this.width / 2;
                const y = (1 - point.y) * this.height / 2;

                this.ctx.fillStyle = point.label > 0 ? '#3b82f6' : '#ef4444';
                this.ctx.beginPath();
                this.ctx.arc(x, y, 5, 0, Math.PI * 2);
                this.ctx.fill();
            });
        }

        drawGrid() {
            this.ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';
            this.ctx.lineWidth = 1;

            for (let i = 0; i <= 10; i++) {
                const x = (i / 10) * this.width;
                const y = (i / 10) * this.height;

                this.ctx.beginPath();
                this.ctx.moveTo(x, 0);
                this.ctx.lineTo(x, this.height);
                this.ctx.stroke();

                this.ctx.beginPath();
                this.ctx.moveTo(0, y);
                this.ctx.lineTo(this.width, y);
                this.ctx.stroke();
            }
        }

        drawLinearBoundary() {
            // 简单的线性分类器（仅作示意）
            this.ctx.strokeStyle = '#ef4444';
            this.ctx.lineWidth = 3;
            this.ctx.setLineDash([10, 10]);

            this.ctx.beginPath();
            this.ctx.moveTo(0, this.height / 2);
            this.ctx.lineTo(this.width, this.height / 2);
            this.ctx.stroke();

            this.ctx.setLineDash([]);
        }

        updateAccuracy() {
            // 计算线性分类器的准确率（简化版）
            let correct = 0;
            this.data.forEach(point => {
                const predicted = point.y > 0 ? 1 : -1;
                if (predicted === point.label) correct++;
            });

            const linearAcc = (correct / this.data.length * 100).toFixed(1);
            document.getElementById('linear-accuracy').textContent = linearAcc + '%';

            // MLP准确率（模拟）
            const mlpAcc = Math.min(95 + Math.random() * 4, 99.9).toFixed(1);
            document.getElementById('mlp-accuracy').textContent = mlpAcc + '%';
        }
    }

    const patternGen = new PatternGenerator();

    function generatePattern() {
        patternGen.generatePattern();
    }

    // 噪声滑块更新
    document.getElementById('noise-slider').addEventListener('input', (e) => {
        document.getElementById('noise-value').textContent = e.target.value + '%';
    });

    // 迷你图案可视化
    function drawMiniPatterns() {
        const patterns = [
            { id: 'mini-xor', type: 'xor' },
            { id: 'mini-circles', type: 'circles' },
            { id: 'mini-spiral', type: 'spiral' },
            { id: 'mini-moons', type: 'moons' }
        ];

        patterns.forEach(pattern => {
            const canvas = document.getElementById(pattern.id);
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;

            // 清空画布
            ctx.clearRect(0, 0, width, height);

            // 生成数据
            const data = generateMiniData(pattern.type, 50);

            // 绘制数据点
            data.forEach(point => {
                const x = (point.x + 1) * width / 2;
                const y = (1 - point.y) * height / 2;

                ctx.fillStyle = point.label > 0 ? '#3b82f6' : '#ef4444';
                ctx.beginPath();
                ctx.arc(x, y, 3, 0, Math.PI * 2);
                ctx.fill();
            });
        });
    }

    function generateMiniData(type, n) {
        const data = [];

        switch(type) {
            case 'xor':
                for (let i = 0; i < n; i++) {
                    const x = Math.random() * 2 - 1;
                    const y = Math.random() * 2 - 1;
                    const label = (x > 0 && y > 0) || (x < 0 && y < 0) ? 1 : -1;
                    data.push({x, y, label});
                }
                break;

            case 'circles':
                for (let i = 0; i < n; i++) {
                    const angle = Math.random() * 2 * Math.PI;
                    const r = i < n/2 ? 0.3 : 0.7;
                    data.push({
                        x: r * Math.cos(angle),
                        y: r * Math.sin(angle),
                        label: i < n/2 ? 1 : -1
                    });
                }
                break;

            case 'spiral':
                for (let i = 0; i < n; i++) {
                    const t = i / n * 4 * Math.PI;
                    const r = t / (4 * Math.PI) * 0.8;
                    const label = i < n/2 ? 1 : -1;
                    const offset = label > 0 ? 0 : Math.PI;
                    data.push({
                        x: r * Math.cos(t + offset),
                        y: r * Math.sin(t + offset),
                        label
                    });
                }
                break;

            case 'moons':
                for (let i = 0; i < n; i++) {
                    const label = i < n/2 ? 1 : -1;
                    const angle = Math.random() * Math.PI;
                    if (label > 0) {
                        data.push({
                            x: Math.cos(angle) * 0.6,
                            y: Math.sin(angle) * 0.6,
                            label
                        });
                    } else {
                        data.push({
                            x: Math.cos(angle) * 0.6 + 0.3,
                            y: -Math.sin(angle) * 0.6 + 0.3,
                            label
                        });
                    }
                }
                break;
        }

        return data;
    }

    // 初始化迷你图案
    drawMiniPatterns();

    // 通用逼近演示
    class ApproximationDemo {
        constructor() {
            this.canvas = document.getElementById('approximation-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;

            this.targetFunction = 'sine';
            this.neurons = 10;
            this.customPoints = [];

            this.draw();
        }

        draw() {
            this.ctx.clearRect(0, 0, this.width, this.height);

            // 绘制坐标轴
            this.drawAxes();

            // 绘制目标函数
            this.drawTargetFunction();

            // 绘制网络输出
            this.drawNetworkOutput();

            // 绘制图例
            this.drawLegend();
        }

        drawAxes() {
            this.ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
            this.ctx.lineWidth = 1;

            // x轴
            this.ctx.beginPath();
            this.ctx.moveTo(50, this.height / 2);
            this.ctx.lineTo(this.width - 50, this.height / 2);
            this.ctx.stroke();

            // y轴
            this.ctx.beginPath();
            this.ctx.moveTo(this.width / 2, 50);
            this.ctx.lineTo(this.width / 2, this.height - 50);
            this.ctx.stroke();
        }

        drawTargetFunction() {
            this.ctx.strokeStyle = '#3b82f6';
            this.ctx.lineWidth = 3;

            this.ctx.beginPath();
            for (let x = -1; x <= 1; x += 0.01) {
                const y = this.getTargetValue(x);
                const px = (x + 1) * (this.width - 100) / 2 + 50;
                const py = this.height / 2 - y * (this.height - 100) / 2;

                if (x === -1) {
                    this.ctx.moveTo(px, py);
                } else {
                    this.ctx.lineTo(px, py);
                }
            }
            this.ctx.stroke();
        }

        drawNetworkOutput() {
            this.ctx.strokeStyle = '#22c55e';
            this.ctx.lineWidth = 2;

            // 模拟网络输出（简化版）
            this.ctx.beginPath();
            for (let x = -1; x <= 1; x += 0.01) {
                const y = this.getApproximation(x);
                const px = (x + 1) * (this.width - 100) / 2 + 50;
                const py = this.height / 2 - y * (this.height - 100) / 2;

                if (x === -1) {
                    this.ctx.moveTo(px, py);
                } else {
                    this.ctx.lineTo(px, py);
                }
            }
            this.ctx.stroke();
        }

        drawLegend() {
            // 已在HTML中实现
        }

        getTargetValue(x) {
            switch(this.targetFunction) {
                case 'sine':
                    return Math.sin(x * Math.PI * 2) * 0.8;
                case 'step':
                    return x > 0 ? 0.8 : -0.8;
                case 'abs':
                    return Math.abs(x) * 0.8;
                case 'custom':
                    // 从自定义点插值
                    return this.interpolateCustom(x);
                default:
                    return 0;
            }
        }

        getApproximation(x) {
            // 简化的神经网络近似
            let sum = 0;
            for (let i = 0; i < this.neurons; i++) {
                const center = -1 + (i / (this.neurons - 1)) * 2;
                const width = 2 / this.neurons;
                const activation = Math.max(0, 1 - Math.abs(x - center) / width);
                sum += activation * Math.sin(i * 0.5);
            }

            const target = this.getTargetValue(x);
            const approx = sum / this.neurons * 2 - 1;

            // 随着神经元增加，逼近效果更好
            const factor = Math.min(this.neurons / 50, 1);
            return target * (1 - factor) + approx * factor;
        }

        interpolateCustom(x) {
            if (this.customPoints.length < 2) return 0;

            // 简单线性插值
            let prevPoint = null;
            let nextPoint = null;

            for (let i = 0; i < this.customPoints.length; i++) {
                if (this.customPoints[i].x <= x) {
                    prevPoint = this.customPoints[i];
                }
                if (this.customPoints[i].x >= x && !nextPoint) {
                    nextPoint = this.customPoints[i];
                }
            }

            if (!prevPoint) return nextPoint ? nextPoint.y : 0;
            if (!nextPoint) return prevPoint.y;

            const t = (x - prevPoint.x) / (nextPoint.x - prevPoint.x);
            return prevPoint.y + t * (nextPoint.y - prevPoint.y);
        }

        setTargetFunction(func) {
            this.targetFunction = func;
            if (func === 'custom') {
                this.setupCustomDrawing();
            }
            this.draw();
        }

        setNeurons(n) {
            this.neurons = n;
            this.draw();
            this.updateLoss();
        }

        setupCustomDrawing() {
            this.customPoints = [];

            const handleMouseDown = (e) => {
                const rect = this.canvas.getBoundingClientRect();
                const x = (e.clientX - rect.left) / rect.width * 2 - 1;
                const y = 1 - (e.clientY - rect.top) / rect.height * 2;

                this.customPoints.push({x, y});
                this.customPoints.sort((a, b) => a.x - b.x);
                this.draw();
            };

            this.canvas.addEventListener('mousedown', handleMouseDown);
        }

        updateLoss() {
            let loss = 0;
            const samples = 100;

            for (let i = 0; i < samples; i++) {
                const x = -1 + (i / (samples - 1)) * 2;
                const target = this.getTargetValue(x);
                const approx = this.getApproximation(x);
                loss += Math.pow(target - approx, 2);
            }

            loss = loss / samples;
            document.getElementById('approx-loss').textContent = loss.toFixed(4);
        }

        train() {
            // 动画训练过程
            let iteration = 0;
            const maxIterations = 50;

            const animate = () => {
                if (iteration < maxIterations) {
                    this.neurons = Math.min(this.neurons + 1, 100);
                    this.draw();
                    this.updateLoss();

                    document.getElementById('neuron-value').textContent = this.neurons;
                    document.getElementById('neuron-slider').value = this.neurons;

                    iteration++;
                    requestAnimationFrame(animate);
                }
            };

            animate();
        }

        reset() {
            this.neurons = 10;
            this.customPoints = [];
            document.getElementById('neuron-slider').value = 10;
            document.getElementById('neuron-value').textContent = '10';
            this.draw();
            this.updateLoss();
        }
    }

    const approxDemo = new ApproximationDemo();

    // 目标函数选择
    document.getElementById('target-function').addEventListener('change', (e) => {
        approxDemo.setTargetFunction(e.target.value);
    });

    // 神经元数量滑块
    document.getElementById('neuron-slider').addEventListener('input', (e) => {
        const value = parseInt(e.target.value);
        document.getElementById('neuron-value').textContent = value;
        approxDemo.setNeurons(value);
    });

    function trainApproximator() {
        approxDemo.train();
    }

    function resetApproximator() {
        approxDemo.reset();
    }

    // 激活函数可视化
    class ActivationVisualizer {
        constructor() {
            this.canvas = document.getElementById('activation-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;
            this.currentActivation = 'sigmoid';

            this.functions = {
                sigmoid: {
                    func: x => 1 / (1 + Math.exp(-x)),
                    derivative: x => {
                        const s = 1 / (1 + Math.exp(-x));
                        return s * (1 - s);
                    },
                    range: [-5, 5],
                    info: {
                        name: 'Sigmoid函数',
                        formula: 'σ(x) = 1 / (1 + e^(-x))',
                        derivative: "σ'(x) = σ(x)(1 - σ(x))",
                        pros: ['输出范围 (0, 1)', '平滑可微'],
                        cons: ['容易饱和', '输出不是零中心']
                    }
                },
                tanh: {
                    func: x => Math.tanh(x),
                    derivative: x => 1 - Math.tanh(x) ** 2,
                    range: [-5, 5],
                    info: {
                        name: 'Tanh函数',
                        formula: 'tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))',
                        derivative: "tanh'(x) = 1 - tanh²(x)",
                        pros: ['输出范围 (-1, 1)', '零中心', '比Sigmoid收敛快'],
                        cons: ['仍然容易饱和', '计算量较大']
                    }
                },
                relu: {
                    func: x => Math.max(0, x),
                    derivative: x => x > 0 ? 1 : 0,
                    range: [-2, 5],
                    info: {
                        name: 'ReLU函数',
                        formula: 'ReLU(x) = max(0, x)',
                        derivative: "ReLU'(x) = 1 if x > 0, else 0",
                        pros: ['计算简单', '缓解梯度消失', '稀疏激活'],
                        cons: ['死亡ReLU问题', '输出无界']
                    }
                },
                leaky: {
                    func: x => x > 0 ? x : 0.01 * x,
                    derivative: x => x > 0 ? 1 : 0.01,
                    range: [-5, 5],
                    info: {
                        name: 'Leaky ReLU函数',
                        formula: 'LeakyReLU(x) = max(0.01x, x)',
                        derivative: "LeakyReLU'(x) = 1 if x > 0, else 0.01",
                        pros: ['解决死亡ReLU', '计算简单'],
                        cons: ['负数部分斜率需要调参']
                    }
                },
                elu: {
                    func: x => x > 0 ? x : Math.exp(x) - 1,
                    derivative: x => x > 0 ? 1 : Math.exp(x),
                    range: [-5, 5],
                    info: {
                        name: 'ELU函数',
                        formula: 'ELU(x) = x if x > 0, else α(e^x - 1)',
                        derivative: "ELU'(x) = 1 if x > 0, else αe^x",
                        pros: ['负数部分平滑', '输出接近零中心'],
                        cons: ['计算开销较大']
                    }
                },
                swish: {
                    func: x => x / (1 + Math.exp(-x)),
                    derivative: x => {
                        const s = 1 / (1 + Math.exp(-x));
                        return s + x * s * (1 - s);
                    },
                    range: [-5, 5],
                    info: {
                        name: 'Swish函数',
                        formula: 'Swish(x) = x · σ(x)',
                        derivative: "Swish'(x) = σ(x) + x·σ(x)·(1-σ(x))",
                        pros: ['平滑非单调', '自门控机制', 'SOTA性能'],
                        cons: ['计算复杂']
                    }
                },
                gelu: {
                    func: x => 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * x**3))),
                    derivative: x => {
                        const a = Math.sqrt(2 / Math.PI);
                        const b = x + 0.044715 * x**3;
                        const tanh_b = Math.tanh(a * b);
                        const sech2_b = 1 - tanh_b**2;
                        return 0.5 * (1 + tanh_b) + 0.5 * x * sech2_b * a * (1 + 0.134145 * x**2);
                    },
                    range: [-5, 5],
                    info: {
                        name: 'GELU函数',
                        formula: 'GELU(x) ≈ 0.5x(1 + tanh[√(2/π)(x + 0.044715x³)])',
                        derivative: '复杂（见代码）',
                        pros: ['Transformer标配', '概率解释', '性能优异'],
                        cons: ['计算最复杂']
                    }
                }
            };

            this.drawAll();
        }

        drawAll() {
            this.ctx.clearRect(0, 0, this.width, this.height);

            if (this.currentActivation === 'all') {
                this.drawComparison();
            } else {
                this.drawSingle(this.currentActivation);
                this.updateInfo(this.currentActivation);
            }
        }

        drawSingle(activationName) {
            const activation = this.functions[activationName];
            const [xMin, xMax] = activation.range;

            // 绘制坐标轴
            this.drawAxes(xMin, xMax);

            // 绘制函数
            this.ctx.strokeStyle = '#3b82f6';
            this.ctx.lineWidth = 3;
            this.ctx.beginPath();

            for (let x = xMin; x <= xMax; x += 0.05) {
                const y = activation.func(x);
                const px = this.transformX(x, xMin, xMax);
                const py = this.transformY(y);

                if (x === xMin) {
                    this.ctx.moveTo(px, py);
                } else {
                    this.ctx.lineTo(px, py);
                }
            }
            this.ctx.stroke();

            // 绘制导数
            this.ctx.strokeStyle = '#ef4444';
            this.ctx.lineWidth = 2;
            this.ctx.setLineDash([5, 5]);
            this.ctx.beginPath();

            for (let x = xMin; x <= xMax; x += 0.05) {
                const y = activation.derivative(x);
                const px = this.transformX(x, xMin, xMax);
                const py = this.transformY(y);

                if (x === xMin) {
                    this.ctx.moveTo(px, py);
                } else {
                    this.ctx.lineTo(px, py);
                }
            }
            this.ctx.stroke();
            this.ctx.setLineDash([]);
        }

        drawComparison() {
            const xMin = -5, xMax = 5;
            this.drawAxes(xMin, xMax);

            const colors = ['#3b82f6', '#ef4444', '#22c55e', '#f59e0b', '#8b5cf6', '#ec4899', '#06b6d4'];

            Object.entries(this.functions).forEach(([name, activation], index) => {
                this.ctx.strokeStyle = colors[index % colors.length];
                this.ctx.lineWidth = 2;
                this.ctx.beginPath();

                for (let x = xMin; x <= xMax; x += 0.05) {
                    const y = activation.func(x);
                    const px = this.transformX(x, xMin, xMax);
                    const py = this.transformY(y);

                    if (x === xMin) {
                        this.ctx.moveTo(px, py);
                    } else {
                        this.ctx.lineTo(px, py);
                    }
                }
                this.ctx.stroke();
            });
        }

        drawAxes(xMin, xMax) {
            this.ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
            this.ctx.lineWidth = 1;

            // x轴
            this.ctx.beginPath();
            this.ctx.moveTo(50, this.height / 2);
            this.ctx.lineTo(this.width - 50, this.height / 2);
            this.ctx.stroke();

            // y轴
            this.ctx.beginPath();
            this.ctx.moveTo(this.width / 2, 50);
            this.ctx.lineTo(this.width / 2, this.height - 50);
            this.ctx.stroke();

            // 刻度
            this.ctx.fillStyle = '#f1f5f9';
            this.ctx.font = '12px Arial';
            this.ctx.textAlign = 'center';

            // x轴刻度
            for (let x = Math.ceil(xMin); x <= Math.floor(xMax); x++) {
                const px = this.transformX(x, xMin, xMax);
                this.ctx.fillText(x.toString(), px, this.height / 2 + 20);
            }

            // y轴刻度
            this.ctx.textAlign = 'right';
            for (let y = -2; y <= 2; y++) {
                const py = this.transformY(y);
                if (Math.abs(py - this.height / 2) > 10) {
                    this.ctx.fillText(y.toString(), this.width / 2 - 10, py + 5);
                }
            }
        }

        transformX(x, xMin, xMax) {
            return 50 + (x - xMin) / (xMax - xMin) * (this.width - 100);
        }

        transformY(y) {
            // y范围固定在[-3, 3]
            return this.height / 2 - y * (this.height - 100) / 6;
        }

        updateInfo(activationName) {
            const info = this.functions[activationName].info;
            const infoElement = document.getElementById('activation-info');

            infoElement.innerHTML = `
                <h4 style="color: var(--accent-purple); margin-bottom: 1rem;">${info.name}</h4>
                <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 2rem;">
                    <div>
                        <p><strong>数学形式：</strong></p>
                        <div class="equation-container">
                            <span>${info.formula}</span>
                        </div>
                        <p><strong>导数：</strong></p>
                        <div class="equation-container">
                            <span>${info.derivative}</span>
                        </div>
                    </div>
                    <div>
                        <p><strong>特性：</strong></p>
                        <ul style="line-height: 1.8;">
                            ${info.pros.map(pro => `<li>✅ ${pro}</li>`).join('')}
                            ${info.cons.map(con => `<li>❌ ${con}</li>`).join('')}
                        </ul>
                    </div>
                </div>
            `;
        }

        setActivation(name) {
            this.currentActivation = name;
            this.drawAll();
        }
    }

    const activationViz = new ActivationVisualizer();

    // 激活函数按钮点击事件
    document.querySelectorAll('.activation-btn').forEach(btn => {
        btn.addEventListener('click', () => {
            document.querySelectorAll('.activation-btn').forEach(b => b.classList.remove('active'));
            btn.classList.add('active');
            activationViz.setActivation(btn.dataset.activation);
        });
    });

    // 深度vs宽度实验
    class ArchitectureComparison {
        constructor() {
            this.configs = {
                'shallow-wide': { layers: [2, 1000, 2], name: '浅而宽' },
                'medium': { layers: [2, 100, 100, 100, 2], name: '中等深度' },
                'deep-narrow': { layers: [2, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 2], name: '深而窄' },
                'custom': { layers: [2, 50, 50, 50, 50, 50, 2], name: '自定义' }
            };

            this.visCanvas = document.getElementById('architecture-vis');
            this.perfCanvas = document.getElementById('performance-chart');
            this.visCtx = this.visCanvas.getContext('2d');
            this.perfCtx = this.perfCanvas.getContext('2d');

            this.currentConfig = 'medium';

            this.draw();
        }

        draw() {
            this.drawArchitecture();
            this.drawPerformance();
            this.updateStats();
        }

        drawArchitecture() {
            const ctx = this.visCtx;
            const width = this.visCanvas.width;
            const height = this.visCanvas.height;

            ctx.clearRect(0, 0, width, height);

            const layers = this.configs[this.currentConfig].layers;
            const maxNeurons = Math.max(...layers);
            const layerSpacing = width / (layers.length + 1);

            // 绘制每层
            layers.forEach((neurons, layerIdx) => {
                const x = layerSpacing * (layerIdx + 1);
                const neuronSpacing = Math.min(height / (neurons + 1), 30);
                const startY = (height - neuronSpacing * (neurons - 1)) / 2;

                // 绘制神经元
                for (let i = 0; i < Math.min(neurons, 20); i++) {
                    const y = startY + i * neuronSpacing;

                    // 连接到下一层
                    if (layerIdx < layers.length - 1) {
                        const nextNeurons = Math.min(layers[layerIdx + 1], 20);
                        const nextX = layerSpacing * (layerIdx + 2);
                        const nextSpacing = Math.min(height / (layers[layerIdx + 1] + 1), 30);
                        const nextStartY = (height - nextSpacing * (nextNeurons - 1)) / 2;

                        for (let j = 0; j < nextNeurons; j += 2) {
                            const nextY = nextStartY + j * nextSpacing;

                            ctx.strokeStyle = 'rgba(139, 92, 246, 0.1)';
                            ctx.lineWidth = 0.5;
                            ctx.beginPath();
                            ctx.moveTo(x, y);
                            ctx.lineTo(nextX, nextY);
                            ctx.stroke();
                        }
                    }

                    // 绘制神经元
                    ctx.fillStyle = this.getNeuronColor(layerIdx, layers.length);
                    ctx.beginPath();
                    ctx.arc(x, y, 5, 0, Math.PI * 2);
                    ctx.fill();
                }

                // 如果神经元太多，显示省略号
                if (neurons > 20) {
                    ctx.fillStyle = '#f1f5f9';
                    ctx.font = '16px Arial';
                    ctx.textAlign = 'center';
                    ctx.fillText('...', x, height / 2);
                    ctx.font = '12px Arial';
                    ctx.fillText(`${neurons}个`, x, height / 2 + 20);
                }

                // 层标签
                ctx.fillStyle = '#f1f5f9';
                ctx.font = '12px Arial';
                ctx.textAlign = 'center';
                if (layerIdx === 0) {
                    ctx.fillText('输入层', x, height - 20);
                } else if (layerIdx === layers.length - 1) {
                    ctx.fillText('输出层', x, height - 20);
                } else {
                    ctx.fillText(`隐藏层${layerIdx}`, x, height - 20);
                }
            });
        }

        getNeuronColor(layerIdx, totalLayers) {
            const hue = (layerIdx / totalLayers) * 60 + 200; // 从蓝到紫
            return `hsl(${hue}, 70%, 60%)`;
        }

        drawPerformance() {
            const ctx = this.perfCtx;
            const width = this.perfCanvas.width;
            const height = this.perfCanvas.height;

            ctx.clearRect(0, 0, width, height);

            // 模拟的性能数据
            const data = {
                'shallow-wide': { trainTime: 100, accuracy: 85, params: 2004 },
                'medium': { trainTime: 150, accuracy: 92, params: 21002 },
                'deep-narrow': { trainTime: 200, accuracy: 94, params: 4222 },
                'custom': { trainTime: 120, accuracy: 90, params: 12752 }
            };

            const metrics = ['训练时间', '准确率', '参数量'];
            const colors = ['#ef4444', '#22c55e', '#3b82f6'];

            // 绘制条形图
            const barWidth = width / 4 / 3;
            const barSpacing = width / 4;

            Object.entries(data).forEach(([config, values], configIdx) => {
                const x = barSpacing * (configIdx + 0.5);

                // 训练时间（归一化）
                const trainHeight = (values.trainTime / 200) * (height - 100);
                ctx.fillStyle = colors[0];
                ctx.fillRect(x - barWidth * 1.5, height - 50 - trainHeight, barWidth * 0.8, trainHeight);

                // 准确率
                const accHeight = (values.accuracy / 100) * (height - 100);
                ctx.fillStyle = colors[1];
                ctx.fillRect(x - barWidth * 0.5, height - 50 - accHeight, barWidth * 0.8, accHeight);

                // 参数量（对数尺度）
                const paramHeight = (Math.log10(values.params) / 5) * (height - 100);
                ctx.fillStyle = colors[2];
                ctx.fillRect(x + barWidth * 0.5, height - 50 - paramHeight, barWidth * 0.8, paramHeight);

                // 配置标签
                ctx.fillStyle = '#f1f5f9';
                ctx.font = '10px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(this.configs[config].name, x, height - 30);
            });

            // 图例
            ctx.font = '12px Arial';
            metrics.forEach((metric, idx) => {
                ctx.fillStyle = colors[idx];
                ctx.fillRect(20, 20 + idx * 25, 15, 15);
                ctx.fillStyle = '#f1f5f9';
                ctx.textAlign = 'left';
                ctx.fillText(metric, 40, 32 + idx * 25);
            });
        }

        updateStats() {
            const config = this.configs[this.currentConfig];
            const layers = config.layers;

            // 计算参数量
            let params = 0;
            for (let i = 0; i < layers.length - 1; i++) {
                params += layers[i] * layers[i + 1] + layers[i + 1]; // 权重 + 偏置
            }

            // 更新显示
            document.getElementById('param-count').textContent = params.toLocaleString();
            document.getElementById('train-speed').textContent = Math.round(100 + params / 100) + 'ms';
            document.getElementById('accuracy').textContent = (85 + Math.min(layers.length * 2, 10)) + '%';
            document.getElementById('generalization').textContent = (80 + Math.min(layers.length * 1.5, 8)) + '%';
        }

        setConfig(config) {
            this.currentConfig = config;
            this.draw();
        }

        setCustom(depth, width) {
            const layers = [2];
            for (let i = 0; i < depth; i++) {
                layers.push(width);
            }
            layers.push(2);

            this.configs.custom.layers = layers;
            this.currentConfig = 'custom';
            this.draw();
        }
    }

    const archComparison = new ArchitectureComparison();

    // 网络配置选择
    document.getElementById('network-config').addEventListener('change', (e) => {
        const config = e.target.value;

        if (config === 'custom') {
            document.getElementById('custom-controls').style.display = 'flex';
        } else {
            document.getElementById('custom-controls').style.display = 'none';
            archComparison.setConfig(config);
        }
    });

    // 自定义网络参数
    document.getElementById('custom-depth').addEventListener('change', updateCustomNetwork);
    document.getElementById('custom-width').addEventListener('change', updateCustomNetwork);

    function updateCustomNetwork() {
        const depth = parseInt(document.getElementById('custom-depth').value);
        const width = parseInt(document.getElementById('custom-width').value);
        archComparison.setCustom(depth, width);
    }

    function compareArchitectures() {
        archComparison.draw();
        showNotification('架构对比完成！查看性能差异。');
    }

    // 决策边界可视化
    class DecisionBoundaryVisualizer {
        constructor() {
            this.dataCanvas = document.getElementById('data-canvas');
            this.boundaryCanvas = document.getElementById('boundary-canvas');
            this.confidenceCanvas = document.getElementById('confidence-canvas');

            this.dataCtx = this.dataCanvas.getContext('2d');
            this.boundaryCtx = this.boundaryCanvas.getContext('2d');
            this.confidenceCtx = this.confidenceCanvas.getContext('2d');

            this.width = this.dataCanvas.width;
            this.height = this.dataCanvas.height;

            this.data = [];
            this.model = null;

            this.generateData();
        }

        generateData() {
            const dataset = document.getElementById('dataset-select').value;
            this.data = [];
            const n = 200;

            switch(dataset) {
                case 'moons':
                    for (let i = 0; i < n; i++) {
                        const angle = i < n/2 ? Math.random() * Math.PI : Math.random() * Math.PI;
                        const noise = (Math.random() - 0.5) * 0.1;

                        if (i < n/2) {
                            this.data.push({
                                x: Math.cos(angle) + noise,
                                y: Math.sin(angle) + noise,
                                label: 0
                            });
                        } else {
                            this.data.push({
                                x: 1 - Math.cos(angle) + noise,
                                y: 0.5 - Math.sin(angle) + noise,
                                label: 1
                            });
                        }
                    }
                    break;

                case 'circles':
                    for (let i = 0; i < n; i++) {
                        const angle = Math.random() * 2 * Math.PI;
                        const r = i < n/2 ? 0.3 : 0.7;
                        const noise = (Math.random() - 0.5) * 0.1;

                        this.data.push({
                            x: r * Math.cos(angle) + noise + 0.5,
                            y: r * Math.sin(angle) + noise + 0.5,
                            label: i < n/2 ? 0 : 1
                        });
                    }
                    break;

                case 'spiral':
                    for (let i = 0; i < n; i++) {
                        const t = i / n * 4 * Math.PI;
                        const r = t / (4 * Math.PI) * 0.4;
                        const label = i < n/2 ? 0 : 1;
                        const offset = label === 0 ? 0 : Math.PI;
                        const noise = (Math.random() - 0.5) * 0.05;

                        this.data.push({
                            x: 0.5 + r * Math.cos(t + offset) + noise,
                            y: 0.5 + r * Math.sin(t + offset) + noise,
                            label: label
                        });
                    }
                    break;

                case 'xor':
                    for (let i = 0; i < n; i++) {
                        const x = Math.random();
                        const y = Math.random();
                        const label = (x > 0.5 && y > 0.5) || (x < 0.5 && y < 0.5) ? 0 : 1;

                        this.data.push({
                            x: x + (Math.random() - 0.5) * 0.1,
                            y: y + (Math.random() - 0.5) * 0.1,
                            label: label
                        });
                    }
                    break;
            }

            this.drawData();
        }

        drawData() {
            const ctx = this.dataCtx;
            ctx.clearRect(0, 0, this.width, this.height);

            this.data.forEach(point => {
                const x = point.x * this.width;
                const y = (1 - point.y) * this.height;

                ctx.fillStyle = point.label === 0 ? '#3b82f6' : '#ef4444';
                ctx.beginPath();
                ctx.arc(x, y, 4, 0, Math.PI * 2);
                ctx.fill();
            });
        }

        async train() {
            document.getElementById('training-progress').textContent = '训练中...';

            const architecture = document.getElementById('architecture-select').value;
            const activation = document.getElementById('activation-select').value;

            // 模拟训练过程
            const startTime = Date.now();

            // 创建模型（这里是模拟）
            this.model = {
                architecture: architecture,
                activation: activation,
                predict: (x, y) => {
                    // 简化的预测函数
                    if (architecture === 'shallow') {
                        return this.shallowPredict(x, y, activation);
                    } else if (architecture === 'medium') {
                        return this.mediumPredict(x, y, activation);
                    } else {
                        return this.deepPredict(x, y, activation);
                    }
                }
            };

            // 模拟训练进度
            for (let epoch = 0; epoch <= 100; epoch += 10) {
                document.getElementById('training-progress').textContent = `${epoch}%`;
                await new Promise(resolve => setTimeout(resolve, 50));
            }

            // 绘制结果
            this.drawBoundary();
            this.drawConfidence();

            // 计算准确率
            let correct = 0;
            this.data.forEach(point => {
                const pred = this.model.predict(point.x, point.y) > 0.5 ? 1 : 0;
                if (pred === point.label) correct++;
            });

            const accuracy = (correct / this.data.length * 100).toFixed(1);
            const trainTime = ((Date.now() - startTime) / 1000).toFixed(1);

            document.getElementById('final-accuracy').textContent = accuracy + '%';
            document.getElementById('training-time').textContent = trainTime + 's';
            document.getElementById('training-progress').textContent = '完成！';
        }

        shallowPredict(x, y, activation) {
            // 简化的浅层网络预测
            const z = x * 0.5 + y * 0.5 - 0.5;
            return this.applyActivation(z, activation);
        }

        mediumPredict(x, y, activation) {
            // 简化的中等深度网络预测
            const h1 = this.applyActivation(x * 2 - 1, activation);
            const h2 = this.applyActivation(y * 2 - 1, activation);
            const z = h1 * 0.5 + h2 * 0.5;
            return this.applyActivation(z, activation);
        }

        deepPredict(x, y, activation) {
            // 简化的深层网络预测
            let h = x + y;
            for (let i = 0; i < 5; i++) {
                h = this.applyActivation(h * 2 - 1, activation);
            }
            return h;
        }

        applyActivation(x, activation) {
            switch(activation) {
                case 'relu':
                    return Math.max(0, x);
                case 'tanh':
                    return Math.tanh(x);
                case 'sigmoid':
                    return 1 / (1 + Math.exp(-x));
                default:
                    return x;
            }
        }

        drawBoundary() {
            const ctx = this.boundaryCtx;
            const imageData = ctx.createImageData(this.width, this.height);
            const data = imageData.data;

            for (let y = 0; y < this.height; y++) {
                for (let x = 0; x < this.width; x++) {
                    const pred = this.model.predict(x / this.width, 1 - y / this.height);

                    const idx = (y * this.width + x) * 4;
                    if (pred > 0.5) {
                        data[idx] = 239;     // r
                        data[idx + 1] = 68;   // g
                        data[idx + 2] = 68;   // b
                        data[idx + 3] = 50;   // a
                    } else {
                        data[idx] = 59;       // r
                        data[idx + 1] = 130;  // g
                        data[idx + 2] = 246;  // b
                        data[idx + 3] = 50;   // a
                    }
                }
            }

            ctx.putImageData(imageData, 0, 0);

            // 叠加数据点
            this.data.forEach(point => {
                const x = point.x * this.width;
                const y = (1 - point.y) * this.height;

                ctx.strokeStyle = point.label === 0 ? '#1e40af' : '#991b1b';
                ctx.lineWidth = 2;
                ctx.beginPath();
                ctx.arc(x, y, 4, 0, Math.PI * 2);
                ctx.stroke();
            });
        }

        drawConfidence() {
            const ctx = this.confidenceCtx;
            const imageData = ctx.createImageData(this.width, this.height);
            const data = imageData.data;

            for (let y = 0; y < this.height; y++) {
                for (let x = 0; x < this.width; x++) {
                    const pred = this.model.predict(x / this.width, 1 - y / this.height);
                    const confidence = Math.abs(pred - 0.5) * 2; // 0到1的置信度

                    const idx = (y * this.width + x) * 4;
                    const intensity = Math.floor(confidence * 255);

                    // 使用渐变色表示置信度
                    if (pred > 0.5) {
                        data[idx] = intensity;        // r
                        data[idx + 1] = 0;            // g
                        data[idx + 2] = 0;            // b
                    } else {
                        data[idx] = 0;                // r
                        data[idx + 1] = 0;            // g
                        data[idx + 2] = intensity;    // b
                    }
                    data[idx + 3] = 255;              // a
                }
            }

            ctx.putImageData(imageData, 0, 0);

            // 添加等高线
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
            ctx.lineWidth = 1;

            for (let level = 0.1; level < 1; level += 0.1) {
                ctx.beginPath();
                for (let x = 0; x < this.width; x++) {
                    for (let y = 0; y < this.height; y++) {
                        const pred = this.model.predict(x / this.width, 1 - y / this.height);
                        const confidence = Math.abs(pred - 0.5) * 2;

                        if (Math.abs(confidence - level) < 0.02) {
                            ctx.moveTo(x, y);
                            ctx.lineTo(x + 1, y);
                        }
                    }
                }
                ctx.stroke();
            }
        }
    }

    const boundaryViz = new DecisionBoundaryVisualizer();

    async function trainAndVisualize() {
        await boundaryViz.train();
    }

    // 数据集改变时重新生成数据
    document.getElementById('dataset-select').addEventListener('change', () => {
        boundaryViz.generateData();
    });

    // 梯度流分析
    class GradientFlowAnalyzer {
        constructor() {
            this.canvas = document.getElementById('gradient-flow-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;

            this.analyze();
        }

        analyze() {
            const depth = parseInt(document.getElementById('depth-slider').value);
            const activation = document.getElementById('gradient-activation').value;

            // 模拟梯度流
            const gradients = this.simulateGradients(depth, activation);

            // 绘制结果
            this.draw(gradients);

            // 更新统计
            this.updateStats(gradients);
        }

        simulateGradients(depth, activation) {
            const gradients = [];
            let gradient = 1.0; // 从输出层开始

            for (let i = depth - 1; i >= 0; i--) {
                // 根据激活函数计算梯度衰减
                const decay = this.getGradientDecay(activation, i, depth);
                gradient *= decay;

                gradients.unshift({
                    layer: i,
                    gradient: gradient,
                    log_gradient: Math.log10(Math.max(gradient, 1e-10))
                });
            }

            return gradients;
        }

        getGradientDecay(activation, layer, totalLayers) {
            // 模拟不同激活函数的梯度衰减
            switch(activation) {
                case 'sigmoid':
                    return 0.25; // sigmoid导数最大值是0.25
                case 'tanh':
                    return 0.5;  // tanh导数最大值是1，但平均更低
                case 'relu':
                    return Math.random() > 0.1 ? 1.0 : 0; // 10%概率死亡
                case 'leaky_relu':
                    return Math.random() > 0.5 ? 1.0 : 0.01;
                default:
                    return 0.8;
            }
        }

        draw(gradients) {
            const ctx = this.ctx;
            ctx.clearRect(0, 0, this.width, this.height);

            // 绘制背景网格
            this.drawGrid();

            // 绘制梯度曲线
            ctx.strokeStyle = '#3b82f6';
            ctx.lineWidth = 3;
            ctx.beginPath();

            gradients.forEach((point, idx) => {
                const x = (idx / (gradients.length - 1)) * (this.width - 100) + 50;
                const y = this.height - 50 - (point.log_gradient + 10) / 12 * (this.height - 100);

                if (idx === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }

                // 绘制数据点
                ctx.fillStyle = '#3b82f6';
                ctx.beginPath();
                ctx.arc(x, y, 4, 0, Math.PI * 2);
                ctx.fill();
            });

            ctx.stroke();

            // 绘制危险区域
            ctx.fillStyle = 'rgba(239, 68, 68, 0.1)';
            ctx.fillRect(50, this.height - 50 - 2 / 12 * (this.height - 100),
                this.width - 100, 2 / 12 * (this.height - 100));

            // 标注
            ctx.fillStyle = '#ef4444';
            ctx.font = '12px Arial';
            ctx.fillText('梯度消失区域', 60, this.height - 60);
        }

        drawGrid() {
            const ctx = this.ctx;
            ctx.strokeStyle = 'rgba(255, 255, 255, 0.1)';
            ctx.lineWidth = 1;

            // 横线
            for (let i = 0; i <= 10; i++) {
                const y = 50 + i * (this.height - 100) / 10;
                ctx.beginPath();
                ctx.moveTo(50, y);
                ctx.lineTo(this.width - 50, y);
                ctx.stroke();
            }

            // 竖线（继续）
            for (let i = 0; i <= 10; i++) {
                const x = 50 + i * (this.width - 100) / 10;
                ctx.beginPath();
                ctx.moveTo(x, 50);
                ctx.lineTo(x, this.height - 50);
                ctx.stroke();
            }

            // 坐标轴标签
            ctx.fillStyle = '#f1f5f9';
            ctx.font = '12px Arial';
            ctx.textAlign = 'center';

            // x轴标签（层索引）
            ctx.fillText('输入层', 50, this.height - 30);
            ctx.fillText('输出层', this.width - 50, this.height - 30);
            ctx.fillText('层深度', this.width / 2, this.height - 10);

            // y轴标签（梯度值）
            ctx.textAlign = 'right';
            for (let i = 0; i <= 5; i++) {
                const value = -10 + i * 2;
                const y = this.height - 50 - (value + 10) / 12 * (this.height - 100);
                ctx.fillText(`10^${value}`, 45, y + 5);
            }
        }

        updateStats(gradients) {
            const first = gradients[0].gradient;
            const middle = gradients[Math.floor(gradients.length / 2)].gradient;
            const last = gradients[gradients.length - 1].gradient;

            document.getElementById('first-layer-gradient').textContent = first.toExponential(3);
            document.getElementById('middle-layer-gradient').textContent = middle.toExponential(3);
            document.getElementById('last-layer-gradient').textContent = last.toExponential(3);
        }
    }

    let gradientAnalyzer = null;

    function analyzeGradients() {
        if (!gradientAnalyzer) {
            gradientAnalyzer = new GradientFlowAnalyzer();
        } else {
            gradientAnalyzer.analyze();
        }
    }

    // 深度滑块更新
    document.getElementById('depth-slider').addEventListener('input', (e) => {
        document.getElementById('depth-value').textContent = e.target.value + '层';
    });

    // 思维导图绘制
    class MindMapDrawer {
        constructor() {
            this.canvas = document.getElementById('mindmap-canvas');
            this.ctx = this.canvas.getContext('2d');
            this.width = this.canvas.width;
            this.height = this.canvas.height;

            this.nodes = this.createNodes();
            this.draw();
        }

        createNodes() {
            return {
                root: {
                    x: this.width / 2,
                    y: this.height / 2,
                    text: 'MLP核心概念',
                    color: '#8b5cf6',
                    children: ['activation', 'architecture', 'training', 'problems']
                },
                activation: {
                    x: this.width / 4,
                    y: this.height / 3,
                    text: '激活函数',
                    color: '#3b82f6',
                    children: ['relu', 'sigmoid', 'modern']
                },
                architecture: {
                    x: 3 * this.width / 4,
                    y: this.height / 3,
                    text: '网络架构',
                    color: '#22c55e',
                    children: ['depth', 'width', 'residual']
                },
                training: {
                    x: this.width / 4,
                    y: 2 * this.height / 3,
                    text: '训练技巧',
                    color: '#f59e0b',
                    children: ['init', 'regularization', 'optimization']
                },
                problems: {
                    x: 3 * this.width / 4,
                    y: 2 * this.height / 3,
                    text: '常见问题',
                    color: '#ef4444',
                    children: ['vanishing', 'overfitting', 'dying']
                },
                // 子节点
                relu: { x: 100, y: 100, text: 'ReLU家族', color: '#3b82f6' },
                sigmoid: { x: 100, y: 150, text: 'Sigmoid/Tanh', color: '#3b82f6' },
                modern: { x: 100, y: 200, text: 'GELU/Swish', color: '#3b82f6' },
                depth: { x: 600, y: 100, text: '深度设计', color: '#22c55e' },
                width: { x: 600, y: 150, text: '宽度选择', color: '#22c55e' },
                residual: { x: 600, y: 200, text: '残差连接', color: '#22c55e' },
                init: { x: 100, y: 400, text: '权重初始化', color: '#f59e0b' },
                regularization: { x: 100, y: 450, text: '正则化', color: '#f59e0b' },
                optimization: { x: 100, y: 500, text: '优化算法', color: '#f59e0b' },
                vanishing: { x: 600, y: 400, text: '梯度消失', color: '#ef4444' },
                overfitting: { x: 600, y: 450, text: '过拟合', color: '#ef4444' },
                dying: { x: 600, y: 500, text: '死亡ReLU', color: '#ef4444' }
            };
        }

        draw() {
            this.ctx.clearRect(0, 0, this.width, this.height);

            // 绘制连接线
            Object.entries(this.nodes).forEach(([key, node]) => {
                if (node.children) {
                    node.children.forEach(childKey => {
                        const child = this.nodes[childKey];
                        this.drawConnection(node, child);
                    });
                }
            });

            // 绘制节点
            Object.values(this.nodes).forEach(node => {
                this.drawNode(node);
            });
        }

        drawNode(node) {
            const ctx = this.ctx;

            // 绘制圆形背景
            ctx.fillStyle = node.color;
            ctx.beginPath();
            ctx.arc(node.x, node.y, 40, 0, Math.PI * 2);
            ctx.fill();

            // 绘制文字
            ctx.fillStyle = 'white';
            ctx.font = '14px Arial';
            ctx.textAlign = 'center';
            ctx.textBaseline = 'middle';
            ctx.fillText(node.text, node.x, node.y);
        }

        drawConnection(parent, child) {
            const ctx = this.ctx;

            ctx.strokeStyle = 'rgba(139, 92, 246, 0.3)';
            ctx.lineWidth = 2;

            ctx.beginPath();
            ctx.moveTo(parent.x, parent.y);

            // 使用贝塞尔曲线
            const cx = (parent.x + child.x) / 2;
            const cy = (parent.y + child.y) / 2;
            ctx.quadraticCurveTo(cx, cy, child.x, child.y);

            ctx.stroke();
        }
    }

    // 初始化思维导图
    new MindMapDrawer();

    // 实践练习答案检查
    const practiceAnswers = {
        'practice-feedback-1': {
            correct: 'b',
            feedback: {
                'a': '不是计算能力的问题，单层感知机的计算能力足够。',
                'b': '正确！单层感知机只能表示线性决策边界，无法分离XOR这样的非线性模式。',
                'c': '参数数量不是关键，即使增加参数也无法解决线性限制。',
                'd': '反向传播算法没有问题，是模型表达能力的限制。'
            }
        },
        'practice-feedback-2': {
            correct: 'b',
            feedback: {
                'a': '错误，没有激活函数的多层网络仍然是线性的。',
                'b': '正确！多个线性变换的组合仍然是线性变换。',
                'c': '层数不影响，关键是缺少非线性。',
                'd': '无论多少层，线性组合还是线性。'
            }
        }
    };

    // 练习答案检查功能
    document.querySelectorAll('.practice-option').forEach(option => {
        option.addEventListener('click', function() {
            const answer = this.dataset.answer;
            const feedbackId = this.closest('.instant-practice').querySelector('.practice-feedback').id;
            const feedback = practiceAnswers[feedbackId];

            // 清除之前的选择
            this.parentElement.querySelectorAll('.practice-option').forEach(opt => {
                opt.classList.remove('selected', 'correct', 'incorrect');
            });

            // 标记选择
            this.classList.add('selected');

            // 显示反馈
            const feedbackElement = document.getElementById(feedbackId);
            if (answer === feedback.correct) {
                this.classList.add('correct');
                feedbackElement.className = 'practice-feedback show correct';
                feedbackElement.innerHTML = `
                    <strong>✅ 正确！</strong><br>
                    ${feedback.feedback[answer]}
                `;

                // 记录成就
                if (!completedSections.has(feedbackId)) {
                    completedSections.add(feedbackId);
                    checkAchievements();
                }
            } else {
                this.classList.add('incorrect');
                feedbackElement.className = 'practice-feedback show incorrect';
                feedbackElement.innerHTML = `
                    <strong>❌ 不太对哦</strong><br>
                    ${feedback.feedback[answer]}<br>
                    <em>提示：${feedback.feedback[feedback.correct]}</em>
                `;
            }
        });
    });

    // 代码标签切换
    document.querySelectorAll('.code-tab').forEach(tab => {
        tab.addEventListener('click', function() {
            const lang = this.dataset.lang;
            const container = this.closest('.code-container');

            // 更新标签状态
            container.querySelectorAll('.code-tab').forEach(t => t.classList.remove('active'));
            this.classList.add('active');

            // 切换代码内容
            container.querySelectorAll('.code-content').forEach(content => {
                content.style.display = 'none';
            });
            container.querySelector(`#${lang}-code`).style.display = 'block';
        });
    });

    // 复制代码功能
    function copyCode() {
        const activeCode = document.querySelector('.code-content:not([style*="display: none"]) code');
        if (activeCode) {
            const text = activeCode.textContent;
            navigator.clipboard.writeText(text).then(() => {
                showNotification('代码已复制到剪贴板！');
            });
        }
    }

    // 运行代码功能（模拟）
    function runCode() {
        const outputElement = document.querySelector('.code-content:not([style*="display: none"]) .code-output');
        if (outputElement) {
            outputElement.classList.add('show');
            showNotification('代码运行完成！查看输出结果。');
        }
    }

    // 通知系统
    function showNotification(message, type = 'success') {
        const notification = document.createElement('div');
        notification.className = 'notification';
        notification.innerHTML = `
            <span>${message}</span>
        `;

        if (type === 'error') {
            notification.style.background = '#ef4444';
        } else if (type === 'warning') {
            notification.style.background = '#f59e0b';
        }

        document.body.appendChild(notification);

        setTimeout(() => {
            notification.style.animation = 'slideOutRight 0.3s ease';
            setTimeout(() => {
                document.body.removeChild(notification);
            }, 300);
        }, 3000);
    }

    // 学习反馈
    function submitFeedback(type) {
        const responseElement = document.getElementById('feedback-response');
        responseElement.style.display = 'block';

        // 发送反馈到分析系统（这里是模拟）
        console.log('Feedback submitted:', type);

        // 根据反馈类型显示不同消息
        if (type === 'confusing') {
            responseElement.innerHTML = `
                <p style="color: var(--accent-yellow);">感谢反馈！哪部分让你困惑？</p>
                <p style="font-size: 0.875rem; margin-top: 0.5rem;">
                    你可以试试调整学习深度，或者向学习伙伴寻求帮助。
                </p>
            `;
        }
    }

    // 监听滚动，标记已读章节
    const observerOptions = {
        root: null,
        rootMargin: '0px',
        threshold: 0.5
    };

    const sectionObserver = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting && !completedSections.has(entry.target.id)) {
                completedSections.add(entry.target.id);
                checkAchievements();
            }
        });
    }, observerOptions);

    // 观察所有章节
    sections.forEach(section => {
        sectionObserver.observe(section);
    });

    // 键盘快捷键
    document.addEventListener('keydown', (e) => {
        // Ctrl/Cmd + K: 打开搜索
        if ((e.ctrlKey || e.metaKey) && e.key === 'k') {
            e.preventDefault();
            // 这里可以实现搜索功能
            showNotification('搜索功能开发中...');
        }

        // Esc: 关闭侧边栏
        if (e.key === 'Escape') {
            sidebar.classList.remove('open');
            sidebarOverlay.classList.remove('active');
        }
    });

    // 自动保存学习进度
    function saveProgress() {
        const progress = {
            completedSections: Array.from(completedSections),
            currentDifficulty: currentDifficulty,
            achievements: achievements,
            timestamp: Date.now()
        };
        localStorage.setItem('mlp-progress', JSON.stringify(progress));
    }

    // 加载学习进度
    function loadProgress() {
        const saved = localStorage.getItem('mlp-progress');
        if (saved) {
            const progress = JSON.parse(saved);
            completedSections = new Set(progress.completedSections);
            currentDifficulty = progress.currentDifficulty || 'beginner';
            achievements = progress.achievements || achievements;

            // 更新UI
            updateContentVisibility(currentDifficulty);
            checkAchievements();

            showNotification('已恢复上次学习进度');
        }
    }

    // 页面加载完成后的初始化
    window.addEventListener('load', () => {
        // 加载进度
        loadProgress();

        // 设置定期保存
        setInterval(saveProgress, 60000); // 每分钟保存一次

        // 初始化可视化
        drawMiniPatterns();

        // 显示欢迎消息
        setTimeout(() => {
            learningBuddy.classList.add('active');
            updateBuddyMessage('欢迎来到多层感知机的学习之旅！我是小智，随时为你提供帮助。试试点击侧边的练习题吧！');
        }, 1000);
    });

    // 页面离开时保存进度
    window.addEventListener('beforeunload', saveProgress);

    // 打印优化
    window.addEventListener('beforeprint', () => {
        document.body.classList.add('printing');
    });

    window.addEventListener('afterprint', () => {
        document.body.classList.remove('printing');
    });

    // 响应式处理
    function handleResize() {
        const isMobile = window.innerWidth < 768;

        if (isMobile) {
            // 移动端优化
            learningBuddy.style.width = '90%';
            learningBuddy.style.left = '5%';
            learningBuddy.style.right = '5%';
        } else {
            // 桌面端恢复
            learningBuddy.style.width = '300px';
            learningBuddy.style.left = 'auto';
            learningBuddy.style.right = '20px';
        }
    }

    window.addEventListener('resize', handleResize);
    handleResize(); // 初始调用

    // 性能优化：懒加载图片
    const lazyImages = document.querySelectorAll('img[data-src]');
    const imageObserver = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                const img = entry.target;
                img.src = img.dataset.src;
                img.removeAttribute('data-src');
                imageObserver.unobserve(img);
            }
        });
    });

    lazyImages.forEach(img => imageObserver.observe(img));

    // 添加页面可见性API支持
    document.addEventListener('visibilitychange', () => {
        if (document.hidden) {
            // 页面隐藏时暂停动画
            document.querySelectorAll('canvas').forEach(canvas => {
                canvas.dataset.paused = 'true';
            });
        } else {
            // 页面显示时恢复动画
            document.querySelectorAll('canvas').forEach(canvas => {
                delete canvas.dataset.paused;
            });
        }
    });

    // 无障碍支持：为屏幕阅读器添加额外信息
    function announceToScreenReader(message) {
        const announcement = document.createElement('div');
        announcement.className = 'visually-hidden';
        announcement.setAttribute('role', 'status');
        announcement.setAttribute('aria-live', 'polite');
        announcement.textContent = message;

        document.body.appendChild(announcement);

        setTimeout(() => {
            document.body.removeChild(announcement);
        }, 1000);
    }

    // 完成章节时通知屏幕阅读器
    const originalCheckAchievements = checkAchievements;
    checkAchievements = function() {
        originalCheckAchievements();
        const completed = completedSections.size;
        announceToScreenReader(`已完成 ${completed} 个章节`);
    };

    console.log('🎉 MLP教程加载完成！祝学习愉快！');
</script>
</body>
</html>