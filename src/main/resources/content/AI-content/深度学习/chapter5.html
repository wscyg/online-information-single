<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第8章：长短期记忆网络LSTM - 让神经网络学会遗忘与记忆 | 深度学习序列建模</title>
    <meta name="description" content="从RNN的梯度消失问题出发，一步步推导LSTM的精妙设计，理解遗忘门、输入门、输出门的数学原理">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css">
    <style>
        /* ===== CSS变量定义 ===== */
        :root {
            --primary-gradient: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            --lstm-gradient: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
            --forget-gradient: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            --input-gradient: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
            --output-gradient: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
            --cell-gradient: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            --bg-dark: #0f172a;
            --bg-section: #1e293b;
            --text-primary: #f1f5f9;
            --text-secondary: #cbd5e1;
            --text-muted: #94a3b8;
            --accent-red: #ef4444;
            --accent-green: #22c55e;
            --accent-blue: #3b82f6;
            --accent-yellow: #fbbf24;
            --accent-purple: #8b5cf6;
            --accent-pink: #ec4899;
            --accent-orange: #f97316;
            --accent-cyan: #06b6d4;
            --border-color: rgba(255, 255, 255, 0.1);
            --shadow-lg: 0 10px 40px rgba(0, 0, 0, 0.3);
            --animation-duration: 0.3s;
            --content-max-width: 1200px;
        }

        /* 亮色主题 */
        [data-theme="light"] {
            --bg-dark: #ffffff;
            --bg-section: #f8fafc;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --text-muted: #64748b;
            --border-color: rgba(0, 0, 0, 0.1);
            --shadow-lg: 0 10px 40px rgba(0, 0, 0, 0.1);
        }

        /* ===== 全局样式 ===== */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            background: var(--bg-dark);
            color: var(--text-primary);
            line-height: 1.6;
            overflow-x: hidden;
            transition: background-color var(--animation-duration) ease;
        }

        /* ===== 布局组件 ===== */
        .container {
            max-width: var(--content-max-width);
            margin: 0 auto;
            padding: 0 1rem;
        }

        /* ===== 导航栏 ===== */
        .nav-header {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(15, 23, 42, 0.95);
            backdrop-filter: blur(10px);
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
            transition: transform 0.3s ease;
        }

        .nav-header.hidden {
            transform: translateY(-100%);
        }

        .nav-content {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 1rem 0;
        }

        .nav-title {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .nav-title h1 {
            font-size: 1.25rem;
            font-weight: 600;
            background: var(--lstm-gradient);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .nav-controls {
            display: flex;
            gap: 1rem;
            align-items: center;
        }

        .btn-control {
            background: transparent;
            border: 1px solid var(--border-color);
            color: var(--text-primary);
            padding: 0.5rem;
            border-radius: 0.5rem;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
            display: flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
        }

        .btn-control:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: translateY(-2px);
        }

        .nav-progress {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: rgba(255, 255, 255, 0.1);
        }

        .progress-bar {
            height: 100%;
            background: var(--lstm-gradient);
            width: 0%;
            transition: width 0.3s ease;
        }

        /* ===== 侧边导航 ===== */
        .sidebar {
            position: fixed;
            left: -300px;
            top: 60px;
            bottom: 0;
            width: 300px;
            background: var(--bg-section);
            border-right: 1px solid var(--border-color);
            padding: 2rem;
            overflow-y: auto;
            transition: transform 0.3s ease;
            z-index: 999;
        }

        .sidebar.open {
            transform: translateX(300px);
        }

        .sidebar-overlay {
            position: fixed;
            inset: 0;
            background: rgba(0, 0, 0, 0.5);
            display: none;
            z-index: 998;
        }

        .sidebar-overlay.active {
            display: block;
        }

        .toc-item {
            display: block;
            padding: 0.75rem 1rem;
            color: var(--text-secondary);
            text-decoration: none;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
            margin-bottom: 0.25rem;
        }

        .toc-item:hover {
            background: rgba(255, 255, 255, 0.05);
            color: var(--text-primary);
            transform: translateX(4px);
        }

        .toc-item.active {
            background: var(--lstm-gradient);
            color: #0f172a;
        }

        /* ===== 主内容区 ===== */
        main {
            margin-top: 80px;
            padding-bottom: 4rem;
        }

        /* ===== 章节卡片 ===== */
        .chapter-hero {
            background: var(--lstm-gradient);
            padding: 4rem 0;
            margin-bottom: 3rem;
            position: relative;
            overflow: hidden;
        }

        .chapter-hero::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(255, 255, 255, 0.2) 0%, transparent 70%);
            animation: pulse 10s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1) rotate(0deg); }
            50% { transform: scale(1.1) rotate(180deg); }
        }

        .chapter-hero-content {
            position: relative;
            z-index: 2;
            text-align: center;
            color: #0f172a;
        }

        .chapter-hero h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            animation: fadeInUp 0.8s ease;
        }

        .chapter-hero p {
            font-size: 1.25rem;
            opacity: 0.9;
            max-width: 600px;
            margin: 0 auto;
            animation: fadeInUp 0.8s ease 0.2s both;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* ===== 内容区块 ===== */
        .section-card {
            background: var(--bg-section);
            border-radius: 1rem;
            padding: 2.5rem;
            margin-bottom: 2rem;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-lg);
            transition: transform var(--animation-duration) ease;
        }

        .section-card:hover {
            transform: translateY(-2px);
        }

        /* ===== 故事卡片 ===== */
        .story-card {
            background: linear-gradient(135deg, rgba(168, 237, 234, 0.15), rgba(254, 214, 227, 0.1));
            border-radius: 1rem;
            padding: 2rem;
            margin-bottom: 2rem;
            border: 2px solid rgba(168, 237, 234, 0.2);
            position: relative;
            overflow: hidden;
        }

        .story-card::before {
            content: '📖';
            position: absolute;
            top: -20px;
            right: -20px;
            font-size: 5rem;
            opacity: 0.1;
            transform: rotate(15deg);
        }

        /* ===== 对话框 ===== */
        .dialogue-box {
            background: rgba(255, 255, 255, 0.05);
            border-left: 4px solid var(--accent-pink);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 0.5rem 0.5rem 0;
            position: relative;
        }

        .dialogue-box .speaker {
            font-weight: bold;
            color: var(--accent-pink);
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .dialogue-box .avatar {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
        }

        /* ===== 概念网格 ===== */
        .concept-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .concept-card {
            padding: 1.5rem;
            border-radius: 0.75rem;
            transition: all var(--animation-duration) ease;
            cursor: pointer;
            position: relative;
            overflow: hidden;
        }

        .concept-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            height: 4px;
            background: var(--lstm-gradient);
            transform: scaleX(0);
            transition: transform var(--animation-duration) ease;
        }

        .concept-card:hover::before {
            transform: scaleX(1);
        }

        .concept-card.forget {
            background: rgba(240, 147, 251, 0.1);
            border: 1px solid rgba(240, 147, 251, 0.3);
        }

        .concept-card.input {
            background: rgba(79, 172, 254, 0.1);
            border: 1px solid rgba(79, 172, 254, 0.3);
        }

        .concept-card.output {
            background: rgba(67, 233, 123, 0.1);
            border: 1px solid rgba(67, 233, 123, 0.3);
        }

        .concept-card.cell {
            background: rgba(250, 112, 154, 0.1);
            border: 1px solid rgba(250, 112, 154, 0.3);
        }

        /* ===== 数学容器 ===== */
        .math-container {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(236, 72, 153, 0.05));
            border: 2px solid rgba(139, 92, 246, 0.3);
            border-radius: 1rem;
            padding: 2.5rem;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }

        .math-container::before {
            content: '∑';
            position: absolute;
            top: -30px;
            right: 20px;
            font-size: 6rem;
            opacity: 0.05;
            font-family: 'Times New Roman', serif;
        }

        /* 美化的数学公式 */
        .math-formula {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 0.75rem;
            padding: 2rem;
            margin: 1.5rem 0;
            text-align: center;
            font-size: 1.2rem;
            overflow-x: auto;
            position: relative;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .math-formula-title {
            position: absolute;
            top: -12px;
            left: 2rem;
            background: var(--bg-section);
            padding: 0 1rem;
            font-size: 0.875rem;
            color: var(--accent-purple);
            font-weight: 600;
        }

        /* ===== 推导盒子 ===== */
        .derivation-box {
            background: linear-gradient(135deg, rgba(79, 172, 254, 0.1), rgba(0, 242, 254, 0.05));
            border: 2px solid rgba(79, 172, 254, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .derivation-box::before {
            content: '🔍';
            position: absolute;
            top: -15px;
            left: 30px;
            font-size: 2rem;
            background: var(--bg-section);
            padding: 0 10px;
        }

        /* ===== 思考盒子 ===== */
        .think-box {
            background: linear-gradient(135deg, rgba(139, 92, 246, 0.15), rgba(124, 58, 237, 0.1));
            border: 2px solid rgba(139, 92, 246, 0.3);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .think-box::before {
            content: '🤔';
            position: absolute;
            top: -15px;
            left: 30px;
            font-size: 2rem;
            background: var(--bg-section);
            padding: 0 10px;
        }

        /* ===== 代码容器 ===== */
        .code-container {
            margin: 2rem 0;
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: var(--shadow-lg);
        }

        .code-header {
            background: #1a1a2e;
            padding: 1rem 1.5rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .code-header-left {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .code-header-right {
            display: flex;
            gap: 0.5rem;
        }

        .code-tab {
            padding: 0.5rem 1rem;
            background: transparent;
            border: none;
            color: var(--text-secondary);
            cursor: pointer;
            border-radius: 0.5rem;
            transition: all var(--animation-duration) ease;
        }

        .code-tab.active {
            background: rgba(255, 255, 255, 0.1);
            color: var(--text-primary);
        }

        .code-content {
            background: #0d1117;
            overflow-x: auto;
            position: relative;
        }

        .code-content pre {
            margin: 0;
            padding: 1.5rem;
        }

        .code-content code {
            font-family: 'SF Mono', 'Monaco', 'Inconsolata', 'Fira Code', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* ===== LSTM门可视化 ===== */
        .lstm-gate-viz {
            display: flex;
            justify-content: space-around;
            align-items: center;
            margin: 2rem 0;
            padding: 2rem;
            background: rgba(255, 255, 255, 0.02);
            border-radius: 1rem;
        }

        .gate-component {
            text-align: center;
            padding: 1.5rem;
            border-radius: 1rem;
            transition: all 0.3s ease;
            cursor: pointer;
            position: relative;
        }

        .gate-component:hover {
            transform: translateY(-5px) scale(1.05);
        }

        .gate-component.forget {
            background: var(--forget-gradient);
            color: white;
        }

        .gate-component.input {
            background: var(--input-gradient);
            color: white;
        }

        .gate-component.output {
            background: var(--output-gradient);
            color: white;
        }

        .gate-icon {
            width: 80px;
            height: 80px;
            margin: 0 auto 1rem;
            display: flex;
            align-items: center;
            justify-content: center;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 50%;
            font-size: 2.5rem;
        }

        /* ===== 推导步骤 ===== */
        .derivation-step {
            background: rgba(255, 255, 255, 0.05);
            border-left: 4px solid var(--accent-cyan);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 0.5rem 0.5rem 0;
            position: relative;
        }

        .derivation-step-number {
            position: absolute;
            left: -20px;
            top: 1rem;
            width: 40px;
            height: 40px;
            background: var(--accent-cyan);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }

        /* ===== 比较表格 ===== */
        .comparison-table {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 1rem;
            overflow: hidden;
            margin: 2rem 0;
            box-shadow: var(--shadow-lg);
        }

        .comparison-table table {
            width: 100%;
            border-collapse: collapse;
        }

        .comparison-table th {
            background: var(--lstm-gradient);
            color: #0f172a;
            padding: 1rem;
            text-align: left;
            font-weight: 600;
        }

        .comparison-table td {
            padding: 1rem;
            border-bottom: 1px solid var(--border-color);
        }

        .comparison-table tr:hover {
            background: rgba(255, 255, 255, 0.05);
        }

        /* ===== 时间线 ===== */
        .timeline {
            position: relative;
            padding: 2rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 50%;
            top: 0;
            bottom: 0;
            width: 2px;
            background: var(--border-color);
        }

        .timeline-item {
            position: relative;
            margin: 2rem 0;
            padding: 0 2rem;
        }

        .timeline-item:nth-child(odd) {
            text-align: right;
            padding-right: 50%;
        }

        .timeline-item:nth-child(even) {
            padding-left: 50%;
        }

        .timeline-dot {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 20px;
            height: 20px;
            background: var(--lstm-gradient);
            border-radius: 50%;
            z-index: 1;
        }

        /* ===== 可视化容器 ===== */
        .visualization-container {
            background: rgba(15, 23, 42, 0.8);
            border: 1px solid var(--border-color);
            border-radius: 1rem;
            padding: 2rem;
            margin: 2rem 0;
            position: relative;
        }

        .viz-controls {
            display: flex;
            gap: 2rem;
            align-items: center;
            margin-bottom: 1.5rem;
            flex-wrap: wrap;
        }

        /* ===== 按钮样式 ===== */
        .btn {
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            border: none;
            font-weight: 500;
            cursor: pointer;
            transition: all var(--animation-duration) ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            text-decoration: none;
        }

        .btn-primary {
            background: var(--lstm-gradient);
            color: #0f172a;
        }

        .btn-primary:hover {
            transform: translateY(-2px);
            box-shadow: 0 10px 20px rgba(168, 237, 234, 0.3);
        }

        .btn-secondary {
            background: transparent;
            color: var(--text-primary);
            border: 1px solid var(--border-color);
        }

        .btn-secondary:hover {
            background: rgba(255, 255, 255, 0.1);
        }

        /* ===== 提示框 ===== */
        .tip {
            display: flex;
            gap: 1rem;
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
        }

        .tip-icon {
            font-size: 1.5rem;
            flex-shrink: 0;
        }

        .tip-content {
            flex: 1;
        }

        .tip.info {
            background: rgba(79, 172, 254, 0.1);
            border: 1px solid rgba(79, 172, 254, 0.3);
        }

        .tip.warning {
            background: rgba(251, 191, 36, 0.1);
            border: 1px solid rgba(251, 191, 36, 0.3);
        }

        .tip.success {
            background: rgba(34, 197, 94, 0.1);
            border: 1px solid rgba(34, 197, 94, 0.3);
        }

        /* ===== 响应式设计 ===== */
        @media (max-width: 1024px) {
            .chapter-hero h1 {
                font-size: 2.5rem;
            }

            .concept-grid {
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            }

            .lstm-gate-viz {
                flex-direction: column;
                gap: 2rem;
            }
        }

        @media (max-width: 768px) {
            .nav-title h1 {
                font-size: 1rem;
            }

            .section-card {
                padding: 1.5rem;
            }

            .chapter-hero {
                padding: 3rem 0;
            }

            .chapter-hero h1 {
                font-size: 2rem;
            }

            .chapter-hero p {
                font-size: 1rem;
            }

            .timeline-item:nth-child(odd),
            .timeline-item:nth-child(even) {
                padding-left: 3rem;
                padding-right: 1rem;
                text-align: left;
            }

            .timeline::before {
                left: 20px;
            }

            .timeline-dot {
                left: 20px;
            }
        }

        /* ===== 工具类 ===== */
        .text-center {
            text-align: center;
        }

        .mt-1 { margin-top: 0.5rem; }
        .mt-2 { margin-top: 1rem; }
        .mt-3 { margin-top: 1.5rem; }
        .mt-4 { margin-top: 2rem; }

        .mb-1 { margin-bottom: 0.5rem; }
        .mb-2 { margin-bottom: 1rem; }
        .mb-3 { margin-bottom: 1.5rem; }
        .mb-4 { margin-bottom: 2rem; }

        /* ===== 加载动画 ===== */
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(255, 255, 255, 0.3);
            border-radius: 50%;
            border-top-color: var(--accent-pink);
            animation: spin 1s ease-in-out infinite;
        }

        @keyframes spin {
            to { transform: rotate(360deg); }
        }

        /* LSTM流程动画 */
        @keyframes flow {
            0% { transform: translateX(-100%); opacity: 0; }
            50% { opacity: 1; }
            100% { transform: translateX(100%); opacity: 0; }
        }

        .flow-animation {
            position: relative;
            overflow: hidden;
        }

        .flow-animation::after {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.2), transparent);
            animation: flow 3s infinite;
        }

        /* KaTeX 样式覆盖 */
        .katex-display {
            overflow-x: auto;
            overflow-y: hidden;
            padding: 1rem 0;
        }

        .katex {
            font-size: 1.1em;
        }

        /* 关键概念卡片样式 */
        @media (max-width: 768px) {
            .key-concept-card {
                padding: 2rem !important;
            }

            .key-concept-card > div:nth-child(3) {
                grid-template-columns: repeat(2, 1fr) !important;
                gap: 1.5rem !important;
            }

            .key-concept-card h2 {
                font-size: 1.5rem !important;
            }

            .key-concept-card > p {
                font-size: 1.1rem !important;
            }
        }

        @media (max-width: 480px) {
            .key-concept-card > div:nth-child(3) {
                grid-template-columns: 1fr !important;
            }

            .key-concept-card {
                padding: 1.5rem !important;
            }
        }
    </style>
</head>
<body>

<!-- 导航栏 -->
<nav class="nav-header" role="navigation" aria-label="主导航">
    <div class="container">
        <div class="nav-content">
            <div class="nav-title">
                <button id="toggle-sidebar" class="btn-control" aria-label="切换侧边栏">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <line x1="3" y1="12" x2="21" y2="12"></line>
                        <line x1="3" y1="6" x2="21" y2="6"></line>
                        <line x1="3" y1="18" x2="21" y2="18"></line>
                    </svg>
                </button>
                <h1>第8章：长短期记忆网络LSTM</h1>
            </div>
            <div class="nav-controls">
                <button id="toggle-animations" class="btn-control" aria-label="切换动画">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="10"></circle>
                        <path d="M10 8l6 4-6 4V8z"></path>
                    </svg>
                </button>
                <button id="toggle-theme" class="btn-control" aria-label="切换主题">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                    </svg>
                </button>
            </div>
        </div>
        <div class="nav-progress">
            <div class="progress-bar" id="progress-bar"></div>
        </div>
    </div>
</nav>

<!-- 侧边栏 -->
<aside class="sidebar" id="sidebar">
    <h3 style="margin-bottom: 1.5rem; color: var(--accent-pink);">目录导航</h3>
    <nav>
        <a href="#intro" class="toc-item active">序言：记忆的艺术</a>
        <a href="#problem" class="toc-item">RNN的困境</a>
        <a href="#solution-thinking" class="toc-item">解决思路推导</a>
        <a href="#forget-gate" class="toc-item">遗忘门的诞生</a>
        <a href="#input-gate" class="toc-item">输入门的设计</a>
        <a href="#cell-state" class="toc-item">细胞状态更新</a>
        <a href="#output-gate" class="toc-item">输出门的控制</a>
        <a href="#complete-lstm" class="toc-item">LSTM完整结构</a>
        <a href="#math-proof" class="toc-item">数学证明</a>
        <a href="#implementation" class="toc-item">从零实现LSTM</a>
        <a href="#visualization" class="toc-item">可视化理解</a>
        <a href="#practice" class="toc-item">实战应用</a>
        <a href="#summary" class="toc-item">总结与展望</a>
    </nav>
</aside>

<!-- 侧边栏遮罩 -->
<div class="sidebar-overlay" id="sidebar-overlay"></div>

<!-- 主内容 -->
<main>
    <!-- 章节标题 -->
    <section class="chapter-hero">
        <div class="container">
            <div class="chapter-hero-content">
                <h1>长短期记忆网络：让AI学会遗忘与记忆</h1>
                <p>深入理解LSTM的精妙设计，一步步推导门控机制的数学原理</p>
                <div style="display: flex; justify-content: center; gap: 1.5rem; flex-wrap: wrap; margin-top: 2rem;">
                    <span style="background: rgba(0, 0, 0, 0.1); padding: 0.75rem 1.5rem; border-radius: 30px; display: flex; align-items: center; gap: 0.5rem;">
                        <span style="font-size: 1.3rem;">🚪</span> 门控机制
                    </span>
                    <span style="background: rgba(0, 0, 0, 0.1); padding: 0.75rem 1.5rem; border-radius: 30px; display: flex; align-items: center; gap: 0.5rem;">
                        <span style="font-size: 1.3rem;">🧠</span> 长期记忆
                    </span>
                    <span style="background: rgba(0, 0, 0, 0.1); padding: 0.75rem 1.5rem; border-radius: 30px; display: flex; align-items: center; gap: 0.5rem;">
                        <span style="font-size: 1.3rem;">📐</span> 数学推导
                    </span>
                </div>
            </div>
        </div>
    </section>

    <!-- 一句话理解LSTM -->
    <section style="background: var(--bg-dark); padding: 4rem 0 2rem 0;">
        <div class="container">
            <div class="key-concept-card" style="
                background: linear-gradient(135deg, #334155 0%, #475569 100%);
                border-radius: 1.5rem;
                padding: 3rem;
                margin: 0 auto;
                max-width: 1000px;
                box-shadow: 0 10px 30px rgba(0,0,0,0.3);
                border: 1px solid rgba(255,255,255,0.1);
            ">
                <h2 style="color: white; font-size: 1.8rem; margin-bottom: 2rem; text-align: center; display: flex; align-items: center; justify-content: center; gap: 1rem;">
                    <span style="font-size: 2rem;">💡</span>
                    一句话理解LSTM
                </h2>

                <p style="color: #e2e8f0; font-size: 1.3rem; line-height: 1.8; text-align: center; margin-bottom: 3rem;">
                    LSTM就像一个精密的记忆管理系统，<br>
                    通过三个门控制信息的流动：<br>
                    <span style="color: #f5576c;">遗忘门</span>决定忘记什么，
                    <span style="color: #4facfe;">输入门</span>决定记住什么，
                    <span style="color: #43e97b;">输出门</span>决定输出什么。
                </p>

                <!-- 四个核心概念 -->
                <div style="display: grid; grid-template-columns: repeat(4, 1fr); gap: 2rem;">
                    <div style="text-align: center;">
                        <div style="
                            width: 80px;
                            height: 80px;
                            margin: 0 auto 1rem;
                            background: var(--forget-gradient);
                            border-radius: 20px;
                            display: flex;
                            align-items: center;
                            justify-content: center;
                            font-size: 2.5rem;
                            color: white;
                        ">
                            🚪
                        </div>
                        <h4 style="color: white; margin-bottom: 0.5rem;">遗忘门</h4>
                        <p style="color: #94a3b8; font-size: 0.85rem; line-height: 1.4;">
                            选择性遗忘<br>过滤无用信息
                        </p>
                    </div>

                    <div style="text-align: center;">
                        <div style="
                            width: 80px;
                            height: 80px;
                            margin: 0 auto 1rem;
                            background: var(--input-gradient);
                            border-radius: 20px;
                            display: flex;
                            align-items: center;
                            justify-content: center;
                            font-size: 2.5rem;
                            color: white;
                        ">
                            ➕
                        </div>
                        <h4 style="color: white; margin-bottom: 0.5rem;">输入门</h4>
                        <p style="color: #94a3b8; font-size: 0.85rem; line-height: 1.4;">
                            选择性记忆<br>存储重要信息
                        </p>
                    </div>

                    <div style="text-align: center;">
                        <div style="
                            width: 80px;
                            height: 80px;
                            margin: 0 auto 1rem;
                            background: var(--output-gradient);
                            border-radius: 20px;
                            display: flex;
                            align-items: center;
                            justify-content: center;
                            font-size: 2.5rem;
                            color: white;
                        ">
                            📤
                        </div>
                        <h4 style="color: white; margin-bottom: 0.5rem;">输出门</h4>
                        <p style="color: #94a3b8; font-size: 0.85rem; line-height: 1.4;">
                            控制输出<br>决定展示什么
                        </p>
                    </div>

                    <div style="text-align: center;">
                        <div style="
                            width: 80px;
                            height: 80px;
                            margin: 0 auto 1rem;
                            background: var(--cell-gradient);
                            border-radius: 20px;
                            display: flex;
                            align-items: center;
                            justify-content: center;
                            font-size: 2.5rem;
                            color: white;
                        ">
                            📦
                        </div>
                        <h4 style="color: white; margin-bottom: 0.5rem;">细胞状态</h4>
                        <p style="color: #94a3b8; font-size: 0.85rem; line-height: 1.4;">
                            长期记忆<br>信息高速通道
                        </p>
                    </div>
                </div>

                <!-- 类比说明 -->
                <div style="
                    background: rgba(251, 191, 36, 0.15);
                    border: 2px solid rgba(251, 191, 36, 0.3);
                    border-radius: 1rem;
                    padding: 2rem;
                    margin-top: 2rem;
                    position: relative;
                ">
                    <div style="
                        position: absolute;
                        top: -15px;
                        left: 30px;
                        background: #475569;
                        padding: 0 15px;
                        border-radius: 20px;
                        font-size: 1.2rem;
                    ">
                        🌟 生活类比
                    </div>

                    <p style="color: #f1f5f9; font-size: 1.15rem; line-height: 1.8; margin: 0;">
                        <strong>LSTM就像你的大脑处理日常信息：</strong>
                    </p>

                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 1.5rem; margin-top: 1.5rem;">
                        <div style="background: rgba(255,255,255,0.05); padding: 1.5rem; border-radius: 0.75rem;">
                            <h5 style="color: #f5576c; margin-bottom: 0.5rem;">🗑️ 遗忘门 = 清理垃圾</h5>
                            <p style="color: #cbd5e1; font-size: 0.95rem;">
                                就像清理手机照片，删除模糊的、重复的，只保留有意义的
                            </p>
                        </div>

                        <div style="background: rgba(255,255,255,0.05); padding: 1.5rem; border-radius: 0.75rem;">
                            <h5 style="color: #4facfe; margin-bottom: 0.5rem;">📸 输入门 = 拍照存储</h5>
                            <p style="color: #cbd5e1; font-size: 0.95rem;">
                                看到美景时决定是否拍照保存，以及拍照的清晰度
                            </p>
                        </div>

                        <div style="background: rgba(255,255,255,0.05); padding: 1.5rem; border-radius: 0.75rem;">
                            <h5 style="color: #43e97b; margin-bottom: 0.5rem;">🖼️ 输出门 = 分享照片</h5>
                            <p style="color: #cbd5e1; font-size: 0.95rem;">
                                从相册中选择合适的照片分享给朋友
                            </p>
                        </div>

                        <div style="background: rgba(255,255,255,0.05); padding: 1.5rem; border-radius: 0.75rem;">
                            <h5 style="color: #fa709a; margin-bottom: 0.5rem;">💾 细胞状态 = 相册</h5>
                            <p style="color: #cbd5e1; font-size: 0.95rem;">
                                存储所有照片的相册，可以长期保存重要记忆
                            </p>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </section>

    <!-- 章节内容 -->
    <div style="background: var(--bg-dark); padding-top: 100px;">
        <div class="container">

            <!-- 学习目标 -->
            <section class="section-card">
                <h2 style="color: var(--accent-pink); margin-bottom: 1.5rem;">📚 本章学习目标</h2>
                <div class="concept-grid">
                    <div class="concept-card forget">
                        <h3 style="color: var(--accent-pink); margin-bottom: 0.5rem;">✓ 理解动机</h3>
                        <p>为什么需要LSTM，它解决了什么问题</p>
                    </div>
                    <div class="concept-card input">
                        <h3 style="color: var(--accent-blue); margin-bottom: 0.5rem;">✓ 推导设计</h3>
                        <p>一步步推导LSTM的门控机制</p>
                    </div>
                    <div class="concept-card output">
                        <h3 style="color: var(--accent-green); margin-bottom: 0.5rem;">✓ 数学证明</h3>
                        <p>理解LSTM如何解决梯度消失</p>
                    </div>
                    <div class="concept-card cell">
                        <h3 style="color: var(--accent-orange); margin-bottom: 0.5rem;">✓ 实现应用</h3>
                        <p>从零实现LSTM并应用于实际问题</p>
                    </div>
                </div>
            </section>

            <!-- 序言：记忆的艺术 -->
            <section id="intro" class="section-card">
                <div class="story-card">
                    <h2 style="color: var(--accent-pink); margin-bottom: 1.5rem;">📖 序言：记忆的艺术</h2>

                    <p style="font-size: 1.1rem; line-height: 1.8; margin-bottom: 2rem;">
                        1997年，德国慕尼黑工业大学的一个研究小组正在为一个棘手的问题苦恼...
                    </p>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #f093fb, #f5576c);">👨‍🔬</div>
                            Sepp Hochreiter：
                        </div>
                        <div>"我们的RNN在处理长句子时总是失败。比如这个德语句子，动词在最后，但网络在处理到动词时已经忘记了主语是什么！"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #4facfe, #00f2fe);">👨‍🏫</div>
                            Jürgen Schmidhuber：
                        </div>
                        <div>"问题的根源是梯度消失。当误差信号通过时间反向传播时，它会指数级衰减。我们需要一种新的架构！"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #fa709a, #fee140);">🧑‍🔬</div>
                            研究生：
                        </div>
                        <div>"如果我们能创建一条'高速公路'，让梯度可以无损地传播呢？就像...就像水管中的阀门，可以控制水流！"</div>
                    </div>

                    <div class="timeline mt-4">
                        <div class="timeline-item">
                            <div class="timeline-dot"></div>
                            <h4 style="color: var(--accent-pink);">1991年</h4>
                            <p>Hochreiter发现梯度消失问题</p>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-dot"></div>
                            <h4 style="color: var(--accent-blue);">1997年</h4>
                            <p>LSTM论文发表，引入门控机制</p>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-dot"></div>
                            <h4 style="color: var(--accent-green);">2000年</h4>
                            <p>添加遗忘门，LSTM结构完善</p>
                        </div>
                        <div class="timeline-item">
                            <div class="timeline-dot"></div>
                            <h4 style="color: var(--accent-orange);">2014年</h4>
                            <p>LSTM在机器翻译中大放异彩</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- RNN的困境 -->
            <section id="problem" class="section-card">
                <h2 style="color: var(--accent-red); margin-bottom: 1.5rem;">💔 RNN的困境：为什么需要LSTM？</h2>

                <div class="story-card">
                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #4facfe, #00f2fe);">🧑‍💻</div>
                            学生：
                        </div>
                        <div>"老师，我训练RNN处理这个句子：'我在1990年出生于北京，现在2024年我住在...'，但它总是预测不出'北京'！"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #f093fb, #f5576c);">👨‍🏫</div>
                            老师：
                        </div>
                        <div>"这就是长期依赖问题！让我们分析一下为什么会这样..."</div>
                    </div>
                </div>

                <div class="math-container">
                    <h3 style="color: var(--accent-red); margin-bottom: 1.5rem;">🔬 梯度消失的数学本质</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>RNN的梯度传播 - 矩阵形式</h4>
                        <p>回忆RNN的隐藏状态更新公式：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">RNN前向传播</div>
                            <div class="katex-display">
                                $h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$
                            </div>
                        </div>
                        <div class="math-explanation" style="margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">维度分析：</h4>
                            <ul style="margin-left: 2rem; line-height: 2;">
                                <li>$h_t \in \mathbb{R}^{d_h \times 1}$ : 隐藏状态向量</li>
                                <li>$W_{hh} \in \mathbb{R}^{d_h \times d_h}$ : 隐藏到隐藏的权重矩阵</li>
                                <li>$W_{xh} \in \mathbb{R}^{d_h \times d_x}$ : 输入到隐藏的权重矩阵</li>
                                <li>$x_t \in \mathbb{R}^{d_x \times 1}$ : 输入向量</li>
                            </ul>
                        </div>
                        <p style="margin-top: 1.5rem;">当我们计算损失对早期隐藏状态的梯度时：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">梯度链式法则</div>
                            <div class="katex-display">
                                $\frac{\partial L}{\partial h_1} = \frac{\partial L}{\partial h_T} \prod_{t=2}^{T} \frac{\partial h_t}{\partial h_{t-1}}$
                            </div>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>梯度的矩阵分解</h4>
                        <p>对于每一项 $\frac{\partial h_t}{\partial h_{t-1}}$，使用矩阵微分：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">单步梯度</div>
                            <div class="katex-display">
                                $\frac{\partial h_t}{\partial h_{t-1}} = \text{diag}(\tanh'(z_t)) \cdot W_{hh}$
                            </div>
                        </div>
                        <p>其中 $z_t = W_{hh} h_{t-1} + W_{xh} x_t + b_h$</p>

                        <div style="background: rgba(239, 68, 68, 0.1); padding: 1.5rem; border-radius: 0.75rem; margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-red); margin-bottom: 1rem;">矩阵范数分析：</h4>
                            <p>由于 $\tanh'(x) = 1 - \tanh^2(x) \in (0, 1]$，我们有：</p>
                            <div class="katex-display">
                                $\left\|\frac{\partial h_t}{\partial h_{t-1}}\right\|_2 \leq \|\text{diag}(\tanh'(z_t))\|_2 \cdot \|W_{hh}\|_2 \leq \|W_{hh}\|_2$
                            </div>
                            <p>如果 $\|W_{hh}\|_2 < 1$（谱范数小于1），梯度将指数衰减。</p>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>长序列的梯度爆炸/消失</h4>
                        <p>经过T个时间步的累积效应：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">梯度累积</div>
                            <div class="katex-display">
                                $\left\|\frac{\partial L}{\partial h_1}\right\|_2 \leq \left\|\frac{\partial L}{\partial h_T}\right\|_2 \prod_{t=2}^{T} \|\text{diag}(\tanh'(z_t))\|_2 \cdot \|W_{hh}\|_2$
                            </div>
                        </div>

                        <div class="tip error" style="margin-top: 1.5rem;">
                            <span class="tip-icon">⚠️</span>
                            <div class="tip-content">
                                <strong>关键问题：</strong><br>
                                • 如果 $\|W_{hh}\|_2 < 1$：梯度消失 → $\left\|\frac{\partial L}{\partial h_1}\right\|_2 \approx 0$<br>
                                • 如果 $\|W_{hh}\|_2 > 1$：梯度爆炸 → $\left\|\frac{\partial L}{\partial h_1}\right\|_2 \to \infty$<br>
                                • 即使 $\|W_{hh}\|_2 = 1$，$\tanh'$ 的压缩效应仍会导致梯度消失
                            </div>
                        </div>
                    </div>
                </div>

                <div class="think-box mt-3">
                    <h3 style="color: var(--accent-purple); margin-bottom: 1rem;">💭 关键问题</h3>
                    <p><strong>RNN的根本缺陷：</strong></p>
                    <ol style="margin-left: 2rem; margin-top: 1rem;">
                        <li>信息必须经过所有中间步骤才能传递</li>
                        <li>每一步都会损失一些信息（梯度衰减）</li>
                        <li>tanh激活函数加剧了梯度消失</li>
                        <li>无法选择性地保留重要信息</li>
                    </ol>
                </div>
            </section>

            <!-- 解决思路推导 -->
            <section id="solution-thinking" class="section-card">
                <h2 style="color: var(--accent-blue); margin-bottom: 1.5rem;">💡 解决思路：如何设计更好的记忆机制？</h2>

                <div class="story-card">
                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #fa709a, #fee140);">🧑‍🔬</div>
                            研究员：
                        </div>
                        <div>"让我们从人类的记忆机制中寻找灵感。人是如何处理长期记忆的？"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #4facfe, #00f2fe);">👩‍🔬</div>
                            认知科学家：
                        </div>
                        <div>"人类记忆有三个关键特征：<br>
                            1. 选择性遗忘 - 忘记不重要的细节<br>
                            2. 选择性记忆 - 记住重要的信息<br>
                            3. 按需提取 - 在需要时调用记忆"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #f093fb, #f5576c);">👨‍🔬</div>
                            Hochreiter：
                        </div>
                        <div>"太棒了！如果我们为RNN添加这三种能力呢？"</div>
                    </div>
                </div>

                <div class="derivation-box">
                    <h3 style="color: var(--accent-blue); margin-bottom: 1.5rem;">🎯 核心设计思想</h3>

                    <div class="think-box">
                        <h4>思考1：如何让信息无损传递？</h4>
                        <p>创建一条"高速公路" - <strong>细胞状态(Cell State)</strong>：</p>
                        <ul style="margin-left: 2rem; margin-top: 1rem;">
                            <li>线性传播，避免非线性激活函数</li>
                            <li>只通过加法和乘法操作</li>
                            <li>梯度可以无损地反向传播</li>
                        </ul>
                    </div>

                    <div class="think-box mt-3">
                        <h4>思考2：如何控制信息流动？</h4>
                        <p>引入"门控机制" - <strong>使用Sigmoid函数作为开关</strong>：</p>
                        <ul style="margin-left: 2rem; margin-top: 1rem;">
                            <li>Sigmoid输出在[0,1]之间，像百分比</li>
                            <li>0 = 完全关闭，1 = 完全打开</li>
                            <li>可学习的参数决定开关程度</li>
                        </ul>
                    </div>

                    <div class="think-box mt-3">
                        <h4>思考3：需要哪些门？</h4>
                        <p>基于人类记忆机制，我们需要：</p>
                        <ol style="margin-left: 2rem; margin-top: 1rem;">
                            <li><strong>遗忘门</strong>：决定从细胞状态中丢弃什么信息</li>
                            <li><strong>输入门</strong>：决定什么新信息要存储到细胞状态</li>
                            <li><strong>输出门</strong>：决定基于细胞状态输出什么</li>
                        </ol>
                    </div>
                </div>
            </section>

            <!-- 遗忘门的诞生 -->
            <section id="forget-gate" class="section-card">
                <h2 style="color: var(--accent-pink); margin-bottom: 1.5rem;">🚪 遗忘门：学会忘记</h2>

                <div class="story-card">
                    <p style="font-size: 1.1rem; line-height: 1.8; margin-bottom: 1.5rem;">
                        让我们一步步推导遗忘门的设计...
                    </p>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #f093fb, #f5576c);">👨‍🔬</div>
                            设计者：
                        </div>
                        <div>"首先，我们需要决定从上一时刻的细胞状态 $C_{t-1}$ 中忘记什么。这个决定应该基于什么？"</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #4facfe, #00f2fe);">👩‍🔬</div>
                            同事：
                        </div>
                        <div>"应该基于当前输入 $x_t$ 和上一时刻的隐藏状态 $h_{t-1}$！比如在语言模型中，看到新的主语时就应该忘记旧的主语。"</div>
                    </div>
                </div>

                <div class="math-container">
                    <h3 style="color: var(--accent-pink); margin-bottom: 1.5rem;">📐 遗忘门的数学推导</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>设计目标 - 控制信息遗忘</h4>
                        <p>我们需要一个门控机制，基于当前输入和历史信息决定保留多少过去的记忆：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">遗忘门的输入</div>
                            <div class="katex-display">
                                $\text{输入: } [h_{t-1}, x_t] \in \mathbb{R}^{(d_h + d_x) \times 1} \quad \text{输出: } f_t \in [0,1]^{d_c}$
                            </div>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>线性变换 - 矩阵形式</h4>
                        <p>将拼接的输入映射到细胞状态维度：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">遗忘门变换</div>
                            <div class="katex-display">
                                $z_f = W_f \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_f = \begin{bmatrix} W_{fh} & W_{fx} \end{bmatrix} \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_f$
                            </div>
                        </div>
                        <div class="math-explanation" style="margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">参数维度：</h4>
                            <ul style="margin-left: 2rem; line-height: 2;">
                                <li>$W_f \in \mathbb{R}^{d_c \times (d_h + d_x)}$ : 遗忘门权重矩阵</li>
                                <li>$W_{fh} \in \mathbb{R}^{d_c \times d_h}$ : 隐藏状态部分</li>
                                <li>$W_{fx} \in \mathbb{R}^{d_c \times d_x}$ : 输入部分</li>
                                <li>$b_f \in \mathbb{R}^{d_c \times 1}$ : 偏置向量</li>
                                <li>$d_c$ : 细胞状态维度（通常等于 $d_h$）</li>
                            </ul>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>激活函数 - 逐元素Sigmoid</h4>
                        <p>应用sigmoid函数生成门控信号：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">遗忘门激活</div>
                            <div class="katex-display">
                                $f_t = \sigma(z_f) = \sigma\left(W_f \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_f\right)$
                            </div>
                        </div>
                        <p>其中 $\sigma(x) = \frac{1}{1 + e^{-x}}$ 是逐元素应用的sigmoid函数。</p>

                        <div class="think-box" style="margin-top: 1.5rem;">
                            <h4>为什么选择Sigmoid？</h4>
                            <ul style="margin-left: 2rem; margin-top: 1rem;">
                                <li>输出范围 $[0, 1]$ 完美匹配"门"的概念</li>
                                <li>可微分，支持梯度下降</li>
                                <li>0 = 完全遗忘，1 = 完全保留</li>
                                <li>平滑过渡，避免硬切换</li>
                            </ul>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">4</div>
                        <h4>应用遗忘 - Hadamard积</h4>
                        <p>通过逐元素乘法选择性遗忘信息：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">细胞状态遗忘</div>
                            <div class="katex-display">
                                $C'_t = f_t \odot C_{t-1} = \begin{bmatrix}
                                f_t^{(1)} \cdot C_{t-1}^{(1)} \\
                                f_t^{(2)} \cdot C_{t-1}^{(2)} \\
                                \vdots \\
                                f_t^{(d_c)} \cdot C_{t-1}^{(d_c)}
                                \end{bmatrix}$
                            </div>
                        </div>
                        <p>每个细胞单元独立控制其遗忘程度。</p>
                    </div>
                </div>

                <div class="lstm-gate-viz">
                    <div class="gate-component forget">
                        <div class="gate-icon">🗑️</div>
                        <h3>遗忘门</h3>
                        <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                            决定忘记什么<br>
                            f<sub>t</sub> ∈ [0, 1]
                        </p>
                    </div>
                </div>
            </section>

            <!-- 输入门的设计 -->
            <section id="input-gate" class="section-card">
                <h2 style="color: var(--accent-blue); margin-bottom: 1.5rem;">➕ 输入门：选择记忆</h2>

                <div class="story-card">
                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #4facfe, #00f2fe);">👩‍🔬</div>
                            设计者：
                        </div>
                        <div>"现在我们需要决定存储什么新信息。但这里有个问题：我们需要两个东西..."</div>
                    </div>

                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #f093fb, #f5576c);">👨‍🔬</div>
                            同事：
                        </div>
                        <div>"对！我们需要：<br>
                            1. 候选记忆内容 - 要记住什么<br>
                            2. 重要性权重 - 记住多少"</div>
                    </div>
                </div>

                <div class="math-container">
                    <h3 style="color: var(--accent-blue); margin-bottom: 1.5rem;">📐 输入门的双重设计</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>候选细胞状态 - 新信息编码</h4>
                        <p>首先创建候选记忆内容，使用tanh激活函数：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">候选记忆计算</div>
                            <div class="katex-display">
                                $\tilde{C}_t = \tanh\left(W_C \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_C\right)$
                            </div>
                        </div>
                        <div class="math-explanation" style="margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">矩阵展开：</h4>
                            <div class="katex-display">
                                $\tilde{C}_t = \tanh(W_{Ch} h_{t-1} + W_{Cx} x_t + b_C)$
                            </div>
                            <p>其中：</p>
                            <ul style="margin-left: 2rem; line-height: 2;">
                                <li>$W_C \in \mathbb{R}^{d_c \times (d_h + d_x)}$ : 候选值权重矩阵</li>
                                <li>$\tilde{C}_t \in \mathbb{R}^{d_c \times 1}$ : 候选细胞状态</li>
                                <li>$\tanh$ : 确保输出在 $[-1, 1]$ 范围内</li>
                            </ul>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>输入门控制 - 重要性权重</h4>
                        <p>决定候选值的哪些部分要更新到细胞状态：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">输入门激活</div>
                            <div class="katex-display">
                                $i_t = \sigma\left(W_i \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_i\right)$
                            </div>
                        </div>
                        <p>输入门参数：$W_i \in \mathbb{R}^{d_c \times (d_h + d_x)}$，$b_i \in \mathbb{R}^{d_c \times 1}$</p>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>选择性更新 - 矩阵视角</h4>
                        <p>新增记忆通过Hadamard积实现选择性加入：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">输入调制</div>
                            <div class="katex-display">
                                $\text{新增记忆} = i_t \odot \tilde{C}_t = \begin{bmatrix}
                                i_t^{(1)} \cdot \tilde{C}_t^{(1)} \\
                                i_t^{(2)} \cdot \tilde{C}_t^{(2)} \\
                                \vdots \\
                                i_t^{(d_c)} \cdot \tilde{C}_t^{(d_c)}
                                \end{bmatrix}$
                            </div>
                        </div>

                        <div class="think-box" style="margin-top: 1.5rem;">
                            <h4>设计智慧：为什么分离内容和权重？</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin-top: 1rem;">
                                <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 0.5rem;">
                                    <h5 style="color: var(--accent-blue); margin-bottom: 0.5rem;">候选值 $\tilde{C}_t$</h5>
                                    <p>• 编码"是什么"信息<br>• 使用tanh保持数值范围<br>• 可以是正或负</p>
                                </div>
                                <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 0.5rem;">
                                    <h5 style="color: var(--accent-green); margin-bottom: 0.5rem;">输入门 $i_t$</h5>
                                    <p>• 决定"重要性"<br>• 使用sigmoid生成权重<br>• 范围[0,1]控制强度</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="lstm-gate-viz">
                    <div class="gate-component input">
                        <div class="gate-icon">📝</div>
                        <h3>输入门</h3>
                        <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                            决定存储什么<br>
                            i<sub>t</sub> × C̃<sub>t</sub>
                        </p>
                    </div>
                </div>
            </section>

            <!-- 细胞状态更新 -->
            <section id="cell-state" class="section-card">
                <h2 style="color: var(--accent-orange); margin-bottom: 1.5rem;">📦 细胞状态更新：长期记忆的高速公路</h2>

                <div class="story-card">
                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #fa709a, #fee140);">🧑‍🔬</div>
                            设计者：
                        </div>
                        <div>"现在我们有了遗忘门和输入门，是时候更新细胞状态了！"</div>
                    </div>
                </div>

                <div class="math-container">
                    <h3 style="color: var(--accent-orange); margin-bottom: 1.5rem;">📐 细胞状态的更新公式</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>组合遗忘和输入 - 核心更新方程</h4>
                        <p>新的细胞状态由两部分线性组合而成：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">细胞状态更新</div>
                            <div class="katex-display">
                                $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$
                            </div>
                        </div>
                        <div class="math-explanation" style="margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">矩阵形式展开：</h4>
                            <div class="katex-display">
                                $C_t = \begin{bmatrix}
                                f_t^{(1)} C_{t-1}^{(1)} + i_t^{(1)} \tilde{C}_t^{(1)} \\
                                f_t^{(2)} C_{t-1}^{(2)} + i_t^{(2)} \tilde{C}_t^{(2)} \\
                                \vdots \\
                                f_t^{(d_c)} C_{t-1}^{(d_c)} + i_t^{(d_c)} \tilde{C}_t^{(d_c)}
                                \end{bmatrix}$
                            </div>
                            <p>每个维度独立更新，实现细粒度的记忆控制。</p>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>梯度传播特性 - 为什么能解决梯度消失</h4>
                        <p>计算细胞状态的递归梯度：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">梯度传播</div>
                            <div class="katex-display">
                                $\frac{\partial C_t}{\partial C_{t-1}} = \text{diag}(f_t)$
                            </div>
                        </div>
                        <div style="background: rgba(34, 197, 94, 0.1); padding: 1.5rem; border-radius: 0.75rem; margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-green); margin-bottom: 1rem;">关键洞察：梯度高速公路</h4>
                            <ul style="margin-left: 2rem;">
                                <li>梯度通过 $f_t$ 直接传播，<strong>不经过任何激活函数</strong></li>
                                <li>如果 $f_t^{(i)} \approx 1$，第 $i$ 维梯度几乎无损传播</li>
                                <li>网络可以学习让重要信息的 $f_t$ 接近 1</li>
                                <li>这创建了一条"梯度高速公路"</li>
                            </ul>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>数值稳定性分析 - 矩阵范数视角</h4>
                        <p>分析细胞状态的范数变化：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">范数分析</div>
                            <div class="katex-display">
                                $\|C_t\|_2^2 = \sum_{i=1}^{d_c} \left(f_t^{(i)} C_{t-1}^{(i)} + i_t^{(i)} \tilde{C}_t^{(i)}\right)^2$
                            </div>
                        </div>
                        <p>由于约束条件：</p>
                        <ul style="margin-left: 2rem; margin-top: 1rem;">
                            <li>$f_t^{(i)}, i_t^{(i)} \in [0, 1]$ （sigmoid输出）</li>
                            <li>$\tilde{C}_t^{(i)} \in [-1, 1]$ （tanh输出）</li>
                            <li>通常有 $f_t^{(i)} + i_t^{(i)} \approx 1$（互补关系）</li>
                        </ul>
                        <p>因此细胞状态保持数值稳定，不会爆炸或消失。</p>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">4</div>
                        <h4>长程梯度传播 - 累积效应</h4>
                        <p>经过 $k$ 个时间步的梯度传播：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">多步梯度</div>
                            <div class="katex-display">
                                $\frac{\partial C_{t+k}}{\partial C_t} = \prod_{j=1}^{k} \text{diag}(f_{t+j}) = \text{diag}\left(\prod_{j=1}^{k} f_{t+j}\right)$
                            </div>
                        </div>
                        <p>第 $i$ 维的梯度衰减率：$\prod_{j=1}^{k} f_{t+j}^{(i)}$</p>
                        <p>如果网络学会对重要信息维持 $f_t^{(i)} \approx 0.9$，则 100 步后梯度仍有 $0.9^{100} \approx 2.7 \times 10^{-5}$，远好于 RNN！</p>
                    </div>
                </div>

                <div class="visualization-container">
                    <h3 style="color: var(--accent-cyan); margin-bottom: 1.5rem;">🛤️ 信息高速公路</h3>
                    <canvas id="cell-state-highway" width="800" height="300"></canvas>
                </div>
            </section>

            <!-- 输出门的控制 -->
            <section id="output-gate" class="section-card">
                <h2 style="color: var(--accent-green); margin-bottom: 1.5rem;">📤 输出门：按需提取</h2>

                <div class="story-card">
                    <div class="dialogue-box">
                        <div class="speaker">
                            <div class="avatar" style="background: linear-gradient(135deg, #43e97b, #38f9d7);">👨‍🔬</div>
                            设计者：
                        </div>
                        <div>"细胞状态包含了所有记忆，但我们不需要在每个时刻输出所有信息。我们需要一个过滤器！"</div>
                    </div>
                </div>

                <div class="math-container">
                    <h3 style="color: var(--accent-green); margin-bottom: 1.5rem;">📐 输出门的设计</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>输出门计算 - 控制信息输出</h4>
                        <p>基于当前输入和上一隐藏状态决定输出什么：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">输出门激活</div>
                            <div class="katex-display">
                                $o_t = \sigma\left(W_o \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b_o\right)$
                            </div>
                        </div>
                        <p>参数：$W_o \in \mathbb{R}^{d_h \times (d_h + d_x)}$，$b_o \in \mathbb{R}^{d_h \times 1}$</p>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>隐藏状态生成 - 选择性输出</h4>
                        <p>将细胞状态通过tanh压缩后，用输出门过滤：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">隐藏状态计算</div>
                            <div class="katex-display">
                                $h_t = o_t \odot \tanh(C_t)$
                            </div>
                        </div>

                        <div class="math-explanation" style="margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">设计原理：</h4>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem;">
                                <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 0.5rem;">
                                    <h5 style="color: var(--accent-green); margin-bottom: 0.5rem;">为什么要 tanh(C_t)？</h5>
                                    <ul style="font-size: 0.9rem;">
                                        <li>将细胞状态映射到 [-1, 1]</li>
                                        <li>与其他激活保持一致</li>
                                        <li>非线性变换增加表达能力</li>
                                    </ul>
                                </div>
                                <div style="background: rgba(255, 255, 255, 0.05); padding: 1rem; border-radius: 0.5rem;">
                                    <h5 style="color: var(--accent-blue); margin-bottom: 0.5rem;">输出门的作用</h5>
                                    <ul style="font-size: 0.9rem;">
                                        <li>控制哪些记忆要输出</li>
                                        <li>实现信息的选择性展示</li>
                                        <li>保护敏感的内部状态</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>梯度传播分析 - 输出路径</h4>
                        <p>隐藏状态对细胞状态的梯度：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">输出梯度</div>
                            <div class="katex-display">
                                $\frac{\partial h_t}{\partial C_t} = \text{diag}(o_t) \cdot \text{diag}(1 - \tanh^2(C_t))$
                            </div>
                        </div>
                        <p>注意：虽然这里有 tanh 的导数，但细胞状态 $C_t$ 的梯度主要通过直接路径传播，不受此影响。</p>
                    </div>
                </div>

                <div class="lstm-gate-viz">
                    <div class="gate-component output">
                        <div class="gate-icon">🚀</div>
                        <h3>输出门</h3>
                        <p style="font-size: 0.9rem; margin-top: 0.5rem;">
                            决定输出什么<br>
                            h<sub>t</sub> = o<sub>t</sub> × tanh(C<sub>t</sub>)
                        </p>
                    </div>
                </div>
            </section>

            <!-- LSTM完整结构 -->
            <section id="complete-lstm" class="section-card">
                <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🏗️ LSTM完整结构</h2>

                <div class="math-container">
                    <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📐 LSTM的完整方程组</h3>

                    <div class="math-formula">
                        <div class="math-formula-title">LSTM完整公式 - 矩阵形式</div>
                        <div style="padding: 2rem; font-size: 1.1rem;">
                            <div class="katex-display">
                                $\begin{aligned}
                                f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) & \text{遗忘门} \\
                                i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) & \text{输入门} \\
                                \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) & \text{候选值} \\
                                C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t & \text{细胞状态} \\
                                o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) & \text{输出门} \\
                                h_t &= o_t \odot \tanh(C_t) & \text{隐藏状态}
                                \end{aligned}$
                            </div>
                        </div>
                    </div>

                    <div class="math-explanation" style="margin-top: 2rem;">
                        <h4 style="color: var(--accent-cyan); margin-bottom: 1rem;">参数统计：</h4>
                        <p>设 $d_h$ 为隐藏层维度，$d_x$ 为输入维度，LSTM的参数量为：</p>
                        <div class="katex-display">
                            $\text{参数总数} = 4 \times (d_h \times (d_h + d_x) + d_h) = 4 \times d_h \times (d_h + d_x + 1)$
                        </div>
                        <p>相比RNN增加了4倍（四个门的参数）。</p>
                    </div>

                    <div class="derivation-step" style="margin-top: 2rem;">
                        <div class="derivation-step-number">★</div>
                        <h4>紧凑矩阵表示</h4>
                        <p>实际实现中，常将所有门的计算合并：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">优化实现</div>
                            <div class="katex-display">
                                $\begin{bmatrix} f_t \\ i_t \\ \tilde{C}_t \\ o_t \end{bmatrix} = \begin{bmatrix} \sigma \\ \sigma \\ \tanh \\ \sigma \end{bmatrix} \left( W \begin{bmatrix} h_{t-1} \\ x_t \end{bmatrix} + b \right)$
                            </div>
                        </div>
                        <p>其中 $W \in \mathbb{R}^{4d_h \times (d_h + d_x)}$，$b \in \mathbb{R}^{4d_h \times 1}$</p>
                    </div>
                </div>

                <div class="comparison-table">
                    <table>
                        <thead>
                        <tr>
                            <th>组件</th>
                            <th>作用</th>
                            <th>激活函数</th>
                            <th>输出范围</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td><strong>遗忘门 $f_t$</strong></td>
                            <td>决定忘记什么</td>
                            <td>Sigmoid</td>
                            <td>[0, 1]</td>
                        </tr>
                        <tr>
                            <td><strong>输入门 $i_t$</strong></td>
                            <td>决定存储的权重</td>
                            <td>Sigmoid</td>
                            <td>[0, 1]</td>
                        </tr>
                        <tr>
                            <td><strong>候选值 $\tilde{C}_t$</strong></td>
                            <td>新的记忆内容</td>
                            <td>Tanh</td>
                            <td>[-1, 1]</td>
                        </tr>
                        <tr>
                            <td><strong>输出门 $o_t$</strong></td>
                            <td>决定输出什么</td>
                            <td>Sigmoid</td>
                            <td>[0, 1]</td>
                        </tr>
                        </tbody>
                    </table>
                </div>

                <div class="visualization-container mt-3">
                    <h3 style="color: var(--accent-cyan); margin-bottom: 1.5rem;">🎯 LSTM信息流动图</h3>
                    <canvas id="lstm-flow-diagram" width="900" height="500"></canvas>
                </div>
            </section>

            <!-- 数学证明 -->
            <section id="math-proof" class="section-card">
                <h2 style="color: var(--accent-red); margin-bottom: 1.5rem;">🔬 数学证明：LSTM如何解决梯度消失</h2>

                <div class="math-container">
                    <h3 style="color: var(--accent-red); margin-bottom: 1.5rem;">📐 梯度传播分析</h3>

                    <div class="derivation-step">
                        <div class="derivation-step-number">1</div>
                        <h4>BPTT中的梯度路径</h4>
                        <p>LSTM中存在两条梯度传播路径：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">双路径梯度</div>
                            <div class="katex-display">
                                $\frac{\partial L}{\partial C_t} = \underbrace{\frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial C_t}}_{\text{当前时刻贡献}} + \underbrace{\frac{\partial L}{\partial C_{t+1}} \frac{\partial C_{t+1}}{\partial C_t}}_{\text{未来时刻贡献}}$
                            </div>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">2</div>
                        <h4>细胞状态的递归梯度 - 核心创新</h4>
                        <p>从 $C_{t+1} = f_{t+1} \odot C_t + i_{t+1} \odot \tilde{C}_{t+1}$，计算偏导：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">梯度传播核心</div>
                            <div class="katex-display">
                                $\frac{\partial C_{t+1}}{\partial C_t} = \text{diag}(f_{t+1})$
                            </div>
                        </div>
                        <div style="background: rgba(34, 197, 94, 0.15); padding: 1.5rem; border-radius: 0.75rem; margin-top: 1.5rem;">
                            <h4 style="color: var(--accent-green); margin-bottom: 1rem;">🎯 LSTM的核心优势</h4>
                            <p>这是LSTM最关键的设计！梯度通过遗忘门 $f_{t+1}$ 直接传播：</p>
                            <ul style="margin-left: 2rem; margin-top: 1rem;">
                                <li><strong>无激活函数</strong>：避免了导数压缩</li>
                                <li><strong>可控衰减</strong>：网络学习 $f_t$ 的值</li>
                                <li><strong>选择性传播</strong>：每个维度独立控制</li>
                            </ul>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">3</div>
                        <h4>长期梯度传播 - 矩阵连乘分析</h4>
                        <p>经过 $k$ 个时间步后的梯度：</p>
                        <div class="math-formula">
                            <div class="math-formula-title">多步梯度传播</div>
                            <div class="katex-display">
                                $\frac{\partial C_{t+k}}{\partial C_t} = \prod_{j=1}^{k} \text{diag}(f_{t+j}) = \text{diag}\left(\prod_{j=1}^{k} f_{t+j}\right)$
                            </div>
                        </div>
                        <p>矩阵展开形式：</p>
                        <div class="katex-display">
                            $\frac{\partial C_{t+k}}{\partial C_t} = \begin{bmatrix}
                            \prod_{j=1}^{k} f_{t+j}^{(1)} & 0 & \cdots & 0 \\
                            0 & \prod_{j=1}^{k} f_{t+j}^{(2)} & \cdots & 0 \\
                            \vdots & \vdots & \ddots & \vdots \\
                            0 & 0 & \cdots & \prod_{j=1}^{k} f_{t+j}^{(d_c)}
                            \end{bmatrix}$
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">4</div>
                        <h4>与RNN的定量对比</h4>
                        <div class="comparison-table" style="margin-top: 1.5rem;">
                            <table>
                                <thead>
                                <tr>
                                    <th>模型</th>
                                    <th>梯度传播公式</th>
                                    <th>衰减因子</th>
                                    <th>100步后梯度</th>
                                </tr>
                                </thead>
                                <tbody>
                                <tr>
                                    <td><strong>RNN</strong></td>
                                    <td>$\prod_{j} W_{hh}^T \cdot \text{diag}(\tanh')$</td>
                                    <td>$\approx 0.5^k$ (典型值)</td>
                                    <td>$\approx 10^{-30}$ 💀</td>
                                </tr>
                                <tr>
                                    <td><strong>LSTM</strong></td>
                                    <td>$\prod_{j} \text{diag}(f_{t+j})$</td>
                                    <td>$\approx 0.9^k$ (可学习)</td>
                                    <td>$\approx 10^{-5}$ ✅</td>
                                </tr>
                                </tbody>
                            </table>
                        </div>
                    </div>

                    <div class="derivation-step">
                        <div class="derivation-step-number">5</div>
                        <h4>数学证明：LSTM解决梯度消失</h4>
                        <div class="think-box">
                            <h4>定理：LSTM的梯度保持能力</h4>
                            <p><strong>命题：</strong>对于适当初始化的LSTM，存在参数配置使得梯度可以传播任意长的距离。</p>
                            <p style="margin-top: 1rem;"><strong>证明：</strong></p>
                            <ol style="margin-left: 2rem; margin-top: 0.5rem;">
                                <li>令所有遗忘门 $f_t = \mathbf{1}$（全1向量）</li>
                                <li>令所有输入门 $i_t = \mathbf{0}$（全0向量）</li>
                                <li>则 $C_t = C_{t-1}$（完美记忆）</li>
                                <li>梯度传播：$\frac{\partial C_{t+k}}{\partial C_t} = I$（单位矩阵）</li>
                                <li>因此梯度无损传播！ □</li>
                            </ol>
                        </div>
                    </div>
                </div>

                <div class="tip success mt-3">
                    <span class="tip-icon">✅</span>
                    <div class="tip-content">
                        <strong>数学证明总结：</strong><br>
                        LSTM通过细胞状态创建了一条"梯度高速公路"。遗忘门可以学习接近1，让梯度几乎无损地传播。这就是LSTM能够学习长期依赖的数学原理！
                    </div>
                </div>
            </section>

            <!-- 从零实现LSTM -->
            <section id="implementation" class="section-card">
                <h2 style="color: var(--accent-yellow); margin-bottom: 1.5rem;">🛠️ 从零实现LSTM</h2>

                <div class="code-container">
                    <div class="code-header">
                        <div class="code-header-left">
                            <div class="code-tab active" data-tab="basic">基础实现</div>
                            <div class="code-tab" data-tab="optimized">优化版本</div>
                            <div class="code-tab" data-tab="usage">使用示例</div>
                        </div>
                    </div>

                    <div class="code-content" data-tab-content="basic">
                        <pre><code class="language-python">import numpy as np

class LSTMCell:
                            """LSTM单元的从零实现"""

    def __init__(self, input_dim, hidden_dim):
        """
        初始化LSTM单元

        参数:
        input_dim: 输入维度
        hidden_dim: 隐藏层维度
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # 初始化权重矩阵
        # 遗忘门参数
        self.Wf = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01
        self.bf = np.zeros((hidden_dim, 1))

        # 输入门参数
        self.Wi = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01
        self.bi = np.zeros((hidden_dim, 1))

        # 候选值参数
        self.Wc = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01
        self.bc = np.zeros((hidden_dim, 1))

        # 输出门参数
        self.Wo = np.random.randn(hidden_dim, input_dim + hidden_dim) * 0.01
        self.bo = np.zeros((hidden_dim, 1))

    def forward(self, x, h_prev, c_prev):
        """
        LSTM前向传播

        参数:
        x: 当前输入 (input_dim, 1)
        h_prev: 上一时刻隐藏状态 (hidden_dim, 1)
        c_prev: 上一时刻细胞状态 (hidden_dim, 1)

        返回:
        h: 当前隐藏状态
        c: 当前细胞状态
        cache: 用于反向传播的缓存
        """
        # 拼接输入
        concat = np.vstack((h_prev, x))  # (hidden_dim + input_dim, 1)

        # 遗忘门
        f = self.sigmoid(np.dot(self.Wf, concat) + self.bf)

        # 输入门
        i = self.sigmoid(np.dot(self.Wi, concat) + self.bi)

        # 候选值
        c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)

        # 更新细胞状态
        c = f * c_prev + i * c_tilde

        # 输出门
        o = self.sigmoid(np.dot(self.Wo, concat) + self.bo)

        # 计算隐藏状态
        h = o * np.tanh(c)

        # 缓存中间结果用于反向传播
        cache = {
            'x': x, 'h_prev': h_prev, 'c_prev': c_prev,
            'f': f, 'i': i, 'c_tilde': c_tilde, 'o': o, 'c': c,
            'concat': concat
        }

        return h, c, cache

    def backward(self, dh, dc_next, cache):
        """
        LSTM反向传播

        参数:
        dh: 隐藏状态的梯度
        dc_next: 来自未来的细胞状态梯度
        cache: 前向传播的缓存

        返回:
        dx: 输入的梯度
        dh_prev: 传递给前一时刻的隐藏状态梯度
        dc_prev: 传递给前一时刻的细胞状态梯度
        gradients: 参数梯度字典
        """
        # 解包缓存
        x = cache['x']
        h_prev = cache['h_prev']
        c_prev = cache['c_prev']
        f = cache['f']
        i = cache['i']
        c_tilde = cache['c_tilde']
        o = cache['o']
        c = cache['c']
        concat = cache['concat']

        # 计算各个门的梯度
        # 输出门梯度
        do = dh * np.tanh(c)
        do_input = do * o * (1 - o)  # sigmoid导数

        # 细胞状态梯度
        dc = dc_next + dh * o * (1 - np.tanh(c)**2)

        # 候选值梯度
        dc_tilde = dc * i
        dc_tilde_input = dc_tilde * (1 - c_tilde**2)  # tanh导数

        # 输入门梯度
        di = dc * c_tilde
        di_input = di * i * (1 - i)  # sigmoid导数

        # 遗忘门梯度
        df = dc * c_prev
        df_input = df * f * (1 - f)  # sigmoid导数

        # 计算参数梯度
        dWf = np.dot(df_input, concat.T)
        dbf = df_input

        dWi = np.dot(di_input, concat.T)
        dbi = di_input

        dWc = np.dot(dc_tilde_input, concat.T)
        dbc = dc_tilde_input

        dWo = np.dot(do_input, concat.T)
        dbo = do_input

        # 计算输入梯度
        dconcat = (np.dot(self.Wf.T, df_input) +
                   np.dot(self.Wi.T, di_input) +
                   np.dot(self.Wc.T, dc_tilde_input) +
                   np.dot(self.Wo.T, do_input))

        dh_prev = dconcat[:self.hidden_dim]
        dx = dconcat[self.hidden_dim:]

        dc_prev = dc * f

        gradients = {
            'dWf': dWf, 'dbf': dbf,
            'dWi': dWi, 'dbi': dbi,
            'dWc': dWc, 'dbc': dbc,
            'dWo': dWo, 'dbo': dbo
        }

        return dx, dh_prev, dc_prev, gradients

    @staticmethod
    def sigmoid(x):
        """Sigmoid激活函数"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # 避免溢出


class LSTM:
    """完整的LSTM模型"""

    def __init__(self, input_dim, hidden_dim, output_dim, seq_length):
        """
        初始化LSTM

        参数:
        input_dim: 输入维度
        hidden_dim: 隐藏层维度
        output_dim: 输出维度
        seq_length: 序列长度
        """
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.seq_length = seq_length

        # LSTM单元
        self.lstm_cell = LSTMCell(input_dim, hidden_dim)

        # 输出层参数
        self.Wy = np.random.randn(output_dim, hidden_dim) * 0.01
        self.by = np.zeros((output_dim, 1))

    def forward(self, X):
        """
        LSTM前向传播

        参数:
        X: 输入序列 (seq_length, input_dim)

        返回:
        outputs: 输出序列
        (h, c): 最终的隐藏状态和细胞状态
        """
        seq_length = X.shape[0]

        # 初始化
        h = np.zeros((self.hidden_dim, 1))
        c = np.zeros((self.hidden_dim, 1))

        outputs = []
        caches = []

        # 逐时间步处理
        for t in range(seq_length):
            x = X[t].reshape(-1, 1)
            h, c, cache = self.lstm_cell.forward(x, h, c)

            # 计算输出
            y = np.dot(self.Wy, h) + self.by

            outputs.append(y)
            caches.append(cache)

        return np.array(outputs), (h, c), caches

    def backward(self, X, Y, outputs, caches, learning_rate=0.01):
        """
        LSTM反向传播并更新参数
        """
        seq_length = len(caches)

        # 初始化梯度
        dh_next = np.zeros((self.hidden_dim, 1))
        dc_next = np.zeros((self.hidden_dim, 1))

        # 累积梯度
        dWf = np.zeros_like(self.lstm_cell.Wf)
        dbf = np.zeros_like(self.lstm_cell.bf)
        dWi = np.zeros_like(self.lstm_cell.Wi)
        dbi = np.zeros_like(self.lstm_cell.bi)
        dWc = np.zeros_like(self.lstm_cell.Wc)
        dbc = np.zeros_like(self.lstm_cell.bc)
        dWo = np.zeros_like(self.lstm_cell.Wo)
        dbo = np.zeros_like(self.lstm_cell.bo)
        dWy = np.zeros_like(self.Wy)
        dby = np.zeros_like(self.by)

        # 反向遍历时间步
        for t in reversed(range(seq_length)):
            # 输出层梯度
            dy = outputs[t] - Y[t].reshape(-1, 1)  # 假设MSE损失
            dWy += np.dot(dy, caches[t]['h'].T)
            dby += dy

            # LSTM梯度
            dh = np.dot(self.Wy.T, dy) + dh_next

            dx, dh_next, dc_next, grads = self.lstm_cell.backward(
                dh, dc_next, caches[t]
            )

            # 累积梯度
            dWf += grads['dWf']
            dbf += grads['dbf']
            dWi += grads['dWi']
            dbi += grads['dbi']
            dWc += grads['dWc']
            dbc += grads['dbc']
            dWo += grads['dWo']
            dbo += grads['dbo']

        # 梯度裁剪
        for dparam in [dWf, dbf, dWi, dbi, dWc, dbc, dWo, dbo, dWy, dby]:
            np.clip(dparam, -5, 5, out=dparam)

        # 更新参数
        self.lstm_cell.Wf -= learning_rate * dWf
        self.lstm_cell.bf -= learning_rate * dbf
        self.lstm_cell.Wi -= learning_rate * dWi
        self.lstm_cell.bi -= learning_rate * dbi
        self.lstm_cell.Wc -= learning_rate * dWc
        self.lstm_cell.bc -= learning_rate * dbc
        self.lstm_cell.Wo -= learning_rate * dWo
        self.lstm_cell.bo -= learning_rate * dbo
        self.Wy -= learning_rate * dWy
        self.by -= learning_rate * dby</code></pre>
                    </div>

                    <div class="code-content" data-tab-content="optimized" style="display: none;">
                        <pre><code class="language-python">import numpy as np

class OptimizedLSTM:
    """优化的LSTM实现，将所有门的计算合并"""

    def __init__(self, input_dim, hidden_dim, output_dim):
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim

        # 合并所有门的权重矩阵，提高计算效率
        # [f, i, c̃, o] 四个门的参数合并
        self.W = np.random.randn(4 * hidden_dim, input_dim + hidden_dim) * 0.01
        self.b = np.zeros((4 * hidden_dim, 1))

        # 输出层
        self.Wy = np.random.randn(output_dim, hidden_dim) * 0.01
        self.by = np.zeros((output_dim, 1))

        # Adam优化器参数
        self.mW, self.mb = np.zeros_like(self.W), np.zeros_like(self.b)
        self.vW, self.vb = np.zeros_like(self.W), np.zeros_like(self.b)
        self.mWy, self.mby = np.zeros_like(self.Wy), np.zeros_like(self.by)
        self.vWy, self.vby = np.zeros_like(self.Wy), np.zeros_like(self.by)
        self.t = 0

    def forward(self, X, h0=None, c0=None):
        """
        优化的前向传播

        参数:
        X: 输入序列 (batch_size, seq_length, input_dim)
        h0: 初始隐藏状态
        c0: 初始细胞状态
        """
        batch_size, seq_length, _ = X.shape

        if h0 is None:
            h0 = np.zeros((batch_size, self.hidden_dim))
        if c0 is None:
            c0 = np.zeros((batch_size, self.hidden_dim))

        H = np.zeros((batch_size, seq_length, self.hidden_dim))
        C = np.zeros((batch_size, seq_length, self.hidden_dim))

        h_prev = h0
        c_prev = c0

        for t in range(seq_length):
            # 拼接输入
            x_t = X[:, t, :]
            concat = np.hstack([h_prev, x_t])  # (batch_size, hidden_dim + input_dim)

            # 一次性计算所有门
            gates = np.dot(concat, self.W.T) + self.b.T  # (batch_size, 4 * hidden_dim)

            # 分割门
            f = self.sigmoid(gates[:, :self.hidden_dim])
            i = self.sigmoid(gates[:, self.hidden_dim:2*self.hidden_dim])
            c_tilde = np.tanh(gates[:, 2*self.hidden_dim:3*self.hidden_dim])
            o = self.sigmoid(gates[:, 3*self.hidden_dim:])

            # 更新状态
            c = f * c_prev + i * c_tilde
            h = o * np.tanh(c)

            H[:, t, :] = h
            C[:, t, :] = c

            h_prev = h
            c_prev = c

        # 输出层
        Y = np.zeros((batch_size, seq_length, self.output_dim))
        for t in range(seq_length):
            Y[:, t, :] = np.dot(H[:, t, :], self.Wy.T) + self.by.T

        return Y, H, C

    def train_step(self, X, Y_true, learning_rate=0.001):
        """单步训练"""
        # 前向传播
        Y_pred, H, C = self.forward(X)

        # 计算损失
        loss = np.mean((Y_pred - Y_true) ** 2)

        # 反向传播（简化版）
        dY = 2 * (Y_pred - Y_true) / Y_true.size

        # ... 完整的反向传播实现 ...

        # Adam更新
        self.t += 1
        beta1, beta2, eps = 0.9, 0.999, 1e-8

        # 更新参数（示例）
        self.mW = beta1 * self.mW + (1 - beta1) * dW
        self.vW = beta2 * self.vW + (1 - beta2) * (dW ** 2)
        mW_hat = self.mW / (1 - beta1 ** self.t)
        vW_hat = self.vW / (1 - beta2 ** self.t)
        self.W -= learning_rate * mW_hat / (np.sqrt(vW_hat) + eps)

        return loss

    @staticmethod
    def sigmoid(x):
        """数值稳定的sigmoid"""
        return np.where(x >= 0,
                       1 / (1 + np.exp(-x)),
                       np.exp(x) / (1 + np.exp(x)))

    def save_model(self, filepath):
        """保存模型参数"""
        np.savez(filepath,
                 W=self.W, b=self.b,
                 Wy=self.Wy, by=self.by)

    def load_model(self, filepath):
        """加载模型参数"""
        params = np.load(filepath)
        self.W = params['W']
        self.b = params['b']
        self.Wy = params['Wy']
        self.by = params['by']</code></pre>
                    </div>

                    <div class="code-content" data-tab-content="usage" style="display: none;">
                        <pre><code class="language-python"># 使用示例：情感分析

import numpy as np
from sklearn.preprocessing import LabelEncoder

# 准备数据
texts = [
    "这部电影太棒了！",
    "我不喜欢这个产品",
    "服务非常好",
    "体验很糟糕",
    "推荐给大家",
    "不建议购买"
]
labels = [1, 0, 1, 0, 1, 0]  # 1: 正面, 0: 负面

# 简单的词汇表和编码
vocab = list(set(''.join(texts)))
char_to_idx = {ch: i for i, ch in enumerate(vocab)}
vocab_size = len(vocab)

# 将文本转换为数字序列
def text_to_sequence(text, max_len=20):
    seq = [char_to_idx.get(ch, 0) for ch in text]
    # 填充或截断到固定长度
    if len(seq) < max_len:
        seq += [0] * (max_len - len(seq))
    else:
        seq = seq[:max_len]
    return seq

# 创建训练数据
max_seq_len = 20
X_train = np.array([text_to_sequence(text, max_seq_len) for text in texts])
y_train = np.array(labels)

# 将输入转换为one-hot编码
def to_one_hot(sequences, vocab_size):
    batch_size, seq_len = sequences.shape
    one_hot = np.zeros((batch_size, seq_len, vocab_size))
    for i in range(batch_size):
        for j in range(seq_len):
            if sequences[i, j] != 0:  # 0是填充符
                one_hot[i, j, sequences[i, j]] = 1
    return one_hot

X_train_one_hot = to_one_hot(X_train, vocab_size)

# 创建和训练LSTM模型
lstm = OptimizedLSTM(
    input_dim=vocab_size,
    hidden_dim=64,
    output_dim=2  # 二分类
)

# 训练循环
epochs = 100
batch_size = 3

for epoch in range(epochs):
    # 随机打乱数据
    indices = np.random.permutation(len(X_train))

    total_loss = 0
    for i in range(0, len(X_train), batch_size):
        batch_indices = indices[i:i+batch_size]
        X_batch = X_train_one_hot[batch_indices]

        # 创建目标输出（最后一个时间步的分类）
        y_batch = np.zeros((len(batch_indices), max_seq_len, 2))
        for j, idx in enumerate(batch_indices):
            y_batch[j, -1, y_train[idx]] = 1  # 只在最后时间步输出分类

        loss = lstm.train_step(X_batch, y_batch)
        total_loss += loss

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss/len(X_train):.4f}")

# 预测函数
def predict_sentiment(text):
    seq = text_to_sequence(text, max_seq_len)
    X = to_one_hot(np.array([seq]), vocab_size)

    Y_pred, _, _ = lstm.forward(X)
    # 取最后一个时间步的输出
    prediction = Y_pred[0, -1, :]
    sentiment = np.argmax(prediction)

    return "正面" if sentiment == 1 else "负面"

# 测试模型
test_texts = [
    "非常满意",
    "质量很差",
    "超出预期",
    "浪费钱了"
]

print("\n=== 情感预测结果 ===")
for text in test_texts:
    sentiment = predict_sentiment(text)
    print(f"{text}: {sentiment}")

# 可视化LSTM内部状态
def visualize_lstm_states(text):
    seq = text_to_sequence(text, max_seq_len)
    X = to_one_hot(np.array([seq]), vocab_size)

    _, H, C = lstm.forward(X)

    import matplotlib.pyplot as plt

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # 绘制隐藏状态
    im1 = ax1.imshow(H[0].T, aspect='auto', cmap='RdBu_r')
    ax1.set_title('Hidden States')
    ax1.set_xlabel('Time Step')
    ax1.set_ylabel('Hidden Unit')
    plt.colorbar(im1, ax=ax1)

    # 绘制细胞状态
    im2 = ax2.imshow(C[0].T, aspect='auto', cmap='RdBu_r')
    ax2.set_title('Cell States')
    ax2.set_xlabel('Time Step')
    ax2.set_ylabel('Cell Unit')
    plt.colorbar(im2, ax=ax2)

    plt.tight_layout()
    plt.show()

# 可视化一个例子
visualize_lstm_states("这个产品真的很好用")</code></pre>
                    </div>
                </div>

                <div class="tip info mt-3">
                    <span class="tip-icon">💡</span>
                    <div class="tip-content">
                        <strong>实现要点：</strong><br>
                        • 注意数值稳定性（sigmoid和tanh的实现）<br>
                        • 梯度裁剪防止梯度爆炸<br>
                        • 合并门计算提高效率<br>
                        • 使用适当的初始化方法
                    </div>
                </div>
            </section>

            <!-- 可视化理解 -->
            <section id="visualization" class="section-card">
                <h2 style="color: var(--accent-blue); margin-bottom: 1.5rem;">👁️ 可视化理解LSTM</h2>

                <div class="visualization-container">
                    <h3 style="color: var(--accent-cyan); margin-bottom: 1.5rem;">🎯 LSTM处理序列的过程</h3>

                    <div class="viz-controls">
                        <button class="btn btn-primary" onclick="animateLSTMFlow()">
                            ▶️ 演示信息流动
                        </button>
                        <button class="btn btn-secondary" onclick="visualizeGates()">
                            🚪 查看门的激活
                        </button>
                        <button class="btn btn-secondary" onclick="compareLSTMvsRNN()">
                            📊 对比RNN
                        </button>
                    </div>

                    <div id="lstm-viz-container" style="margin-top: 2rem;">
                        <canvas id="lstm-viz" width="900" height="500"></canvas>
                    </div>
                </div>

                <div class="code-container mt-3">
                    <div class="code-header">
                        <div class="code-header-left">
                            <span style="color: var(--text-secondary);">lstm_visualization.py - LSTM可视化分析</span>
                        </div>
                    </div>
                    <div class="code-content">
                        <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_lstm_behavior(lstm_model, text_sequence):
    """分析LSTM在处理序列时的行为"""

    # 获取各个门的激活值
    gates_history = {
        'forget': [],
        'input': [],
        'output': [],
        'cell_state': [],
        'hidden_state': []
    }

    # 逐步处理序列
    h = np.zeros((lstm_model.hidden_dim, 1))
    c = np.zeros((lstm_model.hidden_dim, 1))

    for char in text_sequence:
        # 前向传播并记录门的值
        x = char_to_vector(char)
        h, c, gates = lstm_model.forward_with_gates(x, h, c)

        gates_history['forget'].append(gates['f'])
        gates_history['input'].append(gates['i'])
        gates_history['output'].append(gates['o'])
        gates_history['cell_state'].append(c)
        gates_history['hidden_state'].append(h)

    # 可视化
    fig, axes = plt.subplots(3, 2, figsize=(15, 12))

    # 1. 遗忘门激活模式
    ax = axes[0, 0]
    forget_matrix = np.hstack(gates_history['forget']).T
    sns.heatmap(forget_matrix, ax=ax, cmap='Reds', cbar_kws={'label': 'Activation'})
    ax.set_title('Forget Gate Activations')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Hidden Unit')

    # 2. 输入门激活模式
    ax = axes[0, 1]
    input_matrix = np.hstack(gates_history['input']).T
    sns.heatmap(input_matrix, ax=ax, cmap='Blues', cbar_kws={'label': 'Activation'})
    ax.set_title('Input Gate Activations')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Hidden Unit')

    # 3. 输出门激活模式
    ax = axes[1, 0]
    output_matrix = np.hstack(gates_history['output']).T
    sns.heatmap(output_matrix, ax=ax, cmap='Greens', cbar_kws={'label': 'Activation'})
    ax.set_title('Output Gate Activations')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Hidden Unit')

    # 4. 细胞状态演化
    ax = axes[1, 1]
    cell_matrix = np.hstack(gates_history['cell_state']).T
    sns.heatmap(cell_matrix, ax=ax, cmap='viridis', center=0,
                cbar_kws={'label': 'Cell State Value'})
    ax.set_title('Cell State Evolution')
    ax.set_xlabel('Time Step')
    ax.set_ylabel('Cell Unit')

    # 5. 门的平均激活
    ax = axes[2, 0]
    avg_activations = {
        'Forget': np.mean([np.mean(g) for g in gates_history['forget']]),
        'Input': np.mean([np.mean(g) for g in gates_history['input']]),
        'Output': np.mean([np.mean(g) for g in gates_history['output']])
    }
    bars = ax.bar(avg_activations.keys(), avg_activations.values(),
                   color=['red', 'blue', 'green'])
    ax.set_ylabel('Average Activation')
    ax.set_title('Average Gate Activations')
    ax.set_ylim(0, 1)

    # 6. 细胞状态的记忆持续时间
    ax = axes[2, 1]
    memory_persistence = compute_memory_persistence(gates_history['cell_state'])
    ax.plot(memory_persistence, 'o-', linewidth=2, markersize=8)
    ax.set_xlabel('Hidden Unit')
    ax.set_ylabel('Average Memory Persistence (steps)')
    ax.set_title('Memory Persistence by Unit')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return gates_history

def compute_memory_persistence(cell_states):
    """计算每个单元的记忆持续时间"""
    n_units = cell_states[0].shape[0]
    persistence = []

    for unit in range(n_units):
        # 追踪每个单元的值变化
        unit_values = [state[unit, 0] for state in cell_states]

        # 计算值的持续时间
        changes = np.diff(unit_values)
        stable_periods = []
        current_period = 1

        for change in changes:
            if abs(change) < 0.1:  # 值基本不变
                current_period += 1
            else:
                stable_periods.append(current_period)
                current_period = 1

        avg_persistence = np.mean(stable_periods) if stable_periods else 1
        persistence.append(avg_persistence)

    return persistence

def compare_gradient_flow():
    """比较LSTM和RNN的梯度流动"""

    sequence_lengths = [5, 10, 20, 50, 100]

    # RNN梯度衰减
    rnn_gradients = []
    for T in sequence_lengths:
        # 假设平均梯度衰减因子为0.5
        gradient = 0.5 ** T
        rnn_gradients.append(gradient)

    # LSTM梯度传播
    lstm_gradients = []
    for T in sequence_lengths:
        # 假设遗忘门平均为0.9
        gradient = 0.9 ** T
        lstm_gradients.append(gradient)

    # 可视化
    plt.figure(figsize=(10, 6))

    plt.semilogy(sequence_lengths, rnn_gradients, 'ro-',
                 label='RNN', linewidth=2, markersize=8)
    plt.semilogy(sequence_lengths, lstm_gradients, 'bo-',
                 label='LSTM', linewidth=2, markersize=8)

    # 添加临界线
    plt.axhline(y=0.01, color='gray', linestyle='--', alpha=0.5,
                label='Vanishing Gradient Threshold')

    plt.xlabel('Sequence Length')
    plt.ylabel('Gradient Magnitude (log scale)')
    plt.title('Gradient Flow: LSTM vs RNN')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # 添加注释
    plt.annotate('RNN fails here',
                 xy=(20, rnn_gradients[2]),
                 xytext=(25, 0.001),
                 arrowprops=dict(arrowstyle='->', color='red'))

    plt.annotate('LSTM still works',
                 xy=(50, lstm_gradients[3]),
                 xytext=(55, 0.1),
                 arrowprops=dict(arrowstyle='->', color='blue'))

    plt.tight_layout()
    plt.show()

# 创建交互式LSTM演示
class InteractiveLSTM:
    """交互式LSTM演示"""

    def __init__(self):
        self.sequence = "LSTM可以记住重要信息"
        self.position = 0
        self.cell_state = np.zeros(10)
        self.hidden_state = np.zeros(10)
        self.history = []

    def step(self):
        """处理下一个字符"""
        if self.position < len(self.sequence):
            char = self.sequence[self.position]

            # 模拟LSTM处理
            # 这里使用简化的规则来演示
            if char in "LSTM重要":
                # 重要字符：输入门开大，遗忘门关小
                forget_gate = 0.2
                input_gate = 0.9
                output_gate = 0.8
            else:
                # 普通字符：正常处理
                forget_gate = 0.5
                input_gate = 0.5
                output_gate = 0.6

            # 更新状态
            self.cell_state = forget_gate * self.cell_state + \
                             input_gate * np.random.randn(10) * 0.1
            self.hidden_state = output_gate * np.tanh(self.cell_state)

            self.history.append({
                'char': char,
                'forget': forget_gate,
                'input': input_gate,
                'output': output_gate,
                'cell_state': self.cell_state.copy(),
                'hidden_state': self.hidden_state.copy()
            })

            self.position += 1
            return True
        return False

    def visualize_current_state(self):
        """可视化当前状态"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # 1. 处理进度
        ax = axes[0, 0]
        ax.text(0.5, 0.7, self.sequence, fontsize=16, ha='center')
        if self.position > 0:
            # 高亮当前字符
            ax.text(0.5, 0.5, ' ' * self.position + '▼',
                   fontsize=16, ha='center', color='red')
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title(f'Processing: Step {self.position}')

        # 2. 门的历史
        ax = axes[0, 1]
        if self.history:
            steps = range(len(self.history))
            forget_vals = [h['forget'] for h in self.history]
            input_vals = [h['input'] for h in self.history]
            output_vals = [h['output'] for h in self.history]

            ax.plot(steps, forget_vals, 'r-o', label='Forget Gate')
            ax.plot(steps, input_vals, 'b-o', label='Input Gate')
            ax.plot(steps, output_vals, 'g-o', label='Output Gate')
            ax.set_ylim(0, 1)
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Gate Activation')
            ax.legend()
            ax.grid(True, alpha=0.3)
        ax.set_title('Gate Activations Over Time')

        # 3. 当前细胞状态
        ax = axes[1, 0]
        ax.bar(range(10), self.cell_state)
        ax.set_xlabel('Cell Unit')
        ax.set_ylabel('Cell State Value')
        ax.set_title('Current Cell State')
        ax.grid(True, alpha=0.3)

        # 4. 记忆强度热图
        ax = axes[1, 1]
        if self.history:
            memory_matrix = np.array([h['cell_state'] for h in self.history]).T
            im = ax.imshow(memory_matrix, aspect='auto', cmap='coolwarm',
                          interpolation='nearest')
            ax.set_xlabel('Time Step')
            ax.set_ylabel('Cell Unit')
            ax.set_title('Cell State Evolution')
            plt.colorbar(im, ax=ax)

        plt.tight_layout()
        plt.show()

# 使用示例
if __name__ == "__main__":
    # 分析LSTM行为
    lstm = LSTM(input_dim=100, hidden_dim=64, output_dim=10, seq_length=20)

    # 比较梯度流动
    print("比较LSTM和RNN的梯度流动...")
    compare_gradient_flow()

    # 交互式演示
    print("\n创建交互式LSTM演示...")
    demo = InteractiveLSTM()

    # 逐步处理
    while demo.step():
        if demo.position % 5 == 0:  # 每5步可视化一次
            demo.visualize_current_state()</code></pre>
                    </div>
                </div>
            </section>

            <!-- 实战应用 -->
            <section id="practice" class="section-card">
                <h2 style="color: var(--accent-pink); margin-bottom: 1.5rem;">⚔️ 实战：用LSTM生成音乐</h2>

                <div class="story-card">
                    <p style="font-size: 1.1rem; line-height: 1.8; margin-bottom: 2rem;">
                        让我们用LSTM创建一个简单的音乐生成器，体验长期记忆的威力！
                    </p>
                </div>

                <div class="code-container">
                    <div class="code-header">
                        <div class="code-header-left">
                            <span style="color: var(--text-secondary);">music_lstm.py - 音乐生成LSTM</span>
                        </div>
                    </div>
                    <div class="code-content">
                        <pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from midiutil import MIDIFile

class MusicLSTM:
    """使用LSTM生成音乐"""

    def __init__(self, n_notes=88, hidden_dim=256, seq_length=32):
        """
        初始化音乐LSTM

        参数:
        n_notes: 音符数量（钢琴88键）
        hidden_dim: 隐藏层维度
        seq_length: 序列长度
        """
        self.n_notes = n_notes
        self.hidden_dim = hidden_dim
        self.seq_length = seq_length

        # LSTM参数
        self.lstm = OptimizedLSTM(n_notes, hidden_dim, n_notes)

        # 音乐理论知识
        self.scales = {
            'C_major': [0, 2, 4, 5, 7, 9, 11],
            'A_minor': [0, 2, 3, 5, 7, 8, 10],
            'pentatonic': [0, 2, 4, 7, 9]
        }

    def encode_melody(self, notes, durations):
        """将旋律编码为LSTM输入"""
        seq_length = len(notes)
        encoded = np.zeros((1, seq_length, self.n_notes + 1))  # +1 for duration

        for t, (note, duration) in enumerate(zip(notes, durations)):
            if note >= 0:  # -1表示休止符
                encoded[0, t, note] = 1
            encoded[0, t, -1] = duration  # 最后一维存储时长

        return encoded

    def decode_output(self, output, temperature=1.0):
        """解码LSTM输出为音符"""
        # 应用温度参数
        if temperature != 1.0:
            output = np.log(output + 1e-7) / temperature
            output = np.exp(output) / np.sum(np.exp(output))

        # 采样音符
        note = np.random.choice(self.n_notes, p=output[:-1])
        duration = np.clip(output[-1], 0.25, 2.0)  # 限制时长范围

        return note, duration

    def train_on_melody(self, melodies, epochs=100):
        """在旋律数据集上训练"""
        print("训练音乐LSTM...")

        losses = []
        for epoch in range(epochs):
            epoch_loss = 0

            for melody in melodies:
                notes, durations = melody

                # 创建训练序列
                X = self.encode_melody(notes[:-1], durations[:-1])
                Y = self.encode_melody(notes[1:], durations[1:])

                # 训练步骤
                loss = self.lstm.train_step(X, Y)
                epoch_loss += loss

            losses.append(epoch_loss / len(melodies))

            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {losses[-1]:.4f}")

        return losses

    def generate_melody(self, seed_notes, length=64, temperature=0.8):
        """生成新的旋律"""
        generated_notes = list(seed_notes)
        generated_durations = [0.5] * len(seed_notes)  # 默认时长

        # 初始化LSTM状态
        h = np.zeros((1, self.hidden_dim))
        c = np.zeros((1, self.hidden_dim))

        # 处理种子序列
        seed_encoded = self.encode_melody(seed_notes, generated_durations)
        _, h, c = self.lstm.forward(seed_encoded, h, c)

        # 生成新音符
        for _ in range(length - len(seed_notes)):
            # 使用最后生成的音符作为输入
            last_note = generated_notes[-1]
            last_duration = generated_durations[-1]

            x = np.zeros((1, 1, self.n_notes + 1))
            if last_note >= 0:
                x[0, 0, last_note] = 1
            x[0, 0, -1] = last_duration

            # LSTM前向传播
            output, h, c = self.lstm.forward(x, h, c)

            # 解码输出
            probs = self.softmax(output[0, 0, :])
            note, duration = self.decode_output(probs, temperature)

            generated_notes.append(note)
            generated_durations.append(duration)

        return generated_notes, generated_durations

    def apply_music_theory(self, notes, scale='C_major'):
        """应用音乐理论约束"""
        scale_notes = self.scales[scale]

        # 将音符映射到最近的音阶音
        constrained_notes = []
        for note in notes:
            if note < 0:  # 休止符
                constrained_notes.append(note)
            else:
                octave = note // 12
                pitch_class = note % 12

                # 找到最近的音阶音
                distances = [abs(pitch_class - s) for s in scale_notes]
                nearest_scale_note = scale_notes[np.argmin(distances)]

                constrained_note = octave * 12 + nearest_scale_note
                constrained_notes.append(constrained_note)

        return constrained_notes

    def save_as_midi(self, notes, durations, filename='generated_music.mid'):
        """保存为MIDI文件"""
        midi = MIDIFile(1)
        track = 0
        channel = 0
        tempo = 120

        midi.addTempo(track, 0, tempo)

        time = 0
        for note, duration in zip(notes, durations):
            if note >= 0:  # 非休止符
                pitch = note + 21  # MIDI音高（A0=21）
                velocity = 80
                midi.addNote(track, channel, pitch, time, duration, velocity)
            time += duration

        with open(filename, 'wb') as f:
            midi.writeFile(f)

        print(f"音乐已保存到 {filename}")

    def visualize_melody(self, notes, durations, title="Generated Melody"):
        """可视化旋律"""
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 8),
                                       gridspec_kw={'height_ratios': [3, 1]})

        # 音高图
        time = 0
        times = []
        pitches = []

        for note, duration in zip(notes, durations):
            if note >= 0:
                times.append(time)
                pitches.append(note)
                ax1.plot([time, time + duration], [note, note],
                        'b-', linewidth=3)
                ax1.scatter(time, note, c='red', s=50, zorder=5)
            time += duration

        ax1.set_xlabel('Time (beats)')
        ax1.set_ylabel('MIDI Note Number')
        ax1.set_title(title)
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(max(0, min(pitches) - 5), max(pitches) + 5)

        # 节奏图
        time = 0
        for i, duration in enumerate(durations):
            ax2.barh(0, duration, left=time, height=0.8,
                    color='green' if notes[i] >= 0 else 'gray',
                    edgecolor='black')
            time += duration

        ax2.set_xlabel('Time (beats)')
        ax2.set_yticks([])
        ax2.set_title('Rhythm Pattern')
        ax2.set_ylim(-0.5, 0.5)

        plt.tight_layout()
        plt.show()

    @staticmethod
    def softmax(x):
        """Softmax函数"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)

# 创建训练数据
def create_training_data():
    """创建简单的训练旋律"""

    # 音阶练习
    scales = []
    for start_note in [60, 62, 64, 65, 67, 69, 71]:  # C大调音阶
        scale = list(range(start_note, start_note + 8))
        scale.extend(reversed(scale[:-1]))
        scales.append((scale, [0.5] * len(scale)))

    # 和弦进行
    chord_progressions = [
        # I-V-vi-IV进行
        ([60, 64, 67, 60, 64, 67,  # C
          67, 71, 74, 67, 71, 74,  # G
          69, 72, 76, 69, 72, 76,  # Am
          65, 69, 72, 65, 69, 72], # F
         [0.5] * 24),

        # 蓝调进行
        ([60, 63, 65, 67, 70,  # C7
          65, 68, 70, 72, 75,  # F7
          60, 63, 65, 67, 70,  # C7
          67, 70, 72, 74, 77], # G7
         [0.4, 0.4, 0.4, 0.4, 0.8] * 4)
    ]

    # 简单旋律
    melodies = [
        # 小星星
        ([60, 60, 67, 67, 69, 69, 67, -1,
          65, 65, 64, 64, 62, 62, 60, -1],
         [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5,
          0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1.0, 0.5]),

        # 欢乐颂片段
        ([64, 64, 65, 67, 67, 65, 64, 62,
          60, 60, 62, 64, 64, 62, 62, -1],
         [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
          0.5, 0.5, 0.5, 0.5, 0.75, 0.25, 1.0, 0.5])
    ]

    return scales + chord_progressions + melodies

# 使用示例
if __name__ == "__main__":
    # 创建模型
    music_lstm = MusicLSTM(n_notes=88, hidden_dim=128)

    # 准备训练数据
    training_data = create_training_data()

    # 训练模型
    losses = music_lstm.train_on_melody(training_data, epochs=50)

    # 可视化训练过程
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title('Music LSTM Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.show()

    # 生成新旋律
    print("\n生成新的音乐...")

    # 使用C大调音阶作为种子
    seed = [60, 62, 64, 65]  # C D E F

    # 生成不同风格的音乐
    temperatures = [0.5, 0.8, 1.2]

    for temp in temperatures:
        print(f"\n温度参数: {temp}")
        notes, durations = music_lstm.generate_melody(
            seed, length=32, temperature=temp
        )

        # 应用音乐理论
        notes = music_lstm.apply_music_theory(notes, 'C_major')

        # 可视化
        music_lstm.visualize_melody(
            notes, durations,
            title=f"Generated Melody (Temperature={temp})"
        )

        # 保存MIDI
        music_lstm.save_as_midi(
            notes, durations,
            filename=f'melody_temp_{temp}.mid'
        )

    # 分析生成的音乐
    print("\n分析生成的音乐模式...")

    # 音程分析
    intervals = np.diff([n for n in notes if n >= 0])
    plt.figure(figsize=(10, 5))
    plt.hist(intervals, bins=25, edgecolor='black')
    plt.xlabel('Interval (semitones)')
    plt.ylabel('Frequency')
    plt.title('Interval Distribution in Generated Music')
    plt.grid(True, alpha=0.3)
    plt.show()</code></pre>
                    </div>
                </div>

                <div class="tip info mt-3">
                    <span class="tip-icon">💡</span>
                    <div class="tip-content">
                        <strong>音乐生成技巧：</strong><br>
                        • 温度参数控制创造性（低=保守，高=实验性）<br>
                        • 应用音乐理论约束提高音乐性<br>
                        • 使用不同的种子序列引导风格<br>
                        • 结合节奏模式让音乐更生动
                    </div>
                </div>
            </section>

            <!-- 总结与展望 -->
            <section id="summary" class="section-card">
                <h2 style="color: var(--accent-purple); margin-bottom: 1.5rem;">🎯 本章总结：LSTM的精妙设计</h2>

                <div class="concept-grid">
                    <div class="concept-card forget">
                        <h3 style="color: var(--accent-pink); margin-bottom: 1rem;">🎯 核心创新</h3>
                        <ul style="margin-left: 1.5rem;">
                            <li>细胞状态：信息高速公路</li>
                            <li>门控机制：精确控制信息流</li>
                            <li>梯度传播：解决长期依赖</li>
                            <li>模块化设计：易于理解和实现</li>
                        </ul>
                    </div>

                    <div class="concept-card input">
                        <h3 style="color: var(--accent-blue); margin-bottom: 1rem;">💡 设计智慧</h3>
                        <ul style="margin-left: 1.5rem;">
                            <li>遗忘门：选择性遗忘</li>
                            <li>输入门：选择性记忆</li>
                            <li>输出门：按需提取</li>
                            <li>线性传播：保持梯度</li>
                        </ul>
                    </div>

                    <div class="concept-card output">
                        <h3 style="color: var(--accent-green); margin-bottom: 1rem;">🚀 应用领域</h3>
                        <ul style="margin-left: 1.5rem;">
                            <li>机器翻译</li>
                            <li>语音识别</li>
                            <li>音乐生成</li>
                            <li>时序预测</li>
                        </ul>
                    </div>

                    <div class="concept-card cell">
                        <h3 style="color: var(--accent-orange); margin-bottom: 1rem;">⚠️ 局限性</h3>
                        <ul style="margin-left: 1.5rem;">
                            <li>计算复杂度高</li>
                            <li>难以并行化</li>
                            <li>参数量大</li>
                            <li>仍有信息瓶颈</li>
                        </ul>
                    </div>
                </div>

                <div class="comparison-table mt-4">
                    <table>
                        <thead>
                        <tr>
                            <th>特性</th>
                            <th>RNN</th>
                            <th>LSTM</th>
                            <th>改进程度</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td><strong>长期依赖</strong></td>
                            <td>~10步</td>
                            <td>~100步</td>
                            <td style="color: var(--accent-green);">10倍提升</td>
                        </tr>
                        <tr>
                            <td><strong>梯度流动</strong></td>
                            <td>指数衰减</td>
                            <td>可控衰减</td>
                            <td style="color: var(--accent-green);">显著改善</td>
                        </tr>
                        <tr>
                            <td><strong>参数量</strong></td>
                            <td>O(h²)</td>
                            <td>O(4h²)</td>
                            <td style="color: var(--accent-yellow);">4倍增加</td>
                        </tr>
                        <tr>
                            <td><strong>计算复杂度</strong></td>
                            <td>低</td>
                            <td>高</td>
                            <td style="color: var(--accent-red);">显著增加</td>
                        </tr>
                        </tbody>
                    </table>
                </div>

                <div class="tip success mt-4">
                    <span class="tip-icon">🎉</span>
                    <div class="tip-content">
                        <strong>恭喜你！</strong><br>
                        你已经深入理解了LSTM的设计原理和数学基础。LSTM通过精巧的门控机制，成功解决了RNN的梯度消失问题，
                        使得神经网络能够学习长期依赖关系。虽然后来出现了更高效的Transformer架构，但LSTM的设计思想仍然具有重要的启发意义。
                    </div>
                </div>

                <!-- 思考题 -->
                <div class="think-box mt-4">
                    <h3 style="color: var(--accent-purple); margin-bottom: 1rem;">🤔 深入思考</h3>
                    <ol style="margin-left: 2rem; line-height: 2;">
                        <li>为什么LSTM使用sigmoid作为门控函数，而不是其他函数？</li>
                        <li>如果去掉某个门（如遗忘门），LSTM会发生什么？</li>
                        <li>GRU是如何简化LSTM的？它牺牲了什么，又获得了什么？</li>
                        <li>在什么情况下，简单的RNN可能比LSTM表现更好？</li>
                        <li>如何设计一个"注意力LSTM"，结合两者的优势？</li>
                    </ol>
                </div>

                <!-- 扩展阅读 -->
                <div style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.1), rgba(124, 58, 237, 0.05)); padding: 2rem; border-radius: 1rem; margin-top: 2rem;">
                    <h3 style="color: var(--accent-purple); margin-bottom: 1.5rem;">📚 扩展阅读</h3>
                    <ul style="margin-left: 2rem; line-height: 2;">
                        <li>原始论文：<em>Long Short-Term Memory</em> (Hochreiter & Schmidhuber, 1997)</li>
                        <li>改进版本：<em>Learning to Forget: Continual Prediction with LSTM</em> (Gers et al., 2000)</li>
                        <li>GRU论文：<em>Learning Phrase Representations using RNN Encoder-Decoder</em> (Cho et al., 2014)</li>
                        <li>应用案例：<em>Show and Tell: A Neural Image Caption Generator</em> (Vinyals et al., 2015)</li>
                    </ul>
                </div>

                <!-- 下一章预告 -->
                <div style="text-align: center; margin-top: 3rem; padding: 2rem; background: var(--lstm-gradient); border-radius: 1rem;">
                    <h3 style="color: #0f172a; margin-bottom: 1rem;">🚀 下一章预告</h3>
                    <p style="color: #0f172a; font-size: 1.2rem; margin-bottom: 1.5rem;">
                        <strong>第9章：注意力机制与Transformer</strong><br>
                        告别循环，拥抱并行！看Attention如何革命性地改变深度学习
                    </p>
                    <button class="btn" style="background: #0f172a; color: white; font-size: 1.1rem; padding: 1rem 2rem;">
                        继续学习 →
                    </button>
                </div>
            </section>

        </div>
    </div>
</main>

<!-- JavaScript代码 -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<script>
    // 初始化代码高亮
    hljs.highlightAll();

    // 初始化数学公式渲染
    renderMathInElement(document.body, {
        delimiters: [
            {left: '$', right: '$', display: true},
            {left: '
    # 重要字符：, right: '
    # 重要字符：, display: false}
    ]
    });

    // ===== 全局变量 =====
    let animationsEnabled = true;
    let lstmAnimationFrame = null;

    // ===== 导航功能 =====
    const sidebar = document.getElementById('sidebar');
    const sidebarOverlay = document.getElementById('sidebar-overlay');
    const toggleSidebarBtn = document.getElementById('toggle-sidebar');

    toggleSidebarBtn.addEventListener('click', () => {
        sidebar.classList.toggle('open');
        sidebarOverlay.classList.toggle('active');
    });

    sidebarOverlay.addEventListener('click', () => {
        sidebar.classList.remove('open');
        sidebarOverlay.classList.remove('active');
    });

    // ===== 主题切换 =====
    const toggleThemeBtn = document.getElementById('toggle-theme');
    toggleThemeBtn.addEventListener('click', () => {
        const currentTheme = document.body.getAttribute('data-theme');
        const newTheme = currentTheme === 'light' ? 'dark' : 'light';
        document.body.setAttribute('data-theme', newTheme);
        localStorage.setItem('theme', newTheme);
    });

    // 加载保存的主题
    const savedTheme = localStorage.getItem('theme') || 'dark';
    document.body.setAttribute('data-theme', savedTheme);

    // ===== 动画控制 =====
    const toggleAnimationsBtn = document.getElementById('toggle-animations');
    toggleAnimationsBtn.addEventListener('click', () => {
        animationsEnabled = !animationsEnabled;
        if (!animationsEnabled && lstmAnimationFrame) {
            cancelAnimationFrame(lstmAnimationFrame);
        }
    });

    // ===== 滚动进度条 =====
    function updateProgressBar() {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
        const scrollHeight = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        const scrollPercentage = (scrollTop / scrollHeight) * 100;
        document.getElementById('progress-bar').style.width = scrollPercentage + '%';
    }

    window.addEventListener('scroll', updateProgressBar);

    // ===== 导航高亮 =====
    const sections = document.querySelectorAll('section[id]');
    const navItems = document.querySelectorAll('.toc-item');

    function updateNavHighlight() {
        const scrollPosition = window.pageYOffset + 100;

        sections.forEach((section, index) => {
            const sectionTop = section.offsetTop;
            const sectionHeight = section.offsetHeight;

            if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
                navItems.forEach(item => item.classList.remove('active'));
                if (navItems[index]) {
                    navItems[index].classList.add('active');
                }
            }
        });
    }

    window.addEventListener('scroll', updateNavHighlight);

    // ===== 代码标签切换 =====
    document.querySelectorAll('.code-tab').forEach(tab => {
        tab.addEventListener('click', function() {
            const container = this.closest('.code-container');
            const tabName = this.dataset.tab;

            // 更新标签状态
            container.querySelectorAll('.code-tab').forEach(t => t.classList.remove('active'));
            this.classList.add('active');

            // 切换内容
            container.querySelectorAll('[data-tab-content]').forEach(content => {
                content.style.display = content.dataset.tabContent === tabName ? 'block' : 'none';
            });
        });
    });

    // ===== LSTM可视化函数 =====
    function drawLSTMFlow() {
        const canvas = document.getElementById('lstm-flow-diagram');
        if (!canvas) return;

        const ctx = canvas.getContext('2d');
        const width = canvas.width;
        const height = canvas.height;

        // 清空画布
        ctx.clearRect(0, 0, width, height);

        // 绘制LSTM结构
        // ... 详细的绘图代码 ...
    }

    function drawCellStateHighway() {
        const canvas = document.getElementById('cell-state-highway');
        if (!canvas) return;

        const ctx = canvas.getContext('2d');
        const width = canvas.width;
        const height = canvas.height;

        ctx.clearRect(0, 0, width, height);

        // 绘制细胞状态高速公路
        ctx.strokeStyle = '#fa709a';
        ctx.lineWidth = 8;
        ctx.beginPath();
        ctx.moveTo(50, height/2);
        ctx.lineTo(width - 50, height/2);
        ctx.stroke();

        // 添加箭头
        ctx.fillStyle = '#fa709a';
        ctx.beginPath();
        ctx.moveTo(width - 50, height/2);
        ctx.lineTo(width - 70, height/2 - 10);
        ctx.lineTo(width - 70, height/2 + 10);
        ctx.closePath();
        ctx.fill();

        // 标签
        ctx.fillStyle = '#f1f5f9';
        ctx.font = '16px Arial';
        ctx.textAlign = 'center';
        ctx.fillText('Cell State Highway', width/2, height/2 - 30);
        ctx.fillText('C(t-1) → C(t)', width/2, height/2 + 40);
    }

    function animateLSTMFlow() {
        const canvas = document.getElementById('lstm-viz');
        if (!canvas) return;

        const ctx = canvas.getContext('2d');
        const width = canvas.width;
        const height = canvas.height;

        let step = 0;
        const totalSteps = 100;

        function animate() {
            if (!animationsEnabled) return;

            ctx.clearRect(0, 0, width, height);

            // 绘制动画
            const progress = step / totalSteps;

            // 绘制门
            const gates = [
                {name: 'Forget', x: 200, y: 150, color: '#f5576c'},
                {name: 'Input', x: 400, y: 150, color: '#4facfe'},
                {name: 'Output', x: 600, y: 150, color: '#43e97b'}
            ];

            gates.forEach((gate, i) => {
                const activation = Math.sin(progress * Math.PI * 2 + i * Math.PI / 3) * 0.5 + 0.5;

                ctx.fillStyle = gate.color;
                ctx.globalAlpha = 0.3 + activation * 0.7;
                ctx.fillRect(gate.x - 50, gate.y - 50, 100, 100);

                ctx.globalAlpha = 1;
                ctx.fillStyle = '#f1f5f9';
                ctx.font = '16px Arial';
                ctx.textAlign = 'center';
                ctx.fillText(gate.name, gate.x, gate.y);
                ctx.font = '14px Arial';
                ctx.fillText(`${(activation * 100).toFixed(0)}%`, gate.x, gate.y + 20);
            });

            step = (step + 1) % totalSteps;
            lstmAnimationFrame = requestAnimationFrame(animate);
        }

        animate();
    }

    function visualizeGates() {
        // 门激活可视化
        console.log('Visualizing gate activations...');
    }

    function compareLSTMvsRNN() {
        // LSTM vs RNN对比
        console.log('Comparing LSTM vs RNN...');
    }

    // ===== 初始化 =====
    window.addEventListener('load', () => {
        drawLSTMFlow();
        drawCellStateHighway();
        updateProgressBar();
        updateNavHighlight();
    });

    console.log('🎉 LSTM章节加载完成！');
</script>

</body>
</html>
# 重要字符：